%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.0 (9/2/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%\documentclass[11pt,fleqn,dvipsnames]{book} % Default font size and left-justified equations
\documentclass[11pt,dvipsnames]{book}

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

%%agregué

%%%My stuff

%\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tgpagella}
%\usepackage{due-dates}
\usepackage[small]{eulervm}
\usepackage{amsmath,amssymb,amstext,amsthm,amscd,mathrsfs,eucal,bm,xcolor}
\usepackage{multicol}
\usepackage{array,color,graphicx}
%\usepackage{enumerate}

\usepackage{epigraph}
%\usepackage[colorlinks,citecolor=red,linkcolor=blue,pagebackref,hypertexnames=false]{hyperref}

%\theoremstyle{remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{example}[theorem]{\bf Example}
%\newtheorem*{solution}{Solution:}

\usepackage{centernot}

\usepackage{filecontents}

\usepackage{tcolorbox}

% Ignore this part, this is the former way of hiding and unhiding solutions, new version is after this
%
%\begin{filecontents*}{MyPackage.sty}
%\NeedsTeXFormat{LaTeX2e}
%\ProvidesPackage{MyPackage}
%\RequirePackage{environ}
%\newif\if@hidden% \@hiddenfalse
%\DeclareOption{hide}{\global\@hiddentrue}
%\DeclareOption{unhide}{\global\@hiddenfalse}
%\ProcessOptions\relax
%\NewEnviron{solution}
%  {\if@hidden\else \begin{tcolorbox}{\bf Solution: }\BODY \end{tcolorbox}\fi}
%\end{filecontents*}
%
%
%
%\usepackage[hide]{MyPackage} % hides all solutions
%\usepackage[unhide]{MyPackage} %shows all solutions

%\usepackage[unhide,all]{hide-soln} %show all solutions
\usepackage[unhide,odd]{hide-soln} %hide even number solutions
%\usepackage[hide]{hide-soln} %hide all solutions

\def\putgrid{\put(0,0){0}
\put(0,25){25}
\put(0,50){50}
\put(0,75){75}
\put(0,100){100}
\put(0,125){125}
\put(0,150){150}
\put(0,175){175}
\put(0,200){200}
\put(25,0){25}
\put(50,0){50}
\put(75,0){75}
\put(100,0){100}
\put(125,0){125}
\put(150,0){150}
\put(175,0){175}
\put(200,0){200}
\put(225,0){225}
\put(250,0){250}
\put(275,0){275}
\put(300,0){300}
\put(325,0){325}
\put(350,0){350}
\put(375,0){375}
\put(400,0){400}
{\color{gray}\multiput(0,0)(25,0){16}{\line(0,1){200}}}
{\color{gray}\multiput(0,0)(0,25){8}{\line(1,0){400}}}
}

%\usepackage{tikz}

%\pagestyle{headandfoot}
%\firstpageheader{\textbf{Proofs \& Problem Solving}}{\textbf{Homework 1}}{\textbf{\PSYear}}
%\runningheader{}{}{}
%\firstpagefooter{}{}{}
%\runningfooter{}{}{}

%\marksnotpoints
%\pointsinrightmargin
%\pointsdroppedatright
%\bracketedpoints
%\marginpointname{ \points}
%\totalformat{[\totalpoints~\points]}

\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\N{{\mathbb{N}}}
\def\Q{{\mathbb{Q}}}
\def\C{{\mathbb{C}}}
\def\hcf{{\rm hcf}}

%%end of my stuff

\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they

\usepackage{graphicx} % paquete que permite introducir imágenes

\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they

%\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%%hasta aquí

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{Figures/blank.png}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering Proofs and Problem Solving \\[15pt] % Book title
{\huge Week 1: Logic and the Reals}\\[20pt] % Subtitle
{\Large Notes  based on Martin Liebeck's \\ \textit{A Concise Introduction to Pure Mathematics}}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

%\newpage
%~\vfill
%\thispagestyle{empty}

%\noindent Copyright \copyright\ 2013 John Smith\\ % Copyright notice

%\noindent \textsc{Published by Publisher}\\ % Publisher

%\noindent \textsc{book-website.com}\\ % URL

%\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

%\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{Figures/blank.png} % Table of contents heading image

%\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

 \tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 1: Logic and the Reals}

\chapterimage{Figures/blank.png}

\chapter{Logic}%
\label{logic}

%
%\title{Week 1: Sets, Logic, and the Real Numbers}
%\date{}
%\maketitle
%
%
%\epigraph{\it Don't just read it; fight it! Ask your own question, look for your own examples, discover your own proofs. Is the hypothesis necessary? Is the converse true? What happens in the classical special case? What about the degenerate cases? Where does the proof use the hypothesis?}{Paul Halm\"os, from his book \\{\it I Want to be a Mathematician: An Automathography}  (1985)}
%
%
%
%\epigraph{\it I read once that the true mark of a pro — at anything — is that he understands, loves, and is good at even the drudgery of his profession.}{Paul Halm\"os, from his book \\{\it I Want to be a Mathematician: An Automathography}  (1985)}
%
%
%
%
%\maketitle

%This course will cover a diverse range of topics, some weeks having no relation to the previous week. However, this week we introduce some fundamental objects and tools that will be required for every week: logic, sets, and the real numbers.

\section{Sets}%
\label{sets}

A set $S$ is a collection of objects, which we call the \emph{elements} of $S$. Those elements can be anything: numbers, letters, animals, other sets. For example:
\begin{itemize}
\item $\mathbb{N} = \{1,2,3, \dots\}$ is the set of strictly positive integers,
\item $\mathbb{N}_0 = \{0,1,2, \dots\}$ is the set of nonnegative integers,
\item $\mathbb{Z} = \{\dots -3, -2, -1, 0,1,2,3, \dots\}$ is the set of all integers,
\item $\mathbb{Q}$ is the set of rational numbers (numbers that can be expressed as $\frac{m}{n}$ for $m \in \mathbb{Z}$ and $n \in \mathbb{N}$), and
\item $\mathbb{R}$ is the set of all real numbers.
\end{itemize}
For the moment, we're assuming that you have some familiarity with these sets:
they will help us build examples to illustrate the different logical concepts that appear below.
Later on we'll study each of these sets of numbers in much more detail.

One way of writing down a set is to list them between curly brackets. So the set whose elements are $1,2,3$ is written as
\[
S=\{1,2,3\}.
\]

Two sets are \emph{equal} if and only if they have the same elements.
That means, for instance, that $\{1,2,3\}$ is the \emph{same set} as $\{3,1,2\}$.
Consequently, when you're writing down a set, the order in which you list the elements is unimportant.

As a matter of notation, if $a$ is an element of the set $S$, we write $a\in S$. We write $a\not\in S$ if $a$ is not an element of the set $S$.
For example, $1\in \{1,2,3\}$ but $4\not\in \{1,2,3\}$.
Similarly, $\text{Clark} \notin \{1,2,3\}$.

Sets can contain anything, even other sets.
For example,  $S=\{1,\{2\}\}$ is the set consisting of the number $1$ and also the set $\{2\}$.
Thus $1\in S$, and $\{2\}\in S$, but $2\notin S$.

\begin{definition}
Let $A$ and $B$ be two sets.
We say that $A$ is a {\it subset} of $B$, or that $A$ is {\it contained} in $B$ if and only if each $x\in A$ is also an element of $B$.
We write $A \subseteq B$ to say that ``$A$ is a subset of $B$.''
If there is an element of $A$ not in $B$, we write $A\not\subseteq B$.
\end{definition}

\begin{example}
Let's write down all the subsets of the set $\{1,2,3\}$.
In all there are $8$.

There's $\{1,2,3\}$ itself.
Then there are the subsets with $2$ elements: these are $\{1,2\}$, $\{1,3\}$, and $\{2,3\}$.
(Note that $\{2,1\}$ is a set that is already on our list!)
Then there are the subsets with $1$ element (sometimes called \emph{singletons}): these are $\{1\}$, $\{2\}$, and $\{3\}$.

There is one more subset of $\{1,2,3\}$.
This is the set with $0$ elements.
This is called the \emph{empty set}.
\end{example}

\begin{definition}
The \emph{empty set}, written $\varnothing$, is the set that has no elements.
\end{definition}

\begin{example}
Recall that two sets $A$ and $B$ are equal if they have the same elements.
In other words, $A=B$ if and only if both $A \subseteq B$ and $B \subseteq A$.
\end{example}

If $A \subseteq B$ and $B \not\subseteq A$, then we say that $A$ is \emph{properly contained} in $B$.
Some people write $A \subset B$ or even $A \subsetneqq B$ to emphasize this.

\begin{example}
We have $\mathbb{N}\subseteq \mathbb{Q}$ since each integer is also a rational number, but $\mathbb{Q}\not\subseteq \mathbb{N}$ since $\frac{1}{2}\in \mathbb{Q}$ but $\frac{1}{2}\not\in \mathbb{N}$.
Thus $\mathbb{N}$ is properly contained in $\mathbb{Q}$.

In fact, we have
\[
\mathbb{N} \subseteq \mathbb{N}_0 \subseteq \mathbb{Z} \subseteq \mathbb{Q} \subseteq \mathbb{R},
\]
and all these containments are proper.

Similarly, $\{1,2\}\subseteq \{1,2,3\}$ since $1\in \{1,2,3\}$ and $2\in \{1,2,3\}$. However, $\{1,2,3\}\not\subseteq \{1,2\}$  since $3\in \{1,2,3\}$ but $3\not\in \{1,2\}$.
\end{example}

\section{Logical Statements}%
\label{logicalstatements}

By a \emph{statement}, we mean a sentence that asserts that something is the case.
Here are some statements:
\begin{itemize}
    \item ``$2$ is even.''
    \item ``$937$ is prime.''
    \item ``$57$ is prime.''
    \item ``The only subset of $\varnothing$ is $\varnothing$ itself.''
    \item ``There are two points on opposite sides of the equator of the Earth where the temperatures are identical.''
\end{itemize}
Sentences like ``go to bed'' or ``why so serious?'' are not statements, because they don't assert that something is the case.

Every statement is either True or False.
That is, to any statement $A$, there is one and only one corresponding element of the set $\{True, False\}$.
This is called the \emph{truth-value} of $A$.
Thus the truth value of ``$2$ is even'' is True; the truth-value of ``$937$ is prime'' is True; the truth-value of ``$57$ is prime'' is False; and the truth-value of ``the only subset of $\varnothing$ is $\varnothing$ itself'' is True.

Let's emphasize that there are plenty of statements whose truth-value is unknown to us.
That is, we just don't know whether they are true or false.
For instance, ``Every even integer greater than $2$ can be written as the sum of exactly two primes'' is a statement, but we don't know whether this sentence is True or False.
(This particular statement is known as the \emph{Goldbach Conjecture}.)

Statements that include a \emph{free variable} are useful in describing subsets of existing sets.
For example, let's say we write down the set of integer multiples of $5$.
If $n$ is an integer, then we can consider the statement
\[
    A(n) = \text{``$n$ is divisible by $5$''}.
\]
Now what we want to do is to look at the set of integers $n$ such that $A(n)$ holds.
This is a subset of the set $\mathbb{Z}$ of integers;
here's how we write it:
\[
    S = \{n \in \mathbb{Z} : A(n)\} = \{n \in \mathbb{Z} : n \text{ is divisible by 5}\} \subseteq \mathbb{Z}.
\]
This reads ``$S$ is the set of integers $n$ such that $n$ is divisible by $5$.''
This is called \emph{set-builder notation}.
The colon $:$ reads as ``such that'';
some authors use a vertical bar $|$ for this, so that the sentence above could be rewritten
\[
S=\{n\; |\; n \text{ is divisible by 5}\}.
\]

Informally, $S$ is the set
\[
    \{ \dots, -10, -5, 0, 5, 10, 15, \dots \},
\]
but the set-builder notation above is generally preferable to this informal description, because it isn't always clear what the pattern is.
For example: if we write
\[
    \{3, 5, 7, \dots \},
\]
is that the set of odd numbers greater than $1$, or the set of odd prime numbers?

\begin{example}
If $x$ is a real number, then let $A(x)$ be the statement ``$x\geq 0$.''
Now we can look at the subset
\[
    \{x \in \mathbb{R} : A(x)\} = \{x \in \mathbb{R} : x \geq 0 \} \subseteq \mathbb{R}
\]
That's the set of nonnegative real numbers.
\end{example}

\begin{example}
Consider the sets
\[
A=\{x\in\mathbb{R} : x^2= 1\} \quad \text{and} \quad B=\{-1,1\}.
\]
These are two subsets of $\mathbb{R}$.
In fact, we claim $A=B$.
To prove this, we will first prove that $A \subseteq B$ and then that $B \subseteq A$:
\begin{enumerate}[label=(\alph*)]
\item If $x\in A$, then $x^2=1$, so $0=1-x^2=(1-x)(1+x)$. The only way the product of two numbers is zero is if at least one of them is zero, so $1-x=0$ or $1+x=0$, i.e. $x=1$ or $x=-1$, hence $x\in B$.
\item If $x\in B$, then $x=1$ or $x=-1$, and in either case $x^2=1$, so $x\in A$.
\end{enumerate}
\end{example}

\begin{example}
Let $S=\{x\in\mathbb{R} : x^2=-1\}$. The sentence ``$x^2=-1$ has no solutions in $\mathbb{R}$'' is the natural language translation of the sentence ``$S=\varnothing$.''
\end{example}

\bigskip

\noindent {\bf Warning.} In order for set-builder notation $\{x \in S : A(x)\}$ to be meaningful, it has to be the case that the statement $A(x)$ makes sense for all elements $x \in S$.
For example, if $S$ is the set of all trees, then the set
\[
    \{ x \in S : x^2 = 1\}
\]
doesn't make any sense, because we don't know how to take the square of a tree, or what it means for such a square to be equal to $1$.
That may seem like a silly point, but this basic error leads to lots of mistakes!

\section{Conjunction}%
\label{conjunction}
One can combine statements together to get new statements.

\begin{definition}
If $A$ and $B$ are statements, their \emph{conjunction} is the statement $A \text{ and }  B$.
Some authors use the notations $ A \wedge B $ or $A \& B$ for the conjunction of $A$ and $B$.
For example, if $A$ is the statement ``$2$ is even'' and $B$ is the statement ``$57$ is prime,'' the statement $A \wedge B$ asserts that ``$2$ is even and $57$ is prime.''
The truth-value of the conjunction operates like this:
\[
A \wedge B \text{ is true exactly when both }A\text{ is true and }B\text{ is true}
\]
\end{definition}

To unpack this, we use of a \emph{truth table}.
When we combine statements $A$, $B$, $C$, etc. to get a statement $P$, a truth table is then a list of the truth-values of $P$ depending on the truth-values of $A$ and $B$.
Here is the truth table for the conjunction:
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $B$ & $A \wedge B$  \\ \hline
        T & T & T \\
        T & F & F \\
        F & T & F \\
        F & F & F \\
    \end{tabular}
\end{center}

\begin{example}
From our truth table, we can see that the truth-value of ``$2$ is even and $57$ is prime'' is False.
\end{example}

\section{Intersection}%
\label{intersection}

\begin{definition}
Let $S$ and $T$ be two sets.
The \emph{intersection} $S \cap T$ is the set consisting of the elements $S$ and $T$ have in common.
That is, $x \in S \cap T$ exactly when both $x \in S$ and $x \in T$.
One can write
\[
    S \cap T = \{x : (x \in S) \wedge (x \in T)\}
\]
\end{definition}

When $S$ and $T$ are subsets of some common set $U$ and are given by set-builder notation, we can use conjunction to describe the intersection.
That is, if
\[
    S = \{x \in U : A(x) \}
\]
for a statement $A(x)$, and if
\[
    T = \{x \in U : B(x) \}
\]
for a statement $B(x)$, then the intersection is given by
\[
    S \cap T = \{x \in U : A(x) \wedge B(x)\}.
\]

\begin{example}
Consider the sets
\[
    S = \{n \in \mathbb{Z} : n\text{ is divisible by }2\}\quad \text{and}\quad T = \{n \in \mathbb{Z} : n\text{ is divisible by }5\}.
\]
Then the intersection is
\[
    S \cap T = \{n \in \mathbb{Z} : n\text{ is divisible by both }2\text{ and }5\}.
\]
From this you can see that:
\[
    S \cap T = \{n \in \mathbb{Z} : n\text{ is divisible by }10\}=\{\dots,-20,-10,0,10,20,\dots\}.
\]
\end{example}

\begin{example}
Consider the sets
\[
    S = \{x \in \mathbb{R} : x \geq 0\}
\]
and
\[
    T = \{x \in \mathbb{R} : x \leq 1\}.
\]
Then the intersection is
\[
    S \cap T = \{x \in \mathbb{R} : 0 \leq x \leq 1 \},
\]
the set of real numbers between $0$ and $1$ (including $0$ and $1$).
(This set is called the \emph{closed interval} from $0$ to $1$, and it is often written as $[0,1]$.)
\end{example}

\begin{example}
Consider the sets
\[
    S = \{x \in \mathbb{R} : x \geq 1\}
\]
and
\[
    T = \{x \in \mathbb{R} : x \leq 0\}.
\]
Then the intersection is
\[
    S \cap T = \{x \in \mathbb{R} : (x \geq 1)\wedge(x \leq 0) \} = \varnothing.
\]
\end{example}

\section{Disjunction}%
\label{disjunction}

\begin{definition}
The \emph{disjunction} of two statements $A$ and $B$ is the statement $A\text{ or } B$.
This is also sometimes written as $A \vee B$.

The truth-value of the disjunction operates like this:
\[
A \vee B \text{ is true exactly when either }A\text{ is true or }B\text{ is true}
\]
\end{definition}

Here is the truth table for the disjunction:
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $B$ & $A \vee B$  \\ \hline
        T & T & T \\
        T & F & T \\
        F & T & T \\
        F & F & F \\
    \end{tabular}
\end{center}
Note that ``or'' in this sense is not taken in the \emph{exclusive} sense: if both $A$ and $B$ are both true, then $A\vee B$ is true as well.

\begin{example}
From this we can see that the truth-value of ``$2$ is even or $57$ is prime'' is True.
\end{example}

\bigskip

\noindent Once you have these basic connectives, you can combine statements to your heart's content, and you can analyze the results.
\begin{example}
For example, let's have a look at the statement $(A \vee B) \wedge C$.
Let's analyze the truth table by dealing with what's in parentheses first, and then combining that with the statement $C$:
\begin{center}
    \begin{tabular}{c|c|c|c|c}
        $A$ & $B$ & $C$ & $A \vee B$ & $(A \vee B) \wedge C$ \\ \hline
        T & T & T & T & T\\
        T & T & F & T & F\\
        T & F & T & T & T\\
        T & F & F & T & F\\
        F & T & T & T & T\\
        F & T & F & T & F\\
        F & F & T & F & F\\
        F & F & F & F & F\\
    \end{tabular}
\end{center}

This is the kind of sentence that we don't say too often in natural language (at least, outside of the legal profession): it asserts that both $A \vee B$ is true and $C$ is true.
Maybe a reasonable way to say this in English is ``Either $A$ or $B$ is true, and $C$ is true too.''
\end{example}

\begin{example}
The previous example brings up a critical issue.
\emph{We must write our connectives carefully!}
For example, consider the statement
\[
\text{``$57$ is prime or $2$ is  and $937$ is prime.''}
\]
What do we mean by this statement?
As written, it's ambiguous, and that ambiguity affects the truth-value!

Parentheses clarify things:
the statement
\[
\text{``($937$ is prime or $2$ is even) and $57$ is prime''}
\]
is false, whereas the statement
\[
\text{``$937$ is prime or ($2$ is even and $57$ is prime)''}
\]
is true.

Here's the truth table for $A \vee (B \wedge C)$;
notice the differences from the truth table for $(A \vee B) \wedge C$ above:
\begin{center}
    \begin{tabular}{c|c|c|c|c}
        $A$ & $B$ & $C$ & $B \wedge C$ & $A \vee (B \wedge C)$ \\ \hline
        T & T & T & T & T\\
        T & T & F & F & T\\
        T & F & T & F & T\\
        T & F & F & F & T\\
        F & T & T & T & T\\
        F & T & F & F & F\\
        F & F & T & F & F\\
        F & F & F & F & F\\
    \end{tabular}
\end{center}
\end{example}

\section{Union}%
\label{union}

\begin{definition}
Let $S$ and $T$ be two sets.
The \emph{union} $S \cup T$ is the set consisting of the elements of either $S$ or $T$.
That is, $x \in S \cup T$ exactly when both $x \in S$ or $x \in T$.
One can write
\[
    S \cup T = \{x : (x \in S) \vee (x \in T)\}
\]
\end{definition}

When $S$ and $T$ are subsets of some common set $U$ and are given by set-builder notation, we can use disjunction to describe the union.
That is, if
\[
    S = \{x \in U : A(x) \}
\]
for a statement $A(x)$, and if
\[
    T = \{x \in U : B(x) \}
\]
for a statement $B(x)$, then the union is given by
\[
    S \cup T = \{x \in U : A(x) \vee B(x)\}.
\]

\begin{example}
Consider the sets
\[
    S = \{n \in \mathbb{Z} : n\text{ is divisible by }2\}\quad \text{and}\quad T = \{n \in \mathbb{Z} : n\text{ is divisible by }5\}.
\]
Then the union is
\[
    S \cup T = \{n \in \mathbb{Z} : n\text{ is divisible by either }2\text{ or }5\}.
\]
We can write out a few of the elements:
\[
    S \cup T =\{\dots,-10,-8,-6,-5,-4,-2,0,2,4,5,6,8,10,12,14,15,\dots\}.
\]
\end{example}

\begin{example}
Consider the sets
\[
    S = \{x \in \mathbb{R} : x \geq 0\}
\]
and
\[
    T = \{x \in \mathbb{R} : x \leq 1\}.
\]
Then the union is
\[
    S \cup T = \{x \in \mathbb{R} : (x \geq 0) \vee (x \leq 1) \} = \mathbb{R}.
\]
\end{example}

\begin{example}
Consider the sets
\[
    S = \{x \in \mathbb{R} : x \geq 1\}
\]
and
\[
    T = \{x \in \mathbb{R} : x \leq 0\}.
\]
Then the union is
\[
    S \cup T = \{x \in \mathbb{R} : (x \geq 1)\vee(x \leq 0) \},
\]
the set of real numbers that are either at least as big as $1$, or no greater than $0$.
\end{example}

\section{Negation}%
\label{negation}

The \emph{negation} of a statement $A$ (which we denote $\overline{A}$ or $\neg A$) is the statement ``$A$ is false.''
For example, the negation of the statement ``$57$ is prime'' is ``$57$ is not prime.''
The negation of the statement ``$57$ is not prime'' is ``$57$ is prime.''
Depending on the statement, its negation can be written differently from just inserting a ``not'':
\begin{enumerate}[label=(\alph*)]
\item If $S$ is a set, and $s$ is not an element of $S$, then the sentence ``$s \notin S$'' is the negation of the sentence ``$s \in S$.''
\item If $B$ is the statement ``$x = y$,'' then the negation $\overline{B}$ is the sentence ``$x \neq y$.''
\item If $C=``a=b''$, then $\bar{C}=``a\neq b''$.
\end{enumerate}

The negation of a statement has the opposite truth value: if $A$ is true, then $\overline{A}$ is false; if $A$ is false, then $\overline{A}$ is true.
\begin{center}
    \begin{tabular}{ c|c}
        $A$ & $\overline{A}$   \\ \hline
        T & F \\
        F & T \\
    \end{tabular}
\end{center}

\begin{example}
For any statement $A$, consider the statement $A \vee \overline{A}$.
That's the statement that either $A$ is true or $A$ is not true.
In other words, it's the assertion that either $A$ is true or false.
Here's the truth table:
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $\overline{A}$ & $A \vee \overline{A}$   \\ \hline
        T & F & T \\
        F & T & T\\
    \end{tabular}
\end{center}
Note that we only have T's in the last column.
That means that the sentence $A \vee \overline{A}$ is always true, no matter what the truth-value of $A$ is.
This statement is sometimes called the \emph{Law of Excluded Middle}.
\end{example}

\begin{example}
Now let's contemplate the statement $A \wedge \overline{A}$.
Here's the truth table:
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $\overline{A}$ & $A \vee \overline{A}$   \\ \hline
        T & F & F \\
        F & T & F\\
    \end{tabular}
\end{center}
Note that we only have F's in the last column.
That means that $A \wedge \overline{A}$ is always false, no matter what the truth-value of $A$ is.
This statement is sometimes just called a \emph{Contradiction}.
\end{example}

\section{Complements}%
\label{complements}

\begin{definition}
Let $S$ be a set, and let $T \subseteq S$ be a subset.
Then the \emph{complement} of $T$ in $S$ is the subset
\[
    S \setminus T = \{ x \in S : x \notin T \} \subseteq S.
\]
\end{definition}

Assume that the subset $T \subseteq S$ is identified using set-builder notation:
\[
    T = \{x \in S : A(x) \}.
\]
Then we can do the same for its complement by using negation:
\[
    S \setminus T = \{x \in S : \overline{A}(x)\}.
\]

\begin{example}
Consider the set of even integers:
\[
    T = \{n \in \mathbb{Z} : 2\text{ divides }n\} \subseteq \mathbb{Z}.
\]
Its complement in $\mathbb{Z}$ is the set of odd integers:
\[
    \mathbb{Z} \setminus T = \{n \in \mathbb{Z} : 2\text{ does not divide }n\} \subseteq \mathbb{Z}.
\]
\end{example}

\begin{example}
Consider the subset
\[
    S = \{ x \in \mathbb{R} : x \geq 0 \},
\]
the set of real numbers no less than $0$.
Its complement in $\mathbb{R}$ is
\[
    \mathbb{R} \setminus S = \{ x \in \mathbb{R} : x < 0 \},
\]
the set of real numbers less than $0$.
\end{example}

\section{Conditionals}%
\label{conditionals}

A \emph{conditional} is a statement of the form ``if $A$ then $B$,'' or ``$A$ only if $B$,'' or ``$A$ implies $B$.''
We write this symbolically as
\[
    A \implies B.
\]
This statement says that whenever $A$ is true, we can conclude $B$ is true.
Statement $A$ is called the \emph{premise} of the conditional, and $B$ is its \emph{conclusion}.
We can also write this backwards as $B \Longleftarrow A$, which is read ``$B$ if $A$.''

Here is the truth table for the conditional
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $B$ & $A \implies B$  \\ \hline
        T & T & T \\
        T & F & F \\
        F & T & T \\
        F & F & T \\
    \end{tabular}
\end{center}
Notice that if the statement $B$ is true, then the statement $A \implies B$ is true.
If $A$ is false, then the statement $A \implies B$ is again true.
The only way $A \implies B$ ends up false is when $A$ is true but $B$ isn't.

\begin{example}
The statement ``If $2$ is even, then $14$ is even'' is true.

The statement ``If $2$ is even, then $15$ is even'' is false.

The statement ``If $57$ is prime, then $937$ is prime'' is true.

The statement ``If $57$ is prime, then $2$ is odd'' is true.
\end{example}

This sometimes surprises people a little, because in English (and in other natural languages), we are a little more casual in our use of the words ``if $A$, then $B$.''
In mathematics, the statement $A \implies B$ can be re-expressed as the following:
\[
    \text{``either $B$ is true or $A$ is false.''}
\]
Indeed, $A \implies B$ has exactly the same truth table as $\overline{A} \vee B$.

To prove a conditional statement $A\Longrightarrow B$ \emph{directly}, you proceed as follows:
\begin{enumerate}[label=(\alph*)]
\item Start your proof by \emph{assuming} the premise $A$.
\item Then deduce the conclusion $B$.
\end{enumerate}

\begin{example}
Consider the following statement
\[
\text{``If $n$ is an odd integer, then $n^2$ is odd.''}
\]
The premise in this example is ``$n$ is an odd integer'' and the conclusion is ``$n^2$ is odd.''
So to prove it, we need to assume the premise and then deduce the conclusion.
\begin{proof}
Assume $n$ is an odd number.
By definition this means $n=2k+1$ for some integer $k$.
Then
\[
n^2=(2k+1)^2=4k^2+4k+1=2(2k^2+2k)+1.
\]
Consequently $n^2$ is odd.
\end{proof}
\end{example}

\begin{example}
If $A\implies B$ and $B\implies C$, then $A\implies C$.
In other words, $((A \implies B) \wedge (B \implies C)) \implies (A \implies C)$.

The premise is $(A\implies B)\wedge(B\implies C)$.
The conclusion is $A\implies C$.
\begin{proof}
Assume $(A\implies B)\wedge(B\implies C)$.
That is, both $A \implies B$ and $B \implies C$ are true.

We must deduce that $A\implies C$.
To do this, assume $A$ is true.
We aim to show that $C$ is true.

Since $A$ is true, and since $A\implies B$, we conclude $B$ is true.
Since $B$ is true, and since $B\implies C$, we conclude $C$ is true.

We have now shown that if $A$ is true, then $C$ is true, thus $A\implies C$.
\end{proof}
\end{example}

\begin{definition}
The \emph{converse} to the conditional $A \implies B$ is the statement $B \implies A$.
\end{definition}

Note that the converse of a conditional is \emph{not the same statement}.
Compare the truth tables:
\begin{center}
    \begin{tabular}{c|c|c|c}
        $A$ & $B$ & $A \implies B$ & $B \implies A$  \\ \hline
        T & T & T & T \\
        T & F & F & T \\
        F & T & T & F \\
        F & F & T & T \\
    \end{tabular}
\end{center}
The conditional ``if $2$ is odd, then $937$ is prime'' is true (if a little strange).
The converse ``if $937$ is prime, then $2$ is odd'' is false!

\begin{definition}
The \emph{contrapositive} of a conditional $A \implies B$ is the statement $\overline{B} \implies \overline{A}$.
\end{definition}

The contrapositive really is just a repackaging of the original conditional.
It has exactly the same truth table:
\begin{center}
    \begin{tabular}{c|c|c|c|c|c}
        $A$ & $B$ & $A \implies B$ & $\overline{A}$ & $\overline{B}$ & $\overline{B} \implies \overline{A}$  \\ \hline
        T & T & T & F & F & T \\
        T & F & F & F & T & F \\
        F & T & T & T & F & T \\
        F & F & T & T & T & T \\
    \end{tabular}
\end{center}

\section{Logical equivalence}%
\label{logicalequivalence}

This idea of ``repackaging'' a sentence in another way is called \emph{logical equivalence}.

\begin{definition}
    We say that two statements $A$ and $B$ are \emph{equivalent} if $A$ is true if and only if $B$ is true.
\end{definition}

One also says ``$A$ if and only if $B$'' or ``$A$ iff $B$'' or $A \iff B$.
Here is the truth table for this connective:
\begin{center}
    \begin{tabular}{ c|c|c}
        $A$ & $B$ & $A \iff B$  \\ \hline
        T & T & T \\
        T & F & F \\
        F & T & F \\
        F & F & T \\
    \end{tabular}
\end{center}

In other words, $A \iff B$ is the same statement as $(A\implies B) \wedge (B \implies A)$.

Equivalence behaves like ``='' does in algebra: you are free to swap one statement for another equivalent statement.
For example, if $A\iff B$, then the statement $A \vee C$ is equivalent to the statement $B \vee C$.

\begin{example}
Let $A$ be a statement.
The \emph{double negation} of $A$ is the statement $\overline{\overline{A}}$.
Then $\overline{\overline{A}}$ is equivalent to $A$.
In other words, $\overline{\overline{A}} \iff A$.
Indeed, $A$ is true if and only if $\overline{A}$ is false, which happens if and only if $\overline{\overline{A}}$ is true.
\end{example}

To prove $A\iff B$, you'll usually have to prove \emph{both} $A\implies B$ and $B\implies A$.

\begin{theorem}
Let $A$ and $B$ be statements.
Then $A \implies B$ is equivalent to $\overline{A} \vee B$.
That is, $(A \implies B) \iff (\overline{A} \vee B)$.
\end{theorem}

\begin{proof}
We begin with the forward direction;
that is, we shall show first that $(A \implies B) \implies (\overline{A} \vee B)$.
So assume $A\implies B$.
We aim to show $\overline{A} \vee B$.

There are two options for $A$: it is either true or false.
Let's consider these options in turn:
\begin{itemize}
\item If $A$ is false, then $\overline{A}$ is true, and so certainly $\overline{A} \vee B$ is true as well.
\item If $A$ is true, then since we are assuming $A\implies B$, we can conclude $B$.
Thus $\overline{A} \vee B$ is true as well.
\end{itemize}
Either way, we see that $\overline{A} \vee B$ holds, so it follows that $A\implies B$ implies $\overline{A} \vee B$.

Now we prove the reverse direction;
that is, we shall show that $(\overline{A} \vee B) \implies (A \implies B)$.
Assume $\overline{A} \vee B$.
We aim to show that $A\Rightarrow B$ is true.
To prove this, we assume that $A$ is true, and we aim to conclude $B$.
But we are \emph{assuming} that either $\overline{A}$ is true or $B$ is true.
Since $A$ is true, $\overline{A}$ is false.
So $B$ must be true.
Thus, we conclude $A\implies B$, and so we have shown that $\overline{A} \vee B$ implies $A \implies B$.
\end{proof}

\begin{theorem}
Let $A$ and $B$ be statements.
Then $A \implies B$ is equivalent to $\overline{B} \implies \overline{A}$.
That is, $(A \implies B) \iff (\overline{A} \vee B)$.
\end{theorem}

\begin{proof}
We begin with the forward direction;
that is, we shall show first that $(A \implies B) \implies (\overline{B} \implies \overline{A})$.
So assume $A\implies B$.
We aim to show that $\overline{B} \implies \overline{A}$.
So assume $\overline{B}$; that is, that $B$ is false.

The previous theorem shows that $A \implies B$ is equivalent to $\overline{A} \vee B$.
Since $B$ is false, the only remaining option is that $\overline{A}$ holds.
We have thus shown that $\overline{B} \implies \overline{A}$.

Now we prove the reverse direction;
that is, we shall show that $(\overline{B} \implies \overline{A}) \implies (A \implies B)$.
So assume that $\overline{B} \implies \overline{A}$.
We aim to show that $A \implies B$.
So assume $A$.
We aim to deduce $B$.

By the previous theorem, $\overline{B} \implies \overline{A}$ is equivalent to $\overline{\overline{B}} \vee \overline{A}$, which is in turn equivalent to $B \vee \overline{A}$.
Since $A$ is true, $\overline{A}$ is false, and so the only option remaining is that $B$ holds.
Thus we have shown that $A \implies B$.
\end{proof}

\section{Proof by Contradiction}%
\label{proofbycontradiction}

Suppose we have a statement $A$ that we suspect is false.
How can we prove that?
One approach is to leverage a statement $B$ that we \emph{know} is false, and try to prove that $A \implies B$.
Remember, $A \implies B$ is the same thing as $\overline{A} \vee B$,
so if we know that $B$ is false, then the only alternative is that $A$ is false.
This is called proof by contradiction.

\begin{theorem}
Let $A$ and $B$ be statements.
Then $\overline{A \vee B}$ is equivalent to $\overline{A} \wedge \overline{B}$.
\end{theorem}

\begin{proof}
We begin with the forward direction;
that is, we shall show first that if $\overline{A \vee B}$, then $\overline{A} \wedge \overline{B}$.
So let us assume $\overline{A \vee B}$.
We aim to prove that both $\overline{A}$ and $\overline{B}$.

In other words, we need to show that $A$ is \emph{false}.
So we want to prove that $A$ implies something we know to be false.
In our case, we are assuming that $\overline{A \vee B}$, so we know $A \vee B$ to be false.
This is great news: if $A$ is true, then certainly $A \vee B$ is true.
That is, $A \implies (A \vee B)$.
Since we know that $A \vee B$ is false, that implies that $A$ is false as well.
Thus $\overline{A}$ holds.
Similarly, $B \implies (A \vee B)$, but that contradicts our assumption that $\overline{A \vee B}$ is true.
Thus $\overline{B}$ holds.

Now let's reverse directions;
that is, let's show that if $\overline{A} \wedge \overline{B}$, then $\overline{A \vee B}$.
So let us assume that both $\overline{A}$ and $\overline{B}$.
We aim to prove that $\overline{A \vee B}$.

In other words, we need to show that $A \vee B$ is \emph{false}.
If $A \vee B$, there are two options:
\begin{itemize}
    \item One possibility is that $A$ is true. But this would contradict our assumption that $\overline{A}$ is true.
    \item So that leaves the other possibility, which is that $B$ is true. But that contradicts our other assumption, that $\overline{B}$ is true.
\end{itemize}
Either way, we arrive at a contradiction, and so $A\vee B$ is indeed false.
\end{proof}

As a quick corollary of this, we deduce that
\[
    \overline{A \wedge B} \iff \overline{\overline{\overline{A}} \wedge \overline{\overline{B}}} \iff \overline{\overline{\overline{A} \vee \overline{B}}} \iff (\overline{A} \vee \overline{B}).
\]

Here's another example of proof by contradiction.
\begin{example}
Let's say we want to prove that $\sqrt{3}< 1+\sqrt{2}$.

\begin{proof}
Let's prove this by contradiction: we assume instead that $\sqrt{3}\geq 1+\sqrt{2}$, and
we aim to deduce something we know to be false.
If we square both sides of the inequality we assumed, we obtain
\[
3\geq (1+\sqrt{2})^2=1+2\sqrt{2}+2=3+\sqrt{2}>3,
\]
which is definitely false.
We have thus shown by contradiction that $\sqrt{3}< 1+\sqrt{2}$.
\end{proof}
\end{example}

Why didn't we do the same computations but with the original inequality?
If we assumed $\sqrt{3}< 1+\sqrt{2}$, then squaring both sides gives
\[
    3< (1+\sqrt{2})^2=1+2\sqrt{2}+2=3+\sqrt{2}
\]
which is certainly true, but this doesn't help us.
We started by assuming the thing we wanted to prove and then deducing something we know to be true.
if $A$ is the statement ``$\sqrt{3}< 1+\sqrt{2}$'' and $B$ is the statement ``$3<3+\sqrt{2}$,'' then what we just showed was $A\implies B$, but that can't help us conclude that $A$ is true.
In our proof of $A$ by contradiction, we assumed first that $A$ is \emph{false}, and then we deduce the statement $C$: ``$3>3$,'' which we know perfectly well is false.
Since $C$ is false, the implication $\overline{A} \implies C$ ensures that $\overline{A}$ has to be false as well; in other words, $A$ is true.

\section{Quantifiers}%
\label{quantifiers}

Sometimes a statement tells us about a number of things satisfying a certain property.
For example, consider the two statements ``$x^2=4$'' and ``there is an integer $x$ so that $x^2=4$.''
The first statement asserts some condition about a \emph{particular} $x$, whereas the second statement says there is \emph{at least one} $x$ satisfying this condition.
Words like ``there is,'' ``there exists,'' and ``for some'' are called \emph{existential quantifiers}, and we write $\exists$ for short. So the statement
\[
\text{there is an integer $x$ such that $x^2=4$}
\]
can be written more succinctly as
\[
(\exists x\in\mathbb{Z})(x^2=4).
\]
(Some authors will write this a little more informally, as ``$\exists x \in \mathbb{Z} \text{ st } x^2 = 4$,'' where the ``st'' is shorthand for ``such that.'')

To prove an existentially quantified sentence $(\exists x\in S)(A(x))$, you generally have to \emph{construct} or \emph{find} an $x\in S$ for which $A(x)$ holds.
\begin{example}
    To prove the statement $(\exists x\in\mathbb{Z})(x^2=4)$, we have to construct an $x \in \mathbb{Z}
    $ whose square is $4$.
    But this is not too hard: $x = 2$ has this property.

    Note that this sentence didn't say anything about the question of whether $x=2$ is the \emph{only} integer that satisfies this condition.
    (And of course, it isn't!)
\end{example}

If $S$ is a set, and $A(x)$ is a statement with variable $x$, then the sentence $(\exists x \in S)(A(x))$ is equivalent to the sentence ``the set $\{x \in S : A(x)\}$ is not the empty set.''

Some statement assert that \emph{every} $x \in S$ satisfies some condition.
Terms like ``for every'' and ``for all'' are called \emph{universal quantifiers}.
Symbolically, we just write $\forall$ to mean any one of these.
So the statement
\[
\text{every positive integer $n$ is less than or equal to its square}
\]
can be written
\[
(\forall n\in\mathbb{N})(n\leq n^2).
\]

To prove a universally quantified sentence $(\forall x \in S)(A(x))$, you generally have to contemplate an arbitrary element $x$ of $S$ and to prove that $A(x)$ holds for it.
\begin{example}
    To prove the statement $(\forall n \in \mathbb{N})(n \leq n^2)$, we start with the word ``let.''
    Let $n \in \mathbb{N}$.
    Then $n-1$ is an integer, and $n-1\geq 0$.
    Thus $n^2-n = n(n-1) \geq 0$.
    Consequently, $n^2 \geq n$, as desired.
\end{example}

Most mathematical statements involve many quantifiers at once.
For example, the statement that ``every positive real number is the square of some negative real number'' has two quantifiers.
To write it, let $P = \{ x \in \mathbb{R} : x > 0\}$, the set of positive real numbers, and let $N = \{ x \in \mathbb{R} : x < 0\}$, the set of negative real numbers.
Now our sentence can be written:
\[
    (\forall x \in P)(\exists y \in N)(y^2 = x).
\]
\begin{example}
    Let's define the \emph{successor} to a natural number $n$ as the smallest natural number that is larger than $n$.
    Let's consider the sentence ``every natural number has a successor.''

    Let's unpack this sentence in stages.
    First, we have
    \[
        (\forall n \in \mathbb{N})(\text{there is a natural number $k$ such that $k$ is a successor to $n$}).
    \]
    What we have there in parentheses can itself be unpacked;
    now our sentence is
    \[
        (\forall n \in \mathbb{N})(\exists k \in \mathbb{N})(\text{$k$ is a successor to $n$}).
    \]
    Going even farther, this becomes
    \[
        (\forall n \in \mathbb{N})(\exists k \in \mathbb{N})(\forall m \in \mathbb{N})((m>n) \implies (n<k\leq m)).
    \]
    This is about as unpacked as you can get.

    The advantage of unpacking sentences like this is that you can now prove this claim by following the order of the quantifiers, from left to right:
    \begin{enumerate}[label=(\alph*)]
        \item The first quantifier is universal, so the first sentence of our proof has to be: ``Let $n \in \mathbb{N}$.''
        \item The second quantifier is existential, so we have to \emph{specify} or \emph{construct} the successor $k$. We are free to \emph{use} $n$ in our definition of $k$ (in this case we'll \emph{have} to), and our $k$ will be defined so that the condition that ``for any natural number $m>n$, one has $n<k\leq m$'' holds.
        \item Finally, we have to prove that condition, which is our third quantifier, which is universal. So that means we have to begin this portion of the proof with the words: ``Let $m \in \mathbb{N}$.''
    \end{enumerate}

    Let's follow that recipe to write our proof.
    \begin{proof}
        Let $n \in \mathbb{N}$.
        We aim to construct a successor $k$;
        so define $k$ as $n+1$.
        Now we aim to prove that the condition holds.
        So let $m \in \mathbb{N}$.
        Assume $m > n$.
        Thus $m - n$ is an integer and $m-n>0$.
        That implies that $m-n\geq 1$.
        Thus $m \geq n+1=k$.
        Also, $n< n+1=k$, so we deduce that $n< k \leq m$, just as we wanted.
    \end{proof}
\end{example}

It's a good idea to reflect on what happened here.
Often, the hardest part about proving a statement is getting precise control over what the statement actually \emph{is}.
In particular, it should always be possible for you to convert a mathematical statement into a series of $\forall$'s and $\exists$'s with logical connectives relating simple statements.
Doing this in the right order, and manipulating the resulting expressions carefully, is at the heart of doing pure mathematics.

\bigskip

{\bf How do we negate a statement with a quantifier?}
Consider the statement ``every person in this room has blue eyes.''
What would it take for this statement to be false?
You need at least one person in the room to \emph{not} have blue eyes, and you're in business!
That is, the statement ``there is some person in this room who does not have blue eyes'' is the negation of the statement ``every person in this room has blue eyes.''
Note that the original statement has the universal quantifier ``every'' and its negation has the existential quantifier ``there is some,'' and the last part of the sentence ``has blue eyes'' become ``does not have blue eyes.''

So to negate a statement with a quantifier, we can ``push'' the negation past the quantifier, and as we do, $\forall$ turns into $\exists$, and $\exists$ turns into a $\forall$.
It's maybe a little easier to see this if we use the $\neg$ notation for negation.
Thus,
\[
    \neg(\forall x \in S)(A(x)) \iff (\exists x \in S)(\neg A(x)).
\]
Similarly,
\[
    \neg(\exists x \in S)(A(x)) \iff (\forall x \in S)(\neg A(x)).
\]

\begin{example}
Consider the statement
\[
    (\forall n \in \mathbb{N})(\exists m \in \mathbb{N})(m^2 = n).
\]
This statement isn't true, so let's write down its negation.
\begin{center}
\[
\neg(\forall n\in\mathbb{N})(\exists m\in\mathbb{N})(m^2=n)
\]
\[\Updownarrow\]
\[
(\exists n\in\mathbb{N})(\neg(\exists m\in\mathbb{N})(m^2=n))
\]
\[\Updownarrow\]
\[
(\exists n\in\mathbb{N})(\forall m\in\mathbb{N})\neg(m^2=n)
\]
\[\Updownarrow\]
\[
(\exists n\in\mathbb{N})(\forall m\in\mathbb{N})(m^2\neq n).
\]
\end{center}

The reason it's important to be able to do this is that if our aim is to \emph{disprove} the sentence $(\forall n \in \mathbb{N})(\exists m \in \mathbb{N})(m^2 = n)$, then this unpacking shows us what we need to do.
We have to \emph{specify} or \emph{construct} a natural number $n$; let's take $n=2$.
Then we have to show that for every natural number $m$, it is not the case that $m^2 = n$.
In this case, we can note that $1^2 = 1 < 2$, and for any natural number $m \geq 2$, we have $m^2 > 2^2 = 4 > 2$.
\end{example}

\noindent {\bf All this work to negate quantifiers! Do we need to show the above individual steps when negating in our homework?} No! Don't worry, in your homework you won't be expected to give the full justification of why the negation of some statement is what it is. But, especially for complicated statements, it's important to get keep this straight!

\section{Exercises}%
\label{logicexercises}

The relevant exercises in Liebeck's book are at the end of Chapter 1.

\begin{exercise}
Find statements $A$ and $B$ so that $A\Rightarrow B$ is true but
\begin{enumerate}[label=(\alph*)]
\item $A$ is false and $B$ is true.
\item $B$ is false.
\end{enumerate}
\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item
Recall that $A\Rightarrow B$ is equivalent to $\bar{A}$ or $B$, so we just need to find a statement so that $A$ is false (so $\bar{A}$ is true) and $B$ is true. This could be $A$=``2 is odd'' and $B$=``2 is even.''
\item Recall that $A\Rightarrow B$ is equivalent to $\bar{A}$ or $B$, so if $B$ is false, then $\bar{A}$ must be true (i.e. $A$ is false). We  can  let $A=B$=``2 is odd'', for example.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Show that $A\Rightarrow B$ is not equivalent to $A\Leftarrow B$, either by finding examples or using their truth tables.
\begin{solution}
One could have A=``$n$ is a prime bigger than 2'' and B=``$n$ is odd.''

Recall that $A\Rightarrow B$ is equivalent to ``$\bar{A}$ or $B$,'' and  $B\Rightarrow A$ is equivalent to ``$\bar{B}$ or $A$,'' which have truth tables
\begin{center}
\begin{multicols}{2}
\begin{tabular}{ c|c|c|c}
$A$ & $B$ & $\bar{A}$ & $\bar{A}$ or ${B}$  \\ \hline
T & T & F  & T \\
T & F & F  & F \\
F & T & T  & T \\
F & F &  T  &  T \\
\end{tabular}

\begin{tabular}{ c|c|c|c}
$A$ & $B$ & $\bar{B}$ & $\bar{B}$ or ${A}$  \\ \hline
T & T & F  & T \\
T & F & F  & T \\
F & T & T  & T \\
F & F &  T  &  F \\
\end{tabular}

\end{multicols}
Since the truth tables are not the same, they are not equivalent.

\end{center}

\end{solution}
\end{exercise}

\begin{exercise}
Suppose $A,B,C$ are statements. Negate the statements
\[
A\mbox{ and }(B \mbox{ or }C)\]
and
\[
A\mbox{ or }(B \mbox{ and }C)
.
\]
\begin{solution}
The negations are, respectively,
\[
\bar{A} \mbox{ or } (\bar{B}\mbox{ and }\bar{C})
\]
and
\[
\bar{A} \mbox{ and } (\bar{B}\mbox{ or }\bar{C}).
\]
\end{solution}

\end{exercise}

\begin{exercise}
Suppose $P,Q,R$ are statements and $P\Rightarrow Q$, $Q\Rightarrow R$ and $R\Rightarrow P$. Show that $P\Leftrightarrow R$.

\begin{solution}
Assume $P\Rightarrow Q$, $Q\Rightarrow R$ and $R\Rightarrow P$, we will show that $P\Leftrightarrow R$. To prove this, we need to show $P\Rightarrow R$ and $R\Rightarrow P$. We area already assuming $R\Rightarrow P$, so we just need to prove $P\Rightarrow R$. Hence, assume $P$. Then since $P\Rightarrow Q$, we know $Q$ is also true, and since $Q\Rightarrow R$, we also know $R$ is true. Thus $P\Rightarrow R$.
\end{solution}
\end{exercise}

\begin{exercise}
Negate the following statements
\begin{enumerate}[label=(\alph*)]
\item $(\forall x \;\; P)\Rightarrow ((\exists y \mbox{ s.t. } Q) \mbox
{ and }R)$.
\item $\forall x\;\; \exists y \mbox{ s.t. } P\Rightarrow Q$
\item $\exists x  \mbox{ s.t. } ((\forall y \;\; A)\Rightarrow (\exists z  \mbox{ s.t. } B))$.
\end{enumerate}

\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item $(\forall x \;\; P) \mbox{ and }((\forall y \;\; \bar{Q}) \mbox
{ or }\bar{R})$.
\item $\exists x \mbox{ s.t. }  \;\; \forall y \; (P\mbox{ and } \bar{Q})$.
\item $\forall x ( (\forall y \;\; A) \mbox{ and } (\forall z   \bar{B}))$.
\end{enumerate}
\end{solution}
\end{exercise}

\chapterimage{Figures/blank.png}

\chapter{The Real Line}%
\label{realline}

\section{The Reals}%
\label{realnumbers}

In this course, our aim is to derive as many useful tools and theorems as we can from the fewest assumptions. We could spend a whole year (or years!) on the foundations of mathematics, and try to make sense of, for example, what the number 2 is, but instead we will take for granted the existence the {\it real} numbers $\mathbb{R}$ that you are all familiar with from school. We assume that, if an expression contains a term $x$ and $x=y$, we can replace $x$ with $y$ in that expression.  For example, $3+(5-1)=3+4$ since $5-1=4$.

We will also assume we have the familiar addition operation ``$+$'' and multiplication operation ``$\cdot$''  that combines two numbers $a$ and $b$ to make another real number $a+b$ and $a\cdot b=ab$ respectively, and which make $\mathbb{R}$ into what we call a ``field'':

\begin{definition}
A {\it field} is a set $F$ along with operations $+$ and $\cdot$ so that the following hold:\\

\begin{description}
\item[{\bf Rules of Addition $+$:}] For $a,b,c\in F$,
\begin{itemize}%[label=(\alph*)]
\item[(A0)] $a+b\in F$
\item[(A1)] Commutativity: $a+b=b+a$.
\item[(A2)] Associativity: $a+(b+c)=(a+b)+c$.
\item[(A3)] There is an element $0\in F$ so that $0+a=a$ for all $a\in F$.
\item[(A4)] There is an element we denote $-a\in F $ so that $a+(-a)=0$. We write $b+(-a)=b-a$ for short.
\end{itemize}
\end{description}

\begin{description}
\item[{\bf Rules of Multiplication $\cdot$:}] For $a,b,c\in F$,
\begin{itemize}%[label=(\alph*)]
\item[(M0)] $a\cdot b\in F$
\item[(M1)] Commutativity: $a\cdot b=b\cdot a$
\item[(M2)] Associativity: $a\cdot (b\cdot c)=(a\cdot b)\cdot c$
\item[(M3)] There is an element $1\in F$ so that $1\cdot a=a$
\item[(M4)] If $a\neq 0$, there is a number we denote $\frac{1}{a}$ so that $a\cdot\frac{1}{a}=1$. We write $\frac{1}{a}\cdot b=\frac{b}{a}$ for short.
\item[(M5)] Distribution: $a\cdot (b+c)=a\cdot b+a\cdot c$.
\end{itemize}
\end{description}
\end{definition}

We will frequently write $ab$ for $a\cdot b$ when there is no chance of ambiguity, so for example, ``$a$ times $b$'' can be write $ab$ or $a\cdot b$, but ``$2$ times $3$'' must be written $2\cdot 3$ to avoid confusion with the number $23$.

We will take as an {\it axiom} that the real numbers $\mathbb{R}$ form a field with the usual addition and multiplication operations, that is, we assume they hold without proof.  There are many other fields apart from the reals, and we will see a few more in this class (you will learn about more interesting ones in future algebra classes).

Without these rules it would be hard to solve any equations. For example, if we want to solve $3x+2=2x+5$, what rules do we need? Below we prove some propositions using only the axioms above that list the ``moves'' we would usually make in solving such an equation.

\begin{proposition}
\label{p:field-consequences}
The properties (A0),...,(A4) above imply that, for $x,y,z\in \mathbb{R}$,
\begin{enumerate}[label=(\alph*)]
\item $x+y=x+z$ if and only if $y=z$
\item If $x+y=x$, then $y=0$
\item If $x+y=0$, then $x=-y$
\item $-(-x)=x$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item If $y=z$, then we can just substitute $y$ with $z$ in $x+y$ to get that this equals $x+z$. Conversely, if $x+y=x+z$, then
\begin{align*}
y
& \stackrel{(A3)}{=}y+0\stackrel{(A4)}{=}y+(x-x)\stackrel{(A2)}{=}(y+x)-x\stackrel{(A1)}{=}(x+y)-x
=(x+z)-x\stackrel{(A1)}{=}(z+x)-x\\
&
\stackrel{(A2)}{=}z+(x-x)
\stackrel{(A4)}{=}z+0
\stackrel{(A3)}{=}z.
\end{align*}

\item If $x+y=x$, then by (A3), $x+y=x=x+0$, so (a) (with $z=0$) implies $y=0$.
\item If $x+y=0$, then
\[
x+y=0 \stackrel{(A4)}{=}y+(-y)
\]
and so (a) (with $z=-y$) implies $x=-y$.

\item By definition,
\[
-x+-(-x)\stackrel{(A4)}{=}
0\stackrel{(A2)}{=}x-x
\stackrel{(A1)}{=}-x+x
\]
so (a) implies $-(-x)=x$.
\end{enumerate}
\end{proof}

Try this proposition on your own (the proof is very similar to the above):

\begin{proposition}
The properties (M0),...,(M5) imply that, for $x,y,z\in \mathbb{R}$, if $x\neq 0$,
\begin{enumerate}[label=(\alph*)]
\item $xy=xz$ if and only if $y=z$
\item If $xy=x$, then $y=1$
\item If $xy=1$, then $y=\frac{1}{x}$
\item $1/(1/x)=x$.
\end{enumerate}
\end{proposition}

We mention a few more useful facts about the reals. Again, they look obvious, but it takes some work to show that they are derived from the property that $\mathbb{R}$ being a field. We leave their proofs as an exercise, but remember to use the above axioms and propositions that we have proved so far:

\begin{proposition}
\label{p:0x=0}
For $x,y\in \mathbb{R}$,
\begin{enumerate}[label=(\alph*)]
\item $0\cdot x=0$.
\item If $x\neq 0\neq y$, then $xy\neq 0$.
\item $(-x)y=-(xy)=x(-y)$.
\item $(-x)(-y)=xy$.
\end{enumerate}\end{proposition}

%\begin{proof}
%\begin{enumerate}[label=(\alph*)]
%\item Note that
%\[
%0\cdot x \stackrel{(A3)}{=}(0+0)x
%\stackrel{(M5)}{=}0\cdot x+0\cdot x
%\]
%so Proposition \ref{p:field-consequences}(c) implies $0\cdot x=0$.
%
%
%\end{proof}

\section{The integers and the Archimedean Property}%
\label{archimedeanproperty}

An important subset of the reals is the {\it integers }
\[
\mathbb{Z}=\{...-2,-1,0,1,2,...\}
\]
We will also let $\mathbb{N}=\{1,2,3...\}$ denote the {\it positive integers} or {\it natural numbers}\footnote{Some courses assume $\mathbb{N}$ also contains $0$. It's best to clarify this with the instructor if you aren't sure.}.

Notice that the set $\mathbb{N}$ has the properties that $1 \in \mathbb{N}$, and that if $x \in \mathbb{N}$, then also $x+1 \in \mathbb{N}$. {\em Moreover, $\mathbb{N}$ is the smallest subset of the reals which has these two properties.} It is this observation which makes mathematical induction such a powerful tool for proving statements about the natural numbers -- the natural numbers and mathematical induction are inextricably bound up together. We shall examine induction more carefully below in Section 4.

\medskip
An important result about the reals and integers is the following:

\begin{description}
\item[Archimedean property:] For every $x, y \in \mathbb{R}$ with $x,y>0$, there is an integer $n\in \mathbb{N}$ so that $ny>x$. \\
\end{description}

This might seem like it should be obvious, but how would you prove it? Consider just finding an integer $n>x$. If $x$ was an integer, we could simply let $n=x+1$, but if $x$ is not an integer what would we do? You might think ``round up to the nearest integer,'' but that this number exists {\it is} what we are trying to show! {\it Why not look at the decimal expansion $x=a_{0}.a_{1}...$ and let $n=a_0+1$?} Firstly, we haven't proven that decimal expansions even exist for any real number yet (we will in a few weeks), and second, the proof that each number has a decimal expansion {\it requires} the Archimedean property.

In a couple of weeks' time, we will prove the Archimedean property, as a consequence of the completeness axiom for the real numbers, which we shall study in Section 5.\footnote{It is perhaps disconcerting that it's {\em not} possible to prove the Archimedean property only using the rules (A0-A4) and (M0-M5), together with (R1-R5) which are introduced in Section 3. Indeed, it can be established that, in the terminology of Section 3, there are ordered fields which do not satisfy the Archimedean property. It is precisely because of pitfalls such as this that we are adopting a careful axiomatic perspective for the real numbers.}

\section{The rationals and irrationals}%
\label{rationals}

The next important subset of the reals is the {\it rationals}:
\[
\mathbb{Q} = \left\{\frac{a}{b} \;|\; a,b\in \mathbb{Z},\; b\neq 0\right\}
\]

While we can multiply and add integers together, $\mathbb{Z}$ not a field since (M4) fails: given an integer $x$ we can't find another {\it integer} $y$ with $xy=1$. The rationals is the smallest subset of $\mathbb{R}$ that contains $\mathbb{Z}$ and {\it is} a field. Indeed, if we add two rationals $\frac{a}{b}$ and $\frac{c}{d}$  we get
\[
\frac{a}{b}+\frac{c}{d} = \frac{ad}{bd}+\frac{cb}{db} = \frac{ad+cb}{bd}\in\mathbb{Q}
\]
so (A0) holds, and similarly (M0) holds since
\[
\frac{a}{b}\cdot \frac{c}{d}=\frac{ac}{bd}\in\mathbb{Q}.
\]
The other axioms can easily be checked as well.\\

The rationals are {\it dense} in the reals in the following sense:

\begin{theorem}
Given any two real numbers $a<b$, there is $r\in\mathbb{Q}$ with $a<r<b$.
\end{theorem}

{\it Idea for proof:} Imagine placing dots along the real line equally spaced apart by a distance $d$ with $d<b-a$. Notice that the interval between $a$ and $b$ is of length bigger than $d$, so one of the dots in this progression must land in this interval (otherwise, it would have to make a jump a distance at least $b-a$ to avoid the interval). Now let's give the real proof:

\begin{proof}
Since $b>a$, $b-a>0$, and by the Archimedean property, there is $n\in  \mathbb{Z}$ with $n>\frac{1}{b-a}$, and so $\frac{1}{n}<b-a$. Let's look at the numbers $\{j/n:j\in\mathbb{Z}\}$: they are equally spaced along the real line by a distance $\frac{1}{n}$. Now we need to find one of them between $a$ and $b$. We pick the last one before $b$, that is, we let $j$ be the largest integer so that $\frac{j}{n}<b$. We claim that $\frac{j}{n}>a$ (which will then finish the theorem, as now we have a rational $\frac{j}{n}$ between $a$ and $b)$. We prove by contradiction: If $\frac{j}{n}\leq a$, then
\[
\frac{j+1}{n}=\frac{j}{n}+\frac{1}{n}\leq a+\frac{1}{n}<a+(b-a)=b,
\]
but then $j$ wasn't the largest integer for which $\frac{j}{n}<b$, since $\frac{j+1}{n}$ is, too, and so we get a contradiction. Thus, $\frac{j}{n}>a$.
\end{proof}

In other words, no matter how small an interval $(a,b)$ that we look at\footnote{The notation $(a,b)$ here means $\{ x \in \mathbb{R} :  a < x < b\}$.}, we can always find a rational number inside. However, not all real numbers are rational numbers. A real number is said to be {\it irrational} if it is not a rational number. In other words, the set of irrational numbers is the set $\mathbb{R} \setminus \mathbb{Q}$.

The first known irrational number is $\sqrt{2}$, that is, the positive real number whose square is $2$. We will actually prove that $\sqrt{2}$ exists in a couple weeks. For now, let us assume this fact and prove that it is irrational:
\begin{theorem}
The number $\sqrt{2}$ is irrational.
\end{theorem}

\begin{proof}
We prove by contradiction. Assume $\sqrt{2}$ is rational, so there are integers $p$ and $q\neq 0$ so that $\sqrt{2}=\frac{p}{q}$. By reducing the fractions, we can assume $p$ and $q$ share no common factor. Squaring both sides gives
\[
2=\frac{p^{2}}{q^2,}
\]
whence $2q^2=p^2$.
This means that $p^2$ is even, so $p$ must be even (otherwise, $p$ is odd, and we have already proved that the square of an odd number is odd). Thus, $p=2r$ for some integer $r$, so the above equation gives
\[
2q^2=p^2=(2r)^2=4r^2,
\]
whence $ q^2=2r^2$.
Again, this means $q$ is also even, but now $p$ and $q$ are both divisible by $2$, contradicting our assumption that they shared no common factor. Thus, $\sqrt{2}$ must be irrational.
\end{proof}

This is just one irrational number, but the next proposition says that we can construct infinitely many irrational numbers just from one:

\begin{proposition}
Let $a$ be rational and $b$ be irrational.
\begin{enumerate}[label=(\alph*)]
\item $a+b$ is irrational.
\item If $a\neq 0$, then $ab$ is irrational.
\end{enumerate}
\end{proposition}

\begin{proof}
We just prove (a) as (b) is similar. Suppose $a+b$ is rational. Then
\[
b=0+b=(-a+a)+b=-a+(a+b)
\]
that is, $b$ is a sum of two rational numbers: $-a$ and $a+b$, so it is rational, and we get a contradiction since $b$ is irrational. Thus $a+b$ is irrational.
\end{proof}

There are of course many other irrational numbers out there like $e$ and $\pi$. Interestingly, there are some numbers that seem like they should be irrational but to this date we still don't know. For example, it is unknown whether the following numbers are irrational:

\[
\pi\pm e,\;\;  \pi e,\;\;  \pi/e,\;\; \pi^e,\;\; \pi^{\sqrt{2}},\;\; \pi^\pi,\;\; e^{\pi^2}, \;\; 2^{e},\;\; e^{e}.
\]

We will discuss how to construct many irrational numbers not involving $e$, $\pi$, in the coming weeks when we discuss decimals.

\section{Roots}%
\label{roots}

In this class we will be working a lot with roots of numbers like $\sqrt{2}$, but like $\sqrt{2}$, that they exist at all takes justification. This is given in the following proposition whose proof we postpone until Week 3. Recall that $y^n=y\cdot y\cdots y$ with $n$ $y$'s in the product.

\begin{proposition}
\label{p:roots-exist}
Given $x>0$ and $n\in\mathbb{N}$, there is a unique $y>0$ so that $y^n=x$, and we write this number $y$ as $y=x^{\frac{1}{n}}$.
\end{proposition}

 We can define more generally any rational root: if $\frac{m}{n}>0$ is rational and $x>0$, we define
\begin{equation}
\label{e:x^m/n}
x^{\frac{m}{n}} = \left(x^{\frac{1}{n}}\right)^{m}=\underbrace{x^{\frac{1}{n}}\cdots x^{\frac{1}{n}}}_{m\mbox{ times}}.
\end{equation}

\begin{proposition}
Let $x>0$ and $p,q\in \mathbb{Q}$. Then
\begin{enumerate}[label=(\alph*)]
\item $x^{p}x^q=x^{p+q}$
\item $(x^{p})^{q}=x^{pq}$
\item $(xy)^{p}=x^p y^p$.
\end{enumerate}
\end{proposition}

\begin{protip}
{\bf Try special cases:} When given something to prove as above, it might not be clear how to get started, but you don't have to prove everything on the first try! Try first proving the statement in some easier cases, and then see if you can adjust your proof or even use your easy cases to prove the general case.
\end{protip}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item First notice that this holds when $p$ and $q$ are integers, since then
\[
x^{p+q}=\underbrace{x\cdot x\cdots x}_{\mbox{$p+q$ times}}
=\underbrace{x\cdot x\cdots x}_{\mbox{$p$ times}}\cdot \underbrace{x\cdot x\cdots x}_{\mbox{$q$ times}} = x^{p}x^{q}.
\]
Now suppose $p=\frac{a}{b}$ and $q=\frac{c}{d}$ are two positive rationals and $x>0$. Then
\begin{align*}
x^{p+q}
& =x^{\frac{a}{b}+\frac{c}{d}}
=x^{\frac{ad+bc}{bd}}
\stackrel{\eqref{e:x^m/n}}{=} \left(x^{\frac{1}{bd}}\right)^{ad+bc}
=\left(x^{\frac{1}{bd}}\right)^{ad}\left(x^{\frac{1}{bd}}\right)^{bc}
\stackrel{\eqref{e:x^m/n}}{=}x^{\frac{ad}{bd}}x^{\frac{bc}{bd}}=x^{\frac{a}{b}}x^{\frac{c}{d}}=x^{p}x^{q}
\end{align*}
where in the fourth equation we used the integer case we proved earlier.
\item

Let's first prove the slightly simpler case that
\begin{equation}
\label{e:x^abd}
\left(x^{\frac{a}{b}}\right)^{\frac{1}{d}}=x^{\frac{a}{b}\cdot\frac{1}{d}}.
\end{equation}
By Proposition \ref{p:roots-exist}, for the right side to equal the $d$th root on the left, we need to show $(x^{\frac{a}{b}\frac{1}{d}})^{d} = x^{\frac{a}{b}}$:
\[
\left(x^{\frac{a}{b}\frac{1}{d}}\right)^{d} =\left(x^{\frac{a}{bd}}\right)^{d}
\stackrel{\eqref{e:x^m/n}}{=}
\left(\left(x^{\frac{1}{bd}}\right)^{a}\right)^{d}
=\left(x^{\frac{1}{bd}}\right)^{ad}
\stackrel{\eqref{e:x^m/n}}{=}x^{\frac{ad}{bd}}=x^{\frac{a}{b}},
\]
and so \eqref{e:x^abd} follows.

Now we prove the full case: if $p=\frac{a}{b}$ and $q=\frac{c}{d}$, then
\[
(x^{p})^{q} = \left(x^{\frac{a}{b}}\right)^{\frac{c}{d}}
\stackrel{\eqref{e:x^m/n}}{=}\left( \left(x^{\frac{a}{b}}\right)^{\frac{1}{d}}\right)^{c}
\stackrel{\eqref{e:x^abd}}{=}\left(x^{\frac{a}{bd}}\right)^{c}
\stackrel{\eqref{e:x^m/n}}{=} x^{\frac{ac}{bd}}=x^{pq}.
\]
This proves (b).

\item Let's first consider trying to show $(xy)^{\frac{1}{b}}=x^{\frac{1}{b}}y^{\frac{1}{b}}$. By Proposition \ref{p:roots-exist}, we just need to show that $(x^{\frac{1}{b}}y^{\frac{1}{b}})^{b}=xy$. If we take $x^{\frac{1}{b}}y^{\frac{1}{b}}$ times itself $b$ times, and rearrange the terms so that all the $x^{\frac{1}{b}}$'s are on the left, we get
\[
(x^{\frac{1}{b}}y^{\frac{1}{b}})^{b}
=x^{\frac{1}{b}}y^{\frac{1}{b}}  x^{\frac{1}{b}}y^{\frac{1}{b}}  \cdots x^{\frac{1}{b}}y^{\frac{1}{b}}
=\underbrace{x^{\frac{1}{b}}\cdots x^{\frac{1}{b}}}_{b\mbox{ times}}\underbrace{y^{\frac{1}{b}}\cdots y^{\frac{1}{b}}}_{b\mbox{ times}}
=(x^{\frac{1}{b}})^{b}(y^{\frac{1}{b}})^{b}=x\cdot y.
\]
Now let's try the general case: If $p=\frac{a}{b}$, then by the case we just proved:
\[
(xy)^{\frac{a}{b}}
\stackrel{\eqref{e:x^m/n}}{=} \left((xy)^{\frac{1}{b}}\right)^{a}
=\left(x^{\frac{1}{b}}y^{\frac{1}{b}}\right)^{a}\]
Just as before, if we take $x^{\frac{1}{b}}y^{\frac{1}{b}}$ times itself $a$ times and then move all the $x^{\frac{1}{b}}$'s to the left, we get that the above is
\[
=(x^{\frac{1}{b}})^{a}(y^{\frac{1}{b}})^{a}
\stackrel{\eqref{e:x^m/n}}{=} x^{\frac{a}{b}}y^{\frac{a}{b}}=x^{p}y^{p}.
\]

%Let's first play around with a simple case: let's first try and show that
%\begin{equation}
%\left(x^{\frac{1}{b}}\right)^{\frac{1}{d}}=x^{\frac{1}{bd}}.
%\label{e:x^1/mn}
%\end{equation}
%Proposition \ref{p:roots-exist} tells us that, if we want to show some number $y$ is equal to $x^{\frac{1}{bd}}$, we need to show $y^{bd}=x$, so let's take the $mn$th power of the term on the left above (and recalling that for any $y>0$ and $k\in\mathbb{N}$, $(y^{\frac{1}{k}})^{k}=y$)
%\[
%\left(\left(x^{\frac{1}{b}}\right)^{\frac{1}{d}}\right)^{bd}
%=\left(\left(\left(x^{\frac{1}{b}}\right)^{\frac{1}{d}}\right)^{d}\right)^{b}
%=\left(x^{\frac{1}{b}}\right)^{b}=x.
%\]

\end{enumerate}
\end{proof}

\section{Exercises}%
\label{realexercises}

The relevant exercises in Liebeck's book are at the end of Chapter 2.

\begin{exercise}
Prove $(-1)^2=1$.
\begin{solution}
By Proposition \ref{p:0x=0} and [M3]:
\[
(-1)^2=(-1)\cdot (-1) = 1\cdot 1=1.
\]
\end{solution}
\end{exercise}

\begin{exercise}
Let $F=\{0,1\}$, and for $a,b\in F$, define $a\cdot b$ and $a+b$ to equal what they would under normal multiplication except that now we define $1+1=0$. Is this a field?
\begin{solution}
Yes. One can prove all the field properties on a case by case basis: for example, one can show $a+(b+c)=(a+b)+c$ by try all possible values for $a,b$ and $c$.
\end{solution}
\end{exercise}

\begin{exercise}
Prove $\sqrt{6}$ is irrational.
\begin{solution}
Suppose $\sqrt{6}=\frac{p}{q}$ where $p,q>0$ are integers with no common factor. Then
\[
6=\frac{p^2}{q^2} \;\; \Longrightarrow \;\;  6q^2=p^2.
\]
Note that $p$ must be even, so $p=2k$ for some integer $k$, thus
\[
6q^2=p^2=4k^2  \;\; \Longrightarrow \;\;   3q^2=2k^2.
\]
Now $q$ must be divisible by $2$. Otherwise, $q=2j+1$ for some $j$, and so
\[
2k^2=3q^2=3(2j+1)^2=12j^2+12j+3
\]
which is a contradiction since now we have an even number equal to an odd number. Hence, $q=2j$ for some integer $j$. But now $p$ and $q$ are both even, which contradicts our assumption that they had no common factor.

\end{solution}
\end{exercise}

\begin{exercise}
Is $\sqrt{1+\sqrt{2}}$ rational? Can you write a more general statement based on your proof?
\begin{solution}
No. Suppose it were, then so is$\sqrt{1+\sqrt{2}}^2=1+\sqrt{2}$, and hence so is $\sqrt{2}=1+\sqrt{2}-1$.

A more general statement is that if $x$ is irrational, then so is $\sqrt{x}$, since otherwise $\sqrt{x}^2=x$ is rational, a contradiction.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose $a+b\sqrt{2}=c+d\sqrt{2}$ where $a,b,c,d\in\mathbb{Q}$. Prove that $a=c$ and $b=d$.
\begin{solution}
Homework question.
%Suppose for the sake of a contradiction that $b\neq d$. If $a+b\sqrt{2}=c+d\sqrt{2}$, then rearranging, we get $a-c=(d-b)\sqrt{2}$, and dividing by $d-b$, we have
%\[
%\sqrt{2}=\frac{a-c}{d-b}\in\mathbb{Q},
%\]
%which is a contradiction since $\sqrt{2}$ is irrational. Thus, we must have that $b=d$. Then setting $b=d$ in $a+b\sqrt{2}=c+d\sqrt{2}$ implies $a=c$ as well.
\end{solution}
\end{exercise}

\begin{exercise}
Let
\[
\mathbb{Q}[\sqrt{2}] = \{a+b\sqrt{2}\; |\; a,b\in\mathbb{Q}\}.
\]
Show that $\mathbb{Q}[\sqrt{2}] $ is a field using the usual rules of addition and multiplication.
\end{exercise}

\begin{solution}
The only difficult property to verify is (M4), so let's focus on this. Let $a+b\sqrt{2}\in \mathbb{Q}[\sqrt{2}]$ where $a,b\in\mathbb{Q}$. Note that
\[
(a+b\sqrt{2})(a-b\sqrt{2})=a^2-2b^2.
\]
Note that since $2$ is irrational, we cannot have $a^2-2b^2=0$. Thus,
\[
(a+b\sqrt{2}) \frac{a-b\sqrt{2}}{a^2-2b^2}=1
\]
and so we have
\[
(a+b\sqrt{2})^{-1}
=\frac{a-b\sqrt{2}}{a^2-2b^2}
=\frac{a}{a^2-2b^2}-\frac{b}{a^2-2b^2}\sqrt{2} \in\mathbb{Q}[\sqrt{2}].\]
Thus, we have verified (M4).
\end{solution}

\begin{exercise}
Are there rational numbers $a$ and $b$ so that $a\sqrt{2}+b\sqrt{3}=\sqrt{6}$?
\begin{solution}
No. Suppose there were. They would both have to be non-zero. Then
\[
6=(a\sqrt{2}+b\sqrt{3})^2=2a^2+3b^2+12ab\sqrt{6}
\]
and so $\sqrt{6} = (6-(2a^2+3b^2))/(12ab)$, which is a rational number, but by an earlier exercise, $\sqrt{6}$ is not rational, which is a contradiction.
\end{solution}
\end{exercise}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 2: Inequalities and Induction}

\chapterimage{Figures/blank.png}
\chapter{Inequalities}%
\label{inequalities}

This week we will examine inequalities more closely, and describe them formally in terms of just a few rules or {\it axioms}.

\section{The axioms for order}%
\label{orderaxioms}

An inequality is a relation on pairs of real numbers.
We use the symbols $\leq,\geq,<,>$ to denote these relationships.
E.g.~we write $x < y$, spoken as ``$x$ is less than $y$.''
The informal idea is that $x<y$ exactly when ``$x$ is strictly to the left of $y$ on the number line.''
$x < y$ means exactly the same thing as $y > x$ (spoken as ``$y$ is greater than $x$'').
When we write $x \leq y$ we mean that either $x < y$ or $x=y$.
Hence it is true that $2 \leq 3$.
We prefer to list the formal properties that $<$ possesses which, together with the rules of addition and rules of multiplication from Section 2, make $\mathbb{R}$ into what is called an {\em ordered field}.
These properties of the relation $<$ on the real numbers should be very familiar!

\medskip
\begin{definition} An {\em ordered field} is a field $F$, together with a relation $<$, so that the following hold: %Given $x,y\in \mathbb{R}$, we may write $x<y$, which we pronounce ``$x$ is less than $y$".  The symbol ``$<$" satisfies the following axioms:
\begin{enumerate}
\item[(R1)] (Positive/negative) If $x\in F$, then exactly one of the following is true: $0<x$, $x=0$, or $x<0$.
\item[(R2)] (Negations reverse) If $y<x$, then $-x<-y$.
\item[(R3)] (Can add constants) If $x<y$, and $c\in F$, then $x+c<y+c$.
\item[(R4)] (Positives multiply) If $0<x$ and $0<y$ then $0<xy$.
\item[(R5)] (Transitive property) If $x<y$ and $y<z$ then $x<z$.
\end{enumerate}
\end{definition}

This definition gives a minimal set of {\it axioms}, that is, rules that we will take for granted, which are minimal in the sense that no axiom can be deduced from the others. Just like we demonstrated last week with the axioms of addition and multiplication, in this section we'll show how many standard facts about inequalities that we are used to can be deduced from these axioms. %(Any other fact about inequalities that we haven't proved here but you want to use in your homework, you will have to prove it!)

We can now define \(x>y\) if and only if \(y<x\).

\begin{proposition}
If $x>0$, then $-x<0$.
\end{proposition}

\begin{proof}
This follows immediately from (R2) with $0$ in place of $y$.
\end{proof}

\begin{proposition}
If $x\neq 0$, then $x^2>0$.
\end{proposition}

\begin{proof}
If $x>0$ this follows from (R4).  If $x<0$ then $-x>0$, so
\[
0<(-x)^2= (-1)^2x^2=x^2.
\]
(Recall from Exercise 2.1 that $(-1)^2=1$.)
\end{proof}

\begin{exercise}
Prove that $1>0$.
\begin{solution}
This follows from the last proposition with $x=1$, since $0<1^2=1\cdot 1=1$.
\end{solution}
\end{exercise}

Pause for thought: why are we wasting our time proving things like $1 > 0$, which we have all known and understood since the early years of primary school?
Mathematics is not just to establish what is true and what is false, but also to understand, in a critical way, the relationships between statements.
What we learn from this exercise is that the statement $1 > 0$ can be logically derived from the basic axioms (A0-A4), (M0-M5) and (R1-R5).
The same applies to most other true statements of elementary arithmetic.

\begin{proposition}
\label{p:1/x>0}
If $x>0$, then $\frac{1}{x}>0$.
\end{proposition}

\begin{proof}
Suppose $\frac{1}{x}\leq 0$. Then $-\frac{1}{x}\geq 0$, and so (R4) implies
\[
0\leq -\frac{1}{x}\cdot x = -1 < 0,\]
which is a contradiction, thus $\frac{1}{x}> 0$.
\end{proof}

\begin{proposition}
\label{p:xu>xv}
If $x>0$, then $u>v$ if and only if $xu>xv$.
\end{proposition}

\begin{proof}
First assume $u>v$. If $x>0$, then
\begin{align*}
u>v & \stackrel{[R3]}{\Longrightarrow} u-v>v-v=0\\
&  \stackrel{[R4]}{\Longrightarrow} (u-v)x>0\\
& \Longrightarrow ux-vx>0 \\
& \stackrel{[R3]}{\Longrightarrow} ux>vx
\end{align*}
Now assume $xu>xv$. Then we can use the same proof as above, but replacing $u$ with $ux$, $v$ with $vx$, and $x$ with $\frac{1}{x}$ (since $\frac{1}{x}>0$ by Proposition \ref{p:1/x>0}).
\end{proof}

\begin{proposition}
\label{p:u^2>v^2}
If $0<u,v$, then $u^2>v^2$ if and only if $u>v$.
\end{proposition}

\begin{proof}
First, notice that if $0<v,u$, then
\[
u^2>v^2 \;\;\;   \stackrel{[R3]}{ \Longleftrightarrow} \;\;\;  u^2-v^2>v^2-v^2=0 \;\;\;  \Longleftrightarrow \;\;\;  (u-v)(u+v)>0
\]
and by (R4), this last inequality is true if and only if $u-v>0$ (since $u+v>0$ by assumption), which by (R3) is true if and only if $0+v<u-v+v$, that is, if and only if $v<u$.
\end{proof}

\section{Separating products}%
\label{separatingproducts}

In this section we cover some very useful inequalities that allow us to essentially separate terms in a product. They all stem from the following lemma:

\begin{lemma}
\label{l:little-amgm}
For $u,v\in\mathbb{R}$,
\[uv \leq \frac{u^2+v^2}{2}.\]
\end{lemma}
Notice that on one side of the above inequality, the terms $u$ and $v$ are multiplied together, and on the other side, they are separate (but now squared).

\begin{proof}
Since$(u-v)^2\geq 0$, we get
\[
0\leq (u-v)^2 =u^2-2uv + v^2.\]
So by (R3), we can add $2uv$ to both sides to get
\[
2uv \leq u^2+v^2\]
and then by (R4), we can multiply both sides by $\frac{1}{2}$ to get $uv \leq \frac{u^2+v^2}{2}$ as desired.
\end{proof}

\begin{exercise}
\label{ex:uv=u^2+v^2/2}
When does equality hold in the previous lemma? That is, when do we have $uv = \frac{u^2+v^2}{2}$?
\end{exercise}

\begin{solution}
Let's retrace the proof of  Lemma \ref{l:little-amgm} backwards trying to use only equalities:
\[
uv = \frac{u^2+v^2}{2} \;\;\; \Longleftrightarrow \;\;\; 2uv = u^2+v^2
 \;\;\; \Longleftrightarrow \;\;\; 0=u^2+v^2-2uv = (u-v)^{2}
 \]
 and this last equality holds if and only if $u=v$.
 \end{solution}

From this, we get the famous {Arithmetic Mean-Geometric Mean (AM-GM) inequality}:

\begin{theorem}[AM-GM Inequality]
\label{t:AM-GM}
Let $n\in\mathbb{N}$ and $x_{1},...,x_{n}\geq 0$. Then
\[
(x_{1}\cdot x_{2}\cdots x_{n})^{\frac{1}{n}}\leq \frac{x_{1}+\cdots + x_{n}}{n}.
\]
\end{theorem}
That is, the {\it geometric mean} on the left is at most the {\it arithmetic mean} on the right.

\begin{proof}
The $n=2$ case follows from Lemma \ref{l:little-amgm}: if we set $u=\sqrt{x_{1}}$ and $v=\sqrt{x_{2}}$, then
\begin{equation}
\label{e:x1x2^1/2<x1+x2/2}
(x_{1}x_{2})^{\frac{1}{2}}=uv\leq \frac{u^2+v^2}{2} = \frac{x_{1}+x_{2}}{2}.
\end{equation}
For the $n>2$ case, see Exercise \ref{ex:AMGM}.
\end{proof}

\begin{exercise}
For $x_{1},x_{2}\geq 0$, when do we have $(x_{1}x_{2})^{\frac{1}{2}}= \frac{x_{1}+x_{2}}{2}$?
\end{exercise}

\begin{solution}
If there is equality, that means there is equality in \eqref{e:x1x2^1/2<x1+x2/2}, and by Exercise \ref{ex:uv=u^2+v^2/2}, this holds if and only if $u=v$, that is, if $\sqrt{x_{1}}=\sqrt{x_{2}}$, which holds if and only if $x_{1}=x_{2}$.
\end{solution}

The next useful inequality is the {\it Cauchy-Schwarz} inequality:

\begin{theorem}[Cauchy-Schwarz Inequality]
Let $n\in\mathbb{N}$,  $x_{1},...,x_{n},y_{1},...,y_{n}\in\mathbb{R}$. Then
\[
x_{1}y_{1}+\cdots + x_{n}y_{n}
\leq \sqrt{x_{1}^2+\cdots + x_{n}^{2}} \sqrt{y_{1}^2+\cdots + y_{n}^{2}} .
\]

\end{theorem}

Why is this useful? On the left we have a {\it mixed} product of $x$-terms and $y$-terms, and we are able to relate it to a product of two terms where now the $x$'s and $y$'s are {\it separated}.

We'll just prove the $n=2$ case for now and return to the general case later. Note that
\[
(x_{1}y_{1}+x_{2}y_{2})^2
=x_{1}^2y_{1}^2+x_{2}^2y_{2}^2+2x_{1}y_{1}x_{2}y_{2}
\]
and using Lemma \ref{l:little-amgm} on the second term with $u=x_{1}y_{2}$ and $v=x_{2}y_{1}$, this above is at most
\[
\leq x_{1}^2y_{1}^2+x_{2}^2y_{2}^2+2\frac{(x_{1}y_{2})^2+(x_{2}y_{1})^2}{2}
=x_{1}^2y_{1}^2+x_{2}^2y_{2}^2+x_{1}^2y_{2}^2+x_{2}^2y_{1}^2
=(x_{1}^2+x_{2}^2)(y_{1}^2+y_{2}^2)
\]
Thus we have shown
\[
(x_{1}y_{1}+x_{2}y_{2})^2\leq (x_{1}^2+x_{2}^2)(y_{1}^2+y_{2}^2)
=\left(\sqrt{x_{1}^2+x_{2}^2}\sqrt{y_{1}^2+y_{2}^2}\right)^{2}
\]
and now by Proposition \ref{p:u^2>v^2}, we get
\[
x_{1}y_{1}+x_{2}y_{2}\leq\sqrt{x_{1}^2+x_{2}^2}\sqrt{y_{1}^2+y_{2}^2}.
\]

\begin{example} Show that
\[
\frac{a+b}{2} \leq \left(\frac{a^2+b^2}{2}\right)^{\frac{1}{2}}.
\]
Which of our above inequalities should we try using? Lemma \ref{l:little-amgm} and AM-GM don't seem relevant since we don't have something like $ab$ on the left, so let's see if we can use the Cauchy-Schwarz inequality: Rewrite $\frac{a+b}{2}=a\cdot \frac{1}{2} +b\cdot\frac{1}{2}$, then the Cauchy Schwarz inequality implies:
\[
\frac{a+b}{2} = a\cdot\frac{1}{2} + b\cdot\frac{1}{2} \leq \sqrt{a^2+b^2}\sqrt{\frac{1}{2^{2}}+\frac{1}{2^{2}}}
=\left(\frac{a^2+b^2}{2}\right)^{\frac{1}{2}}.
\]
\end{example}
\section{Absolute Values}%
\label{absolutevalues}

\begin{definition}
For $x\in \mathbb{R}$, we define the {\it absolute value} or {\it modulus} of $x$ to be
\[
|x| = \left\{\begin{array}{ll} x & \mbox{ if } x\geq 0 \\ -x & \mbox{ if } x\leq 0 \end{array}\right.
\]
\end{definition}

Geometrically, $|x|$ measures the distance from $x$ to $0$ along the real line, e.g. $|-3|=3$.

\begin{lemma}
For $x\in\mathbb{R}$,
\[
-|x|\leq x\leq |x|.
\]
\end{lemma}
\begin{proof}
If $x\geq 0$, then since $|x|\geq 0$,
\[
-|x|\leq 0\leq x=|x|
\]
and if $x\leq 0$, then $-x=|x|$, so $x=-|x|$, hence
\[
-|x|=x\leq 0\leq |x|.
\]
\end{proof}

\begin{protip}
{\bf Proofs with $|\cdot|$:} Since the definition of $|x|$ is defined in terms of cases, a proof involving absolute values may need to be split into cases depending on what sign $x$ has.
\end{protip}

\begin{example}
\label{ex:|x|^2=x^2}
For $x\in \mathbb{R}$, $|x|^2=x^2$.

\begin{proof}
We split the proof into two cases. \\

{\bf Case 1:} If $x\geq 0$. Then $|x|=x$ and so $|x|^2=x^2$, which proves the claim in this case.

{\bf Case 2:} If $x<0$, then $|x|=-x$, so $|x|^2=(-x)^2=(-1)^2x^2=1$.

 Thus, the claim is true in all cases.

\end{proof}
\end{example}

\begin{example}
For which $x\in\mathbb{R}$ do we have $|x-1|<|x+2|$?\\

Notice that the signs of the absolute value change as $x$ crosses $1$ and $-2$, so we split into cases depending on where $x$ lies in relation to these numbers:\\

{\bf Case 1:} If $x\leq -2$, then $x-1<0$, so $|x-1|=1-x$ and $x+2\leq 0$ so $|x+2|=-2-x$, so $|x-1|<|x+2|$ holds if and only if $1-x<-2-x$, or equivalently, $2<-1$, which never holds.

{\bf Case 2:} If $-2\leq x\leq 1$, then $|x-1|=1-x$ again but $|x+2|=x+2$, so $|x-1|<|x+2|$  if and only if $1-x<x+2$ , i.e. if $-1<2x$, so the only $x$'s between $-2$ and $1$ where the inequality holds is for $x>-\frac{1}{2}$.

{\bf Case 3:} If $x\geq 1$, then $|x-1|=x-1$ and $|x+2|=x+2$, so $|x-1|<|x+2|$ if and only if $x-1<x+2$, i.e. if $-1<2$, which always holds. \\

Bringing it all together, we see that our inequality holds if and only if $-\frac{1}{2}< x$.

\end{example}
\begin{lemma}
\label{l:|xy|=|x||y|}
For $x,y\in \mathbb{R}$, $|xy|=|x|\cdot |y|$.
\end{lemma}

We leave the proof as an exercise.

\begin{theorem}[The Triangle Inequality]
For $x,y\in\mathbb{R}$,
\[
|x+y|\leq |x|+|y|.
\]
\end{theorem}

\begin{proof}
We split into cases, and will use the fact that $\pm x\leq |x|$ by definition.

{\bf Case 1:}  If $x+y\geq 0$, then
\[
|x+y|=x+y\leq |x|+|y|.
\]

{\bf Case 2:}  If $x+y\leq 0$, then
\[
|x+y|=-(x+y)=-x+(-y)\leq |x|+|y|.
\]
Thus, the claim is true in all cases.

\end{proof}

We leave the following corollary as an exercise.

\begin{corollary}[The Reverse Triangle Inequality]
For $x,y\in\mathbb{R}$,
\[
\bigl||x|-|y|\bigr|\leq |x+y|.
\]
In particular,
\[
|x|-|y|\leq |x+y|.
\]

\end{corollary}


\section{Homogeneity and inequalities}%
\label{homogeneity}

Here we discuss a technique for proving a certain class of inequalities. Let's consider first the Cauchy-Schwartz inequality:
\[
ax+by \leq \sqrt{a^2+b^2}\sqrt{x^2+y^2}.
\]
Notice what happens if I replace $(x,y)$ with $(tx,ty)$ where $t>0$. On the left, I get
\[
a(tx)+a(ty)=t(ax+by)
\]
and on the right,
\[
\sqrt{a^2+b^2}\sqrt{(tx)^2+(ty)^2}
=\sqrt{a^2+b^2}\sqrt{t^2(x^2+y^2)}
=t \sqrt{a^2+b^2}\sqrt{x^2+y^2}.
\]
That is, when I replace $(x,y)$ with $(tx,ty)$, then a $t$ can be factored out from both ends, or in other words, the ratio of both sides of the inequality is unchanged if I replace $(x,y)$ with $(tx,ty)$. The same holds if I replace $(a,b)$ with $(sa,sb)$ for some $s>0$.

Thus, if I can prove the inequality for some $(tx,ty)$ and $(sa,sb)$ in place of $(x,y)$ and $(a,b)$, then it will also hold for $(x,y)$ and $(a,b)$.

How do I pick such an $s$ and $t$? Let's pick them so that they eliminate one side of our inequality. That is, let's pick them so that
\begin{equation}
\label{e:txty}
\sqrt{(tx)^2+(ty)^2}=\sqrt{(sa)^2+(sb)^2}=1.
\end{equation}
That is, we pick
\[
t=\frac{1}{\sqrt{x^2+y^{2}}}, \;\; s= \frac{1}{\sqrt{a^2+b^2}}.
\]
Let $(a',b')=(sa,sb)$ and $(x',y')=(tx,ty)$. We are now left to show
\[
a'x'+b'y'\leq 1.
\]
But notice that by Lemma \ref{l:little-cs},
\[
a'x'+b'y'\leq \frac{(a')^2 +(x')^2 + (b')^2 + (y')^2 }{2}
=\frac{(sa)^2+(sb)^2 + (tx)^2+(ty)^2}{2} \stackrel{\eqref{e:txty}}{=}\frac{1+1}{2}=1.
\]

Thus,
\[
ax+by =s^{-1}t^{-1} (a'x'+b'y')\leq s^{-1} t^{-1}=\sqrt{a^2+b^2}\sqrt{x^2+y^2}.
\]

\section{Exercises}%
\label{inequalitiesexercises}

See Liebeck Chapter 5 and also Exercises 15, 16 and 17 in Chapter 8.
%Many of the exercises below are from Beckenbach and Bellman's {\it Introduction to inequalities}.\\

%{\bf Note:} On exams and homeworks we won't expect you to cite each axiom when proving an inequality. However, for the exercises below, try and keep track of what axioms and results from this section to make sure you are not taking for granted an inequality

\begin{exercise}
Show that
\[
-\max\{a^2,b^2\}\leq ab\leq \max\{a^2,b^2\}.
\]

\begin{solution}
Suppose $|a|\leq |b|$ (the proof for the case that $|b|\leq |a|$ is the same).
By Exercise \ref{ex:|x|^2=x^2} and Lemma \ref{l:|xy|=|x||y|},
\[
ab\leq |ab|=|a|\cdot|b|\leq |b|\cdot |b|=|b|^2=b^2= \max\{a^2,b^2\}.
\]
Similarly,
\[
ab\geq -|ab|=-|a|\cdot |b| \geq -|b|^2=-b^2=- \max\{a^2,b^2\}.
\]

\end{solution}
\end{exercise}

\begin{exercise}
Show that for all $a,b,c\in\mathbb{R}$ we have  $|a-c|\leq |a-b|+|b-c|$.
\begin{solution}
We add and subtract $b$ from $a-c$ and use the triangle inequality:
\[
|a-c|=|a-b+b-c|\leq |a-b|+|b-c|.
\]
\end{solution}
\end{exercise}

\begin{exercise} Show that $|a+b|\geq |a|-|b|$. In fact, show that $|a+b|\geq \left||a|-|b|\right|$.
\end{exercise}

\begin{solution}
By applying the triangle inequality $|x+y|\leq |x|+|y|$ with $x=a+b$ and $y=-b$,
\[
|a|=|a+b-b|\leq |a+b|+|b|
\]
and subtracting $|b|$ from both sides gives $|a+b|\geq |a|-|b|$. A similar proof shows that $|a+b|\geq |b|-|a|=-(|a|-|b|)$. Since $||a|-|b||$ is either $\pm (|a|-|b|)$ (depending on whether $|a|-|b|$ is negative or not), this means $|a+b|\geq \left||a|-|b|\right|$.
\end{solution}

\begin{exercise} For $a,b>0$, show that
\[
\sqrt{ab}\geq \frac{2ab}{a+b}.
\]

\begin{solution}
Using Proposition \ref{p:xu>xv} first with $x=1/\sqrt{ab}$ (which is $\geq 0$ by Proposition \ref{p:1/x>0}) and then with $x=a+b$, we get
\[
\sqrt{ab}\geq \frac{2ab}{a+b} \;\; \; \Longleftrightarrow \;\;\;
1\geq \frac{2\sqrt{ab}}{a+b}
\;\; \; \Longleftrightarrow \;\;\; a+b\geq 2\sqrt{ab}
\;\; \; \Longleftrightarrow \;\;\;
a-2\sqrt{ab}+b=(\sqrt{a}-\sqrt{b})^{2}\geq 0
\]
and since this last inequality is always true, the original inequality is always true.

\end{solution}

\end{exercise}

\begin{exercise} Show that if $0<a\leq b$, then
\[
a\leq \frac{a+b}{2}\leq b.
\]
Show that either of these inequalities is an equality if and only if $a=b$.

\begin{solution}
Since $a\leq b$, we have $a+b\leq b+b=2b$, and so $\frac{a+b}{2} \leq b$. The other inequality has a similar proof. If either of the inequalities is an equality, we can just solve it to get $a=b$.
\end{solution}
\end{exercise}

\begin{exercise} Show that if $a,b,c,d>0$ and $0<\frac{a}{b}<\frac{c}{d}$, then
\[
\frac{a}{b} < \frac{a+c}{b+d}<\frac{c}{d}.
\]
\begin{solution}
Multiplying all sides by $b+d$, by Proposition \ref{p:xu>xv}, this string of inequalities is equivalent to
\[
\frac{a}{b}(b+d) < a+c <\frac{c}{d}(b+d)
\;\; \; \Longleftrightarrow \;\;\;
a+\frac{a}{b}d< a+c <\frac{d}{d}b+c.
\]
Since, $\frac{a}{b}<\frac{c}{d}$, we know $\frac{a}{b}d <\frac{c}{d}d=c$, and adding $a$ to both sides gives the first of the inequalities above. A similar argument gives the second inequality above.
\end{solution}

\end{exercise}

\begin{exercise} Show the following for $a,b,c,d\in\mathbb{R}$:
\begin{enumerate}[label=(\alph*)]
\item $(a^{2}-b^{2})(c^2-d^2)\leq (ac-bd)^2 $
\item $(a^2+b^2)(c^2+d^2)\geq (ac+bd)^2$.
\item $(a^2-b^2)^2\geq 4ab(a-b)^2$.
\end{enumerate}

\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item This inequality holds if and only if (after distributing the products)
\[
a^2c^2-a^2d^2-b^2c^2+b^2d^2\leq a^2c^2-2acbd+b^2d^2\]
and, after cancelling common terms, this is equivalent to
\[
-a^2d^2-b^2c^2\leq -2acbd \;\;\; \Longleftrightarrow a^2d^2+b^2c^2\geq 2acbd.
\]
Now we have a product of terms at most a sum, so this suggests using something like the AM-GM inequality. Indeed, the above inequality is true by the AM-GM inequality $\sqrt{xy}\leq \frac{x+y}{2}$ with $x=ad$ and $y=bc$:
\[
\frac{a^{2}d^{2}+b^2c^2}{2}
\geq abcd
\]
\item Again, by the AM-GM inequality
\begin{align*}
(ac+bd)^2
& = a^{2}c^{2}+2abcd+b^{2}d^{2}\\
& \leq a^{2}c^{2}+2\frac{a^2d^2+b^2c^2}{2}+b^{2}d^{2}\\
& =a^{2}c^{2}+a^{2}d^{2}+b^{2}c^{2}+b^{2}d^{2}\\
&= (ac+bd)^2.
\end{align*}

\item By the AM-GM inequality,
\[
(ab)^{\frac{1}{2}} \leq \frac{a+b}{2}
\]

 and so
\[
4ab(a-b)^2
\leq 4 \left(\frac{a+b}{2}\right)^2(a-b)^2=(a+b)^2(a-b)^2=((a+b)(a-b))^2=(a^2-b^2)^2.
\]
\end{enumerate}
\end{solution}

\end{exercise}

\begin{exercise} Show that for $a,b\in\R$ and $d>0$,

\[
ab \leq \frac{a^2}{d} + 2db^2.
\]
{\it Hint: First $ab=\frac{a}{c}(bc)$.}
\begin{solution}
Notice that by Lemma \ref{l:little-amgm} or the AM-GM inequality,
\[
ab = \left(\frac{a}{c}\right) (bc) \leq \frac{\frac{a^2}{c^2}+b^2c^2}{2}
\]
Set $c=\sqrt{2d}$, then gives the claim.
\end{solution}

\end{exercise}

%
%\begin{exercise} Let $a,b>0$. Show that
%\[
%\frac{2}{1/a+1/b}\leq \sqrt{ab}.
%\]
%
%\end{exercise}

\begin{exercise} Show that for $a>0$, $a+\frac{1}{a}\geq 2$.
\begin{solution}
This follows from Theorem \ref{t:AM-GM}:
\[
a+\frac{1}{a} \geq \sqrt{a\cdot \frac{1}{a}}=1.
\]
\end{solution}
\end{exercise}

\begin{exercise} Suppose $x,y>0$ and $r$ is a rational number so that $0<r\leq 1$. Use the AM-GM inequality to prove
\[
x^{r}y^{1-r} \leq rx+(1-r)y.
\]
\begin{solution}
Let $r=\frac{p}{q}$ For $i=1,...,p$, let $x_i=x$ and for $i=p+1,...,q$, let $x_i=y$. Then
\begin{align*}
x^{r}y^{1-r}
& =(x^{p}y^{1-p})^{\frac{1}{q}}
=(x_{1}\cdots x_{p}\cdot x_{p+1}\cdots x_{q} )^{\frac{1}{q}}\\
& \leq \frac{x_{1}+\cdots + x_{p}+x_{p+1}+\cdots + x_{q}}{q} \\
& =\frac{px+(q-p)y}{q}=\frac{p}{q} x+(1-\frac{p}{q})y=rx+(1-r)y.
\end{align*}
\end{solution}

\end{exercise}

%\begin{exercise}
%Prove that for $a,b>0$ and $p,q\in\mathbb{N}$ with $\frac{1}{p}+\frac{1}{q}=1$,
%\[
%ab\leq \frac{a^{p}}{p}+\frac{b^{q}}{q} .
%\]

%\end{exercise}

\begin{exercise} {\bf(Challenging!)} Prove that if $m_{1},...,m_{k}$ are positive integers and $y_{1},...,y_{k}>0$, then
\[
{m_1y_1+m_2y_2+\cdots+m_ky_k \over m_1+m_2+\cdots+m_k} \geq \sqrt[\large m_1+m_2+\cdots+m_k]{y_1^{m_1}\cdot y_2^{m_2}\cdots y_k^{m_k}}
\]

\begin{solution}
Let $n=m_{1}+\cdots + m_{k}$. Then
\begin{multline*}
{m_1y_1+m_2y_2+\cdots+m_ky_k \over m_1+m_2+\cdots+m_k}
=
{m_1y_1+m_2y_2+\cdots+m_ky_k \over m_1+m_2+\cdots+m_k} \\
=
\frac{\overbrace{(y_1+y_1+\dots+y_1)}^{m_1 \text{ times}}+\overbrace{(y_2+y_2+\dots+y_2)}^{m_2 \text{ times}}+\dots+\overbrace{(y_k+y_k+\dots+y_k)}^{m_k \text{ times}}}n \\
\qquad \geq  \sqrt[n]{\overbrace{(y_1 \cdot y_1 \dots y_1)}^{m_1 \text{ times}}\, \overbrace{(y_2 \cdot y_2 \dots y_2)}^{m_2 \text{ times}} \dots \overbrace{(y_k \cdot y_k \dots y_k)}^{m_k \text{ times}}}\\
=  \sqrt[n]{y_1^{m_1} y_2^{m_2} \dots y_k^{m_k}}\\
= \sqrt[\large m_1+m_2+\cdots+m_k]{y_1^{m_1}\cdot y_2^{m_2}\cdots y_k^{m_k}}.
\end{multline*}
\end{solution}
\end{exercise}

\begin{exercise} Given that $x,y,z\in\mathbb{N}$, solve
\[
\left(1+\frac{1}{x}\right)\left(1+\frac{1}{y}\right)\left(1+\frac{1}{z}\right)=3.
\]
\begin{solution}
We claim that the only solutions $(x,y,z)\in \mathbb N$ with $x\le y\le z$ are $(1,3,8)$, $(1,4,5)$, and $(2,2,3)$.

\begin{proof}
We can assume $0< x\leq y\leq z$. We see that the biggest factor $1+\frac1x$ is at least $\sqrt[3]3$, which implies $x\leq 2$, so $x=1$ or $2$.

\noindent {\bf Case 1:} $x=1$. Then the equation becomes
$$\left(1+\frac1y\right)\left(1+\frac1z\right)=\frac32$$
and so $1+\frac1y\geq\sqrt{\frac32}$, i.e. $y\leq 4$.
Thus the allowed cases $y=3$ and $y=4$ can be easily checked by hand: $y=3$ leads to $1+\frac1z = \frac98$, i.e. $z=8$, whereas $y=4$ leads to $1+\frac1z = \frac{6}{5}$, i.e. $z=5$.

\noindent {\bf Case 2:} $x=2$. Then the equation becomes
$$\left(1+\frac1y\right)\left(1+\frac1z\right)=2$$ and we conclude $1+\frac1y\ge\sqrt{2}$, i.e. $y\leq 2$ and together with $y\geq x$ this means $y=2$ and finally $1+\frac1z=\frac43$, i.e. $z=3$.
\end{proof}
\end{solution}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Induction}%
\label{induction}

Mathematical induction is a particular form of proof.
The goal of a proof by induction is to show that each member of an infinite family of statements is true.

\section{Preliminaries on $\Sigma$ notation}%
\label{summationnotation}

Before getting into induction, it will be useful to review $\Sigma$ notation.
$\Sigma$ notation is spoken as ``sigma notation'' after the Greek letter $\Sigma$.
\begin{definition}
Given a sequence of numbers $a_{m},a_{m+1},...,a_{n}$, we write
\[
\sum_{k=m}^{n} a_{k} = a_{m}+a_{m+1}+\cdots +a_{n}.
\]
\end{definition}
For example,
\[
\sum_{k=1}^{n} k^2 = 1^2+2^2+3^2+\cdots + n^2.
\]

We can split sums and use the distribution \[\sum_{k=m}^{n} (a_{k} +b_{k})=\sum_{k=m}^{n} a_{k} +\sum_{k=m}^{n} b_{k}\;\; \;\;\; \mbox{ and }\;\;\;\;\; \sum_{k=m}^{n} c\cdot a_{k} =c\cdot \sum_{k=m}^{n} a_{k} .
\]

{\bf Note:} this is why we sometimes call numbers {\it constants}: the number $c$ when it appears inside a sum does not vary as $k$ changes, and for this reason we can factor it outside the sum.

The following are useful manipulations of $\Sigma$ notation
\begin{enumerate}
\item Detaching the last element:
\[ \sum_{k=1}^{n+1} a_k = \left(\sum_{k=1}^{n} a_k\right) + a_{n+1}.\]
\item Differences
\[ \mbox{If } S_n=\sum_{k=1}^{n} a_k, \mbox{ then } S_{n+1}-S_n = a_{n+1}.\]
\item Change of variables:
\[
\sum_{k=m}^{n} a_{k+\ell} =\sum_{k=m+\ell}^{n+\ell} a_{k}.
\]
For example,
\[
\sum_{k=3}^{5} \frac{1}{(k+1)^2}
=\frac{1}{(3+1)^2}+\frac{1}{(4+1)^2}+\frac{1}{(5+1)^2}
=\frac{1}{4^2}+\frac{1}{5^2}+\frac{1}{6^2} = \sum_{k=4}^{6} \frac{1}{k^2}.
\]
\item Change of variables:
\[
\sum_{k=m}^{n} a_{k} =\sum_{j=m}^{n} a_{j}.
\]
\end{enumerate}

\section{Induction}%
\label{inductionsection}

The main topic of this chapter is the following fundamental property of the natural numbers/axiom of mathematics called the Principle of Mathematical Induction.

\begin{definition}[The Principle of Mathematical Induction]
Given a list of statements $P(k),P(k+1),...$, we may conclude that $P(n)$ is true for every integer $n\geqslant k$ {\em provided that}

\begin{itemize}
	\item we know that $P(k)$ is true (base case)
\item and we can prove that $P(n)\Rightarrow P(n+1)$ for any integer $n\geqslant k$ (induction step).
\end{itemize}
\end{definition}

The basic idea of an induction proof is to proceed step by step.
We prove the base case is true, and the induction step retains truth as we move from $P(n)$ to $P(n+1)$.

A full written proof by induction has the following four parts clearly separated.
\begin{enumerate}
	\item A clear and explicit statement of the ``induction hypothesis'' $P(n)$ and the intention to prove by induction.
	\item Prove a ``base case'' is true. Typically prove the single statement $P(1)$.
	\item Prove the ``induction step''.  Typically, if $P(n)$ is true then $P(n+1)$ is also true.
	\item Conclude that steps (2) and (3) allow the conclusion that $P(n)$ is true for all natural numbers $n$ by the principal of mathematical induction.
\end{enumerate}
Mathematics written for experts might skimp on some of the detail, e.g. saying `` the base case is trivial''. In this course your proofs must be written systematically using all four parts of the format described above.

\begin{example}
For $n\in\mathbb{N}$, $\sum_{k=1}^{n}k=\frac{n(n+1)}{2}$.

\begin{proof}
Let $P(n)$ be the statement that $\sum_{k=1}^{n}k=\frac{n(n+1)}{2}$. We prove that $P(n)$ is true for all $n\in\mathbb{N}$ by induction as follows:\\

\begin{itemize}
\item  {\bf Base Case:} First let's prove $P(1)$. This is just
\[
\sum_{k=1}^{1}k=1=\frac{1(1+1)}{2}.
\]
Thus, the base case is true. \\

\item  {\bf Induction Step:} Assume $P(n)$ holds and consider
\[
\sum_{k=1}^{n+1}k =\left({\color{magenta} \sum_{k=1}^{n} k}\right) + (n+1)  = {\color{magenta} \frac{n(n+1)}{2}} +(n+1) = \frac{(n+1)(n+2)}{2}.
\]
where we used the statement $P(n)$ to replace \(\sum_{k=1}^{n} k\) with \(\frac{n(n+1)}{2} \).
Hence \(\sum_{k=1}^{n+1}k=\frac{(n+1)(n+2)}{2}\) which proves that
$P(n)\Rightarrow P(n+1)$ is true.
\end{itemize}
Since we have shown the base case and induction step, $P(n)$ is true for all $n$ by induction.
\end{proof}
\end{example}

\begin{exercise}
Prove the following formulas by induction:
\begin{enumerate}[label=(\alph*)]
\item $\sum_{k=1}^{n} k^{2} = \frac{n(n+1)(2n+1)}{6}$.
\item $\sum_{k=1}^{n} k^{3}  = \left(\frac{n(n+1)}{2}\right)^{2}$.
\end{enumerate}
It turns out that for all powers $p$ there is a degree $p+1$ polynomial that gives a formula for the value of $\sum_{k=1}^{n} k^{p}$. To learn more, read up on {\it Bernoulli numbers}.
\end{exercise}

%
%\fbox{%
%    \parbox{\textwidth}{{\bf Sidenote:}
%The above formula $\sum_{k=1}^{n}k=\frac{n(n+1)}{2}$ has been known for ages. It was shown, for example, by Abu Bakr Muḥammad ibn al Ḥasan al-Karaji (c. 953 – c. 1029), and he also found other formulae for $\sum_{k=1}^{n}k^2$ and $\sum_{k=1}^{n}k^3$. However, his proofs were not induction as we know today: he proved his formulas for the first 5 values of $n$ and then said the other statements could be proven similarly.
%
%There is a famous story of Carl Friedrich Gauss (1777-1855) giving another proof of this formula when he was 10: His teacher one day asked students to sum up all integers from $1$ to $100$, and when they were done they were to bring their slate up to the teacher. In a few seconds, Gauss took his slate up to the desk. He explained to his teacher that he figured it out so quickly because he realized that
%\[
%1+2+3+\cdots + 100 = (1+100)+(2+99)+(3+98)+\cdots (50+51) = 50\cdot 101.\]
%
%}}

Some statements may not be true for all $n\geq 1$ but maybe for $n\geq k$ for some $k$, and depending on the problem you will need to figure out what this $k$ is.

%\vspace{10pt}
%When should you use induction? There are a couple of things to look out for:

%Induction is very useful when:
%
%\begin{itemize}
%\item A statement has a nested structure: each statement builds off the last.
%\item A statement involves a clear multi-step algorithm.
%\item A statement involves an identity for a complicated sum.
%\end{itemize}

%{\bf Induction is difficult when:}
%
%\begin{itemize}
%\item The statement for $n$ is not so clearly related to the statement for $n-1$.
%\item There is no clear base case.
%\item For sums, when you don't have a guess for a general answer.
%\end{itemize}

%Not all lists of statements $P(n)$ are true for $n=1$ but for all sufficiently large $n$:

\begin{example}
For which $n\in\mathbb{N}$ do we have $2^{n}<n!$? If we plug in $n=1,2,3$ this inequality fails, but for $n\geq 4$ it starts to hold. This gives rise to the following conjecture:\\

{\bf Claim:} $2^{n}<n!$ for $n\geq 4$.

\begin{proof}
We prove by induction.  Let $P(n)$ be the statement that $2^{n}<n!$.

\begin{itemize}
\item {\bf Base case:} The base case is $n=4$, and it holds because $2^{4} = 16 < 4!=24$. \\

\item {\bf Induction Step:} Assume $P(n)$ is true, i.e. {\color{magenta} $2^{n}<n!$} for some $n\geq 4$, then
\[
2^{n+1} = {\color{magenta} 2^{n}}\cdot 2 < {\color{magenta}n!}\cdot 2 < n! \cdot (n+1)=(n+1)!.
\]
\end{itemize}
Since we have shown the base case and induction step, $P(n)$ is true for all $n\geq 4$ by induction.
\end{proof}
\end{example}

Some problems can be proven with or without induction:

\begin{exercise}
Prove that $n^{3}-n$ is divisible by $6$ for all $n\in\mathbb{N}$, first by induction, then without induction.
\end{exercise}

The following formula will be very useful throughout this course:

\begin{theorem}[Geometric Series Formula]
Let $x\neq 1$ be a real number. Then for $N\in\mathbb{N}$,
\begin{equation}
\label{e:gs}
\sum_{n=1}^{N} x^{n}=\frac{x-x^{N+1}}{1-x} \;\;\;and \;\;\;  \sum_{n=0}^{N} x^{n}=\frac{1-x^{N+1}}{1-x}
\end{equation}
\end{theorem}

\begin{proof}
The second equation follows from the first (just by adding $1$), so we just need to prove the first. We prove this by induction.
\begin{itemize}
\item {\bf Base case:} If $N=1$, then
\[
 \frac{x-x^{2}}{1-x} = \frac{x(1-x)}{1-x}=x=\sum_{n=1}^{1} x^{n}.
 \]
 \item {\bf Induction Step:} Suppose \eqref{e:gs} holds for some $N$. Then
 \[
 \sum_{n=1}^{N+1} x^{n}
 =x^{N+1}+ \sum_{n=1}^{N} x^{n}
 =x^{N+1} + \frac{x-x^{N+1}}{1-x}
 =\frac{x^{N+1}-x^{N+2}}{1-x} + \frac{x-x^{N+1}}{1-x}
 =\frac{x-x^{N+2}}{1-x}.
 \]
 This proves the induction step. Thus, \eqref{e:gs} holds for all $N\in\mathbb{N}$ by induction.
 \end{itemize}
\end{proof}

%\begin{itemize}
%\item See if it is a nested or recursive type statement.
%\item Try and work backwards.  Start from $P(n+1)$ and try to find an occurrence where you can use the assumption of $P(n)$.
%\item If working out an identity for sums, you have to guess at the general form.
%\end{itemize}

%
%
%\frametitle{Tower of Hanoi}
%\begin{center}
% \includegraphics[height=1.25in]{Tower-of-Hanoi.jpg}
%\end{center}
%{\bf Rules:}
%\begin{enumerate}
%\item We start $n$ disks with holes in the center, stacked on one rod from largest to smallest (with largest on bottom).
%\item The objective is to move the disks from the first rod to the third.
%\item You can only move one disk at a time.
%\item You can place a disk on top of any other larger disk (i.e. no disk can be under a larger disk).
%\end{enumerate}
%

\section{Strong Mathematical Induction}%
\label{stronginduction}
Sometimes it's not enough to know that $P(n)$ is true in order to prove that $P(n+1)$ is true; we might additionally need to use that some of the previous statements $P(k), P(k+1), \dots , P(n-1)$ are true.  \\
%
%\fbox{%
%    \parbox{\textwidth}{
%\begin{description}
%\item[Principle of Strong Mathematical Induction]
%  Let $k\in \mathbb{N}$ and let $P(k),P(k+1),...$ are statements. Suppose that\
%  \begin{itemize}
%  \item $P(k)$ is true,  (``base case)\
%  \item for any integer
%$n\geqslant k$:   $$ P(k),P(k+1),P(k+2),\ldots,P(n)\Longrightarrow
%P(n+1).$$ (``induction step).
%
%\end{itemize}
%
%  Then $P(n)$ is true for every positive integer $n\geqslant
%k$.
%\end{description}
%}
%}

\begin{definition}[Principle of Strong Mathematical Induction]
  Let $k\in \mathbb{N}$ and let $P(k),P(k+1),...$ be statements. Suppose that\
  \begin{itemize}
  \item $P(k)$ is true,  (base case)\
  \item for any integer
$n\geqslant k$, statements
\[ P(k),P(k+1),P(k+2),\ldots,P(n)\mbox{{\color{blue} (all taken together)}}\Longrightarrow
P(n+1)\]
(induction step).
\end{itemize}
Then $P(n)$ is true for every positive integer $n\geqslant k$.
\end{definition}

\vspace{10pt}

Strong induction is needed when you need to make use of several previous statements to prove the next one. For example:

\begin{theorem}
Every integer $n\geq 2$ can be written as a product of primes $n=p_{1}\cdots p_{k}$.
\end{theorem}

If $n$ is prime, this statement still makes sense: we just interpret $n$ as being the product of just one number, $n$ itself.

\begin{proof}
We prove by strong induction on $n$. The case $n=2$ immediately holds (taking into account the previous remark). For the induction step, suppose the theorem holds for all integers $n<N$ (this is our strong inductive hypothesis).  If $N$ is prime, there is nothing to prove; otherwise, if $N$ is not prime, then $N=a\cdot b$ for some positive integers $a$ and $b$ greater than $1$. Both $a$ and $b$ can be  decomposed by the strong induction hypothesis, thus so can $n=ab$.
\end{proof}

Another situation when you need strong induction is when studying recurrence relations. A {\it recurrence} relation is a sequence $a_{1},a_{2},...$ of numbers  where each $a_{n}$ is defined in terms of previous terms in the sequence. The most famous recurrence relation is the {\it Fibonacci sequence}, which is defined  as $f_{1}=1$, $f_{2}=1$ and for $n>2$, $f_{n}=f_{n-1}+f_{n-2}$.

\begin{example}
Let $x_{1}=1, x_{2} =3$ and for $n\geq 2$, let
\[
x_{n+1} = 4x_{n}-3x_{n-1}.
\]
Let's try and find a general pattern for what $x_{n}$ is. If we test the first few values, we get
\[
x_{1} = 1, \;\; x_{2} = 3 , \;\; x_{3} = 4\cdot 3-3\cdot 1 = 9, \;\; x_{4} = 4\cdot 9 - 3\cdot 3 = 27
\]
so it seems like $x_{n} = 3^{n-1}$. We will prove this by strong induction. Let $P(n)$ be the statement ``$x_{n} = 3^{n-1}$.'' Let's first prove the first two base cases: $x_{1}=1=3^{1-1}$ and $x_{2}=3=3^{2-1}$, so $P(1)$ and $P(2)$ are true. Now  suppose $P(k)$ is true for $k=1,2,...,n$ for some $n\geq 2$. Then
\[
x_{n+1} =4x_{n}-3x_{n-1} = 4\cdot 3^{n-1} -3x_{n-1} = 4\cdot 3^{n-1} - 3\cdot 3^{n-2} = 3^{n} = 3^{(n+1)-1}.
\]
This proves the induction step, and thus $x_n=3^{n-1}$ for all $n$ by strong induction.\\

{\bf Discussion:} {\it Why did we prove two base cases?} Note that the formula $x_{n+1} =4x_{n}-3x_{n-1} $ holds only for $n\geq 2$, so in particular, I can't use it to prove $P(2)$ (i.e. that $x_{2}=3^{2-1}$). For this reason, I need to prove a few more base cases that can't be covered in my induction step. \\

{\it Why did we need strong induction instead of induction?}  I needed both $P(n)$ and $P(n-1)$ to be true to prove $P(n+1)$, but with standard induction I would need to prove $P(n+1)$ follows from just $P(n)$.\\
\end{example}

\begin{protip}
{\it How do I know when to use induction vs. strong induction?} One way is to first try proving the induction step to see how many previous cases you need, then that will tell you how many base cases you need as well: if it is more than one, then you need strong induction.
\end{protip}

\section{Inequalities revisited}%
\label{inequalitiesredux}

Powered with induction, we can now prove some more useful inequalities.

\begin{proposition}
Suppose $x,y>0$ and $n\in\mathbb{N}$.
\begin{enumerate}[label=(\alph*)]
\item $x^n>y^n$ if and only if $x>y$.
\item $x^{\frac{1}{n}}>y^{\frac{1}{n}}$ if and only if $x>y$.
\end{enumerate}
\end{proposition}

\begin{proof}
Let $x,y>0$.
\begin{enumerate}[label=(\alph*)]
\item First we show $x>y$ implies $x^n>y^n$. We prove this by induction. The $n=1$ case is trivial. Suppose the claim is true for some integer $n$, we will prove the $n+1$ case. Let $x>y>0$. Then
\[
x^{n+1}-y^{n+1}
=y^{n+1}\left(\left(\frac{x}{y}\right)^{n+1}-1\right).
\]
Let $a=\frac{x}{y}$. Since $x>y>0$, Proposition \ref{p:1/x>0} implies $1/y>0$ and  Proposition \ref{p:xu>xv} implies $x\cdot \frac{1}{y}>y\cdot\frac{1}{y}$, that is, $\frac{x}{y}>1$. Letting $a=\frac{x}{y}$,
\[
x^{n+1}-y^{n+1}
=y^{n+1}\left(a^{n+1}-1\right)
=y^{n+1}(a^{n}a-1)
\geq y^{n+1}(a^{n}\cdot 1-1)
= y^{n+1}(a^{n}-1).
\]
By the induction hypothesis, we know $x^n>y^n$, which implies $a^{n}=\frac{x^{n}}{y^{n}}>1$, so the above quantity is positive. Thus, $x>y>0$ implies $x^n>y^n$.

The converse statement is $x^n>y^n$ implies $x>y$, which is equivalent to the contrapositive statement that $x\leq y$ implies $x^{n}\leq y^{n}$. If $x=y$, then clearly $x^n=y^n$; if $x<y$, then $x^n<y^n$ follows from the above proof.

\item By part (a), $x^{\frac{1}{n}}>y^{\frac{1}{n}}$ if and only if $(x^{\frac{1}{n}})^{n}>(y^{\frac{1}{n}})^{n}$, that is, $x>y$.
\end{enumerate}
\end{proof}

Let's finish proving the Cauchy-Schwarz inequality:
\[
x_{1}y_{1}+\cdots + x_{n}y_{n}
\leq \sqrt{x_{1}^2+\cdots + x_{n}^{2}} \sqrt{y_{1}^2+\cdots + y_{n}^{2}} \;\;\; \mbox{ for } \;\;\; x_{1},...,x_{n},y_{1},...,y_{n}\in\mathbb{R}.
\]
We prove this by Strong Induction.  We already proved the base case $n=2$. Suppose we have shown the above inequality for $n=2,3,...,N$. By applying the $n=N$ and $n=2$ cases in succession,

\begin{align*}
x_{1}y_{1}+\cdots + x_{N}y_{N}+x_{N+1}y_{N+1}
& \leq \sqrt{x_{1}^2+\cdots + x_{N}^{2}} \sqrt{y_{1}^2+\cdots + y_{N}^{2}}+x_{N+1}y_{N+1}\\
& \leq  \left(  \sqrt{x_{1}^2+\cdots + x_{N}^{2}}^{2}+x_{N+1}^{2}\right)^{\frac{1}{2}}
\left(  \sqrt{y_{1}^2+\cdots + y_{N}^{2}}^{2}+y_{N+1}^{2}\right)^{\frac{1}{2}}\\
& =\left(x_{1}^{2}+\cdots + x_{N}^{2}+x_{N+1}^{2}\right)^{\frac{1}{2}}
\left(y_{1}^{2}+\cdots + y_{N}^{2}+y_{N+1}^{2}\right)^{\frac{1}{2}}
\end{align*}
This proves the induction step, and thus proves the Cauchy-Schwarz inequality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercises}%
\label{inductionexercises}

The relevant exercises in Liebeck are in Chapter 8.

\begin{exercise}
(Aug 2018 Exam) Show that, for all $n\geq 0$, $\sum_{k=1}^{n} k\cdot k! = (n+1)!-1$.
\begin{solution}
The base case is $\sum_{k=1}^{1} k\cdot k!=1=2!-1$. For the induction step, assuming the $(n-1)$st case is true,
\[
\sum_{k=1}^{n} k\cdot k!
=\sum_{k=1}^{n-1} k\cdot k! + n\cdot n!
=n!-1+ n\cdot n!
=(n+1)\cdot n! -1 = (n+1)!-1.
\]
\end{solution}
\end{exercise}

\begin{exercise}
Show that for all $n\geq 0$, $\sum_{k=1}^{n} k(k+1) = \frac{n(n+1)(n+2)}{3}$.
\begin{solution}
The base case is $\sum_{k=1}^{1} k(k+1) = \frac{1(1+1)(1+2)}{3}$.
For the induction step assume $\sum_{k=1}^{n} k(k+1) = \frac{n(n+1)(n+2)}{3}$ and consider
\[
\sum_{k=1}^{n+1} k(k+1) =
\sum_{k=1}^{n} k(k+1) +(n+1)(n+2) \]
\[
= \frac{n(n+1)(n+2)}{3}+(n+1)(n+2)
=\frac{(n+1)(n+2)(n+3)}{3}.
\]
\end{solution}
\end{exercise}

\begin{exercise}
Show that $n!\leq n^{n}$ for all $n\geq 1$
\begin{solution}
The base holds immediately. Assume $n\geq 1$ is so that $n^{n}\geq n!$. Then
\[
(n+1)!=n! (n+1)\leq n^{n} (n+1)<(n+1)^{n}(n+1)=(n+1)^{n+1}.
\]
\end{solution}
\end{exercise}

\begin{exercise} Suppose $x_{1}=1$ and $x_{n+1} = \sqrt{1+2x_{n}}$ for all $n\geq 1$. Show $x_{n}\leq 4$ for all $n$. \\
\begin{solution}
This clearly holds for $n=1$. Assume $x_{n}\leq 4$, then
\[
x_{n+1}= \sqrt{1+2\cdot x_{n}}\leq \sqrt{1+2\cdot 4}=\sqrt{9}=3<4\]
and thus the induction step also holds.
\end{solution}
\end{exercise}

\begin{exercise}  Let $f_{1}=f_{2}=1$ and $f_{n}=f_{n-1}+f_{n-2}$ for $n>2$.
\begin{enumerate}[label=(\alph*)]
\item Show that $3|f_{4n}$ for all $n\geq 1$.
\item Show that $1\leq f_{n+1}/f_{n}\leq 2$ for all $n\geq 1$.
\end{enumerate}

\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item We prove by induction. We can compute that $f_{4}=3$, which establishes the base case $n=1$. Now assume $3|f_{4n}$ for some $n\geq 1$, we'll show $3|f_{4(n+1)}$:
\[
f_{4(n+1)}=f_{4n+3}+f_{4n+2}
=2f_{4n+2}+f_{4n+1}
=2(f_{4n+1}+f_{4n})+f_{4n+1}
=3f_{4n+1}+2f_{4n}.
\]
By assumption, $3|f_{4n}$, and clearly $3|3f_{4n+1}$, and so $3|f_{4(n+1)}$, which proves the induction step and hence the claim.
\item It is clear the inequalities hold for $n=1$. Assume we have shown it to hold for some $n$. Then
\[
\frac{f_{n+2}}{f_{n+1}}=1+\frac{f_{n}}{f_{n}}\leq 1+1=2
\]
and
\[
\frac{f_{n+2}}{f_{n+1}}=1+\frac{f_{n}}{f_{n}}\geq 1+0=1.\]
This proves the induction step and hence the claim.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Let $F_{n}=2^{2^{n}}+1$ for $n\geq 0$. Prove that for $n>0$,
\[
F_{n} = F_{n-1}\cdots F_{0}+2.
\]
\begin{solution}
We prove by induction. The base case $n=1$ is immediate. Suppose the statement is true for some $n\geq 1$. Then
\[
F_{n}\cdot F_{n-1}\cdots F_{0}+2
= F_{n}(F_{n}-2)+2
=F_{n}^{2}-2F_{n}+2
=(F_{n}-1)^2+1
=(2^{2^{n}})^2+1 = 2^{2^{n+1}}+1 =F_{n+1}.
\]
\end{solution}
\end{exercise}

\begin{exercise}
Find a formula for the number of diagonals in a convex polygon with $n\geq 3$ vertices.
\begin{solution}
Let $f(n)$ denote the number of diagonals in a convex polygon of $n$ vertices. We will find a recurrence relation that $f$ satisfies.

Suppose we have a convex  polygon with $n+1$ vertices. Draw an edge between the second vertex $a$ and last vertex $c$, so splits the polygon into a triangle between $a$, $c$, and the first vertex $c$ and a polygon with $n$ vertices. The number of diagonals in this polygon (which are diagonals for the original polygon) are $f(n)$ many. The ones that are diagonals for the original polygon that weren't counted are the ones that have $b$ as an endpoint (of which there are $n+1-3=n-2$ many) and the edge that we first drew. Thus the total is
\begin{align*}
f(n+1)
& =f(n)+n-2+1=f(n)+n-1  = f(n-1) + n-2+n-1=\cdots \\
& = f(3) + \sum_{k=2}^{n-1}k
=0+\sum_{k=1}^{n-1}k-1 = \frac{n(n-1)}{2}-1.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that for all integers $n\geq 3$, there are {\it distinct} integers $x_{1},...,x_{n}$ so that
\[
\sum_{k=1}^{n} \frac{1}{x_{k}}=1.
\]
(It is an open question whether you can always do this with $x_{n}$ distinct {\it odd} numbers.)
\begin{solution}
We prove this by induction. For the base case $n=3$, we see that
\[
1=\frac{1}{2}+\frac{1}{3}+\frac{1}{6}.
\]
 Suppose the claim is true for some integer $n\geq 3$. Let $x_{1},...,x_{n}$ be so that
\[
\sum_{k=1}^{n} \frac{1}{x_{k}}=1.
\]
Note that we must have $x_{k}>1$ for all $k$. Then
\[
\sum_{k=1}^{n} \frac{1}{2x_{k}}=\frac{1}{2}.
\]
Hence,
\[
\frac{1}{2} + \frac{1}{2x_{1}}+\cdots + \frac{1}{2x_{n}}=\frac{1}{2}+\frac{1}{2} =1,
\]
Note that as $x_{k}\geq 2$ for all $k$, $2x_{k}\neq 2$ for all $k$, thus the integers $2,2x_{1},...,2x_{n}$ are $n+1$ distinct integers whose reciprocals add up to one, which proves the induction step and hence the claim.
\end{solution}
\end{exercise}

\begin{exercise} Joseph Bertrand conjectured in 1845 (and it was proven by Chebychev in 1852) that, if $p_{1},p_{2},...$ are the primes in ascending order, then
\[
p_{n+1}<2p_{n}.
\]
This is called {\bf Bertrand's postulate}. Using this result, show that for all $n\geq 4$ we have
\[
p_{n} <\sum_{k=1}^{n-1} p_{k}.
\]
\begin{solution}
We prove this by induction. For $n=4$, we have
\[
p_{4}=7 < 2+3+5 = \sum_{k=1}^{3} p_{k}.
\]
This establishes the base case. Now assume the statement is true for some integer $n$. Then
\[
p_{n+1}<2p_{n} = p_{n}+p_{n} < p_{n}+\sum_{k=1}^{n-1} p_{k} = \sum_{k=1}^{n} p_{k}.
\]
This proves the induction step and hence the claim.
\end{solution}
\end{exercise}

\begin{exercise}
\label{ex:AMGM}
 {\bf Challenge:} In this exercise, we will prove the Arithmetic-Geometric Inequality: for $x_{1},...,x_{n}\geq 0$,
\[
(x_{1}\cdots x_{n})^{\frac{1}{n}}\leq \frac{x_{1}+\cdots + x_{n}}{n}.
\]
We will do this in several steps.

\begin{enumerate}[label=(\alph*)]
\item Prove that for all $n\in \mathbb{N}$, if $x_{1},...,x_{2^n}$ are positive numbers, then

\[
(x_{1}x_{2}\cdots x_{2^n})^{1/2^{n}} \leq \frac{x_{1}+\cdots + x_{2^n}}{2^n}.
\]

\begin{solution}
This follows by induction on $n$. The $n=2$ case was shown just below the statement of Theorem \ref{t:AM-GM}. For the induction step, assume the $n$th case is true for some $n\geq 2$, we can prove the $(n+1)$st case using the base case: Let $a=(x_{1}\cdots x_{2^{n}})^{\frac{1}{2^{n}}}$ and $y=(x_{2^{n}+1}\cdots x_{2^{n+1}})^{\frac{1}{2^{n}}}$. Then
\[
(x_{1}....x_{2^{n+1}})^{\frac{1}{2^{n+1}}}
=\sqrt{ab}\leq \frac{a+b}{2}
\leq \frac{x_{1}+\cdots + x_{2^{n}}+x_{2^{n}+1}+\cdots + x_{2^{n+1}}}{2^{n+1}}\]
where in the last line we used the induction hypothesis.\end{solution}

\item Use  the previous case to prove the AM-GM inequality. {\it Hint: Let $2^{m}>n$ and define a new sequence $y_{1},...,y_{2^{m}}$ with $y_{i}=x_{i}$ for $i=1,2,...,n$ and $y_{i}=A$ otherwise where $A=\frac{x_{1}+\cdots + x_{n}}{n}$. Apply the previous case to the product of the $y_{i}$ and find an appropriate choice of $A$ that will imply the inequality for the $x_{i}$.}

\begin{solution}
With the notation as in the hint, we see that
\begin{align*}
(x_{1}\cdots x_{n}A^{2^{m}-n})^{\frac{1}{2^{m}}}
& = (y_{1}\cdots y_{2^{m}})^{\frac{1}{2^{m}}}\\
& \leq \frac{y_{1}+\cdots + y_{2^{m}}}{2^{m}}\\
& =\frac{x_{1}+\cdots + x_{n} + (2^{m}-n)A}{2^{m}}\\
& = \frac{nA+(2^{m}-n)A}{2^{m}}\\
& = A
\end{align*}
Taking both sides to the power $\frac{2^{m}}{n}$, we get
\[
(x_{1}\cdots x_{n}A^{2^{m}-n})^{\frac{1}{n}}
\leq A^{\frac{2^{m}}{n}}.
\]
Now moving the $A$ from the left side to the right, we get
\[
(x_{1}\cdots x_{n})^{\frac{1}{n}}\leq A=\frac{x_{1}+\cdots + x_{n}}{n}.
\]
\end{solution}
\end{enumerate}
\end{exercise}

%\item  Suppose two players play a game, where they are given $N$ stones and each player takes turns removing up to 4 stones from the pile (so they can remove 1,2,3, or 4 stones, and they have to take at least 1). The winner is the person who removes the last stone. Conjecture and prove a rule, depending on $N$, that determines whether the first player or second player has a winning strategy.
%
%\begin{solution}
%If $N=5n$ for some integer $n\geq 1$, then player 2 always wins. We prove this by induction. If $n=1$, then regardless of how many stones Player 1 takes, Player 2 can take the rest and win. Now suppose we know that player 2 always wins if $N=5n$ for some integer $n$. If $N=5(n+1)$, and player 1 takes away 1,2,3, or 4 stones, then player 2 can take away 4,3,2, or 1 stones so that there are only 5N left. The induction hypothesis says that, now that it is player 1's turn, player 2 has a winning strategy.
%
%If $n=5n+j$ for some $j\in \{1,2,3,4\}$, then player 1 has a winning strategy. Indeed, player 1 needs to remove j stones so that there are $5n$, now the first part of the problem says that player 1 (who now has second move since it is player 2's turn) has a winning strategy.
%
%\end{solution}
%

%Exam 2019
%\item Let $u_{n}$ be the sequence defined by $u_{1}=u_{2}=1$ and for $n>2$, $u_{n+1}=u_{n}+u_{n-1}$ (these are the Fibonacci numbers). Show that $u_{n}^2=u_{n-1}u_{n+1}+(-1)^{n-1}$ for all integers $n\geq 2$.
%
%\begin{solution}
%We prove by induction. The base cases $n=2$ and $n=3$ can easily be established, so let $n>2$ and assume we have verified that $u_{k}^2=u_{k-1}u_{k+1}+(-1)^{n-1}$ for all $k\leq n$. Then
%\begin{align*}
%u_{n+1}^2
%& = (u_{n+2}-u_{n})(u_{n}+u_{n-1})\\
%& =u_{n+2}u_{n}+u_{n+2}u_{n-1}-u_{n}^2-u_{n}u_{n-1}\\
%& = u_{n+2}u_{n}+u_{n+2}u_{n-1}-(u_{n+1}u_{n}+(-1)^{n-1})-u_{n}u_{n-1}\\
%& = u_{n+2}u_{n}+(u_{n+1}+u_{n})u_{n-1}-u_{n+1}u_{n}+(-1)^{n}-u_{n}u_{n-1}\\
%& =  u_{n+2}u_{n}+u_{n+1}u_{n-1}+u_{n}u_{n-1}-u_{n+1}u_{n}+(-1)^{n}-u_{n}u_{n-1}\\
%& = u_{n+2}u_{n}+(-1)^{n}.
%\end{align*}
%This proves the induction step and hence the claim.
%\end{solution}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 3: Analysis}

\chapterimage{Figures/blank.png}

\chapter{Upper Bounds and Least Upper Bounds}%
\label{upperbounds}

For the next two weeks, we'll learn about the basics of {\it analysis}. This branch of mathematics studies limits and approximations, and you may have already met these when studying calculus. If you end up enjoying this topic, you might also like the following courses in subsequent years:

\begin{itemize}
\item Fundamentals of Pure Mathematics, 2nd year.
\item Honours Analysis, 3rd year.
\item Linear Analysis, Essentials in Analysis and Probability, Functional Analysis, Fourier Analysis, 4th year.
\end{itemize}

\medskip
{\bf Why care?} Firstly, a lot of what we now call analysis was not very rigorous prior to the 1800s. The mathematician Niels Abel went so far as to say the following about the state of analysis at the time:\\

{\it  I shall devote all my efforts to bring light into the immense obscurity that today reigns in Analysis. It so lacks any plan or system, that one is really astonished that so many people devote themselves to it -- and, still worse, it is absolutely devoid of any rigour.}\footnote{Niels Abel, {\it Oeuvres (1826).}}\\

To give an idea about what he was talking about, Leibniz (that is, together with Newton, one of the founders of modern calculus) believed at one time that
\[
\sum_{n=1}^{\infty} (-1)^{n} = \frac{1}{2},
\]
but with modern rigour we now understand that this is sadly not true. In fact, this sum is meaningless, as we shall see later. Making analysis rigorous was a big problem since it is the foundation of calculus:  many of the computations you perform in calculus, such as taking derivatives and evaluating integrals, in fact rely on analysis under the bonnet. More shockingly, we need tools from analysis even to just take $n$'th roots, as we shall see later as well. Before getting into limits, however, we will need to discuss upper and lower bounds for sets, and to introduce a final axiom for the real numbers.

\section{Upper and Lower Bounds}%
\label{upperlowerbounds}

\def\LUB{{\rm LUB}}
\def\GLB{{\rm GLB}}
\def\ve{\varepsilon}
\def\limn{\lim_{n\rightarrow\infty}}

\def\R{\mathbb{R}}

\begin{definition}
Let $A$ be a nonempty subset of $\R$.
\begin{itemize}
\item $A$ is said to be {\it bounded above} if
there exists $M\in\R$ such that $x\leq M$ for all $x\in A$. Such a number $M$ is called an {\it upper bound} for $A$. If no upper bound exists, we say $A$ is {\it unbounded above}.

\item Similarly, $A$ is said to be {\it bounded below} if
there exists $m\in\R$ such that $x\geq m$ for all $x\in A$. Such a number $m$ is called a {\it lower bound} for $A$. If no lower bound exists, we say $A$ is {\it unbounded below}.

\item We say $A$ is {\it bounded} if it is both bounded above and bounded below.\\
\end{itemize}
\end{definition}

\begin{exercise}
\label{ex:bounded}
Show that $A$ is bounded if and only if there is $t\geq 0$ so that  $|x|\leq t$ for all $x\in A$.
\begin{solution}
If $A$ is bounded, there are $m,M\in\mathbb{R}$ so that $m\leq x\leq M$ for all $x\in A$. We claim that $|x|\leq \max\{|m|,|M|\}$ for all $x\in A$. Indeed, if $x\in A$ then
\[
x\leq M\leq  |M| \leq \max\{|m|,|M|\},
\]
and similarly
\[
x\geq m \geq - |m| \geq -\max\{|m|,|M|\}
\]
so that
\[|x|\leq \max\{|m|,|M|\}\]
for all $x\in A$.
Conversely, if $|x|\leq t$ for all $x\in A$, then $-t\leq x\leq t$ for all $x\in A$ (since either $x\geq 0$ in which case $x=|x|\leq t$, or $x\leq 0$, so $x=-|x|\geq -t$). Thus, $-t$ is a lower bound and $t$ is an upper bound for $A$.
\end{solution}
\end{exercise}

Recall that we write intervals using the notation
\[
[a,b]=\{x\in\mathbb{R} \;\; | \;\; a\leq x\leq b\}, \;\;\;
(a,b)=\{x\in\mathbb{R} \;\; | \;\; a< x< b\},
\]
\[
(a,b]=\{x\in\mathbb{R} \;\; | \;\; a< x\leq b\}, \;\;\;
[a,b)=\{x\in\mathbb{R} \;\; | \;\; a\leq  x< b\}.
\]
For unbounded intervals, we write
\[
[a,\infty)=\{x\in\mathbb{R} \;\; | \;\; a\leq x \}, \;\;\;
(a,\infty)=\{x\in\mathbb{R} \;\; | \;\; a< x \},
\]
\[
(-\infty,b]=\{x\in\mathbb{R} \;\; | \;\;  x\leq b\}, \;\;\;
(-\infty,b)=\{x\in\mathbb{R} \;\; | \;\; x< b\}.
\]

\begin{example}

\begin{enumerate}[label=(\alph*)]
\item Let $A=[0,3)$. $A$ is bounded above since $3$ is an upper
bound; $A$ is bounded below since $0$ is a lower bound.\\

\item Let $A = \{ t+\frac{1}{t}\, :\, t>0\}$. %\subseteq (0,\infty ).$
$A$ is
bounded below since $0$ is a lower bound. (We could have also shown it was bounded below by using the AM-GM inequality: for $t>0$,
\[
\frac{t+1/t}{2}\geq \sqrt{t\cdot \frac{1}{t}}=1,
\]
so $2$ is also a lower bound for $A$.) But $A$ is not bounded above, since, for any $M\in\mathbb{R}$ we can find $t>0$ such that $t+\frac{1}{t}\geq M$: if $M \leq 0$ then any $t>0$ works; if $M>0$ then $t= M$ works since $M+\frac{1}{M}\in A$ is greater than $M$.

\item Let $ A=\{ (-1)^n(1-\frac{1}{n})\, |\,
n\in\N\}=\{0,\frac{1}{2},-\frac{2}{3},\frac{3}{4},-\frac{4}{5},\frac{5}{6},\cdots
\}$. $A$ is bounded: 1 is an upper bound since for all $n$:
\[
(-1)^{n}\left(1-\frac{1}{n}\right)\leq 1-\frac{1}{n}<1
\]
and one can show similarly $-1$ is a lower bound.\\

\item Let $A=\{\sin n\, |\, n\in\N\}=\{ \sin 1,\, \sin 2,\, \sin 3,\,
\cdots \}.$ Because $-1 \leq \sin x \leq 1$ for all $x$, $A \subseteq
       [-1,1]$, so $A$ is bounded.
\end{enumerate}
\end{example}

\section{Least Upper Bounds}%
\label{sups}

Note that a nonempty set which is bounded above will have many upper bounds: if $M$ is an upper bound for a set $A$, so is every $t\geq M$. It is often useful to know what is the best or {\it least} upper bound for a set.

\begin{definition}
Given a nonempty set $A\subseteq \mathbb{R}$, a number $L$ is a {\it least upper bound} (LUB) or {\it supremum} for $A$ if
\begin{enumerate}[label=(\alph*)]
\item $L$ is an upper bound for $A$, that is, $x\leq L$ for all $x\in A$, and
\item for all $t<L$, $t$ is not an upper bound, that is, there is $x\in A$ so that $x>t$.
\end{enumerate}
Similarly, we say $L$ is a {\it greatest lower bound} (GLB) or {\it infimum} for $A$ if
\begin{enumerate}[label=(\alph*)]
\item $L$ is a lower bound for $A$, that is, $x\geq L$ for all $x\in A$, and
\item for all $t>L$, $t$ is not a lower bound, that is, there is $x\in A$ so that $x<t$.
\end{enumerate}
If an LUB exists for a set $A$, we denote it by $\LUB(A)$. Similarly, we denote a GLB for $A$ by $\GLB(A)$.
\end{definition}

In other words, $L$ is a least upper bound for a set $A$ if and only if it is the mimimum of all upper bounds for $A$. This allows us to speak of \emph{the} LUB of a set rather than \emph{an} LUB. When proving that a number is the LUB for some set, we follow a two-step process of verifying conditions (a) and (b) in the definition.

\begin{example}
Consider the set $[0,1)$. The least upper bound is $1$ since (a) $1$ is clearly an upper bound for the set by the definition of $[0,1)$ and (b) if $t<1$, then $t$ is not an upper bound, since we can find $x\in [0,1)$ bigger than $t$: if $t<0$, we can just take $x=0$; otherwise, if $0\leq t<1$, then $\frac{1+t}{2}\in [0,1)$ is larger than $t$.
\end{example}

\begin{example}
Let $S=\{x\in\mathbb{Q} \; | \; x<0\}$. We claim that $\LUB(S)=0$.

\begin{enumerate}[label=(\alph*)]
\item Since every $x\in S$ is negative by definition, $0$ is an upper bound.
\item Let $t<0$, we need to show $t$ is not an upper bound. Recall from Week 1 that every interval contains a rational, so in particular, the interval $(t,0)$ contains a rational $r$, and since $r<0$, we know $r\in S$. Since $r>t$, $t$ is not an upper bound for $S$.
\end{enumerate}
\end{example}

\begin{example}
Let $A=\{\frac{1}{n} \; | \; n\in\mathbb{N}\}$, we claim that $\GLB(A)=0$.

Again, we verify the two conditions of being a GLB:
\begin{enumerate}[label=(\alph*)]
\item Since $\frac{1}{n}>0$ for all $n\in\mathbb{N}$, $0$ is clearly a lower bound for $A$.
\item Next, we need to show each $t>0$ is not a lower bound, that is, that there is $\frac{1}{n} \in A$ with $\frac{1}{n}<t$. Let $t>0$. By the Archimedean property, we can find $n\in\mathbb{N}$ so that $n>\frac{1}{t}$, and so $\frac{1}{n}<t$. Since $\frac{1}{n}\in A$, this proves $t$ is not a lower bound.
\end{enumerate}
\end{example}

Given that a maximum or minimum of a set need not exist, why should the least upper bound (which is just the minimum of all upper bounds) exist at all? It turns out that this is an important axiom for the real numbers which we shall need to assume, and it cannot be derived from any of the axioms we have assumed so far:\\

\begin{tcolorbox}
{\noindent {\bf Completeness Axiom for the real numbers:} Every nonempty subset of $\mathbb{R}$ which is bounded above has a least upper bound. \\}
\end{tcolorbox}

\begin{exercise}
Why does the completeness axiom imply that every nonempty subset of $\mathbb{R}$ which is bounded below has a GLB?
\begin{solution}
See Exercise 5.3 (c) below.
\end{solution}
\end{exercise}
An important consequence of the Completeness Axiom is the existence of $n$'th roots of positive real numbers as described in Proposition 2.7. We will establish this as Theorem 6.5. Another very important consequence of the Completeness Axiom for the real numbers is the Archimedean property which we mentioned earlier. We will visit this implication later. For now you should feel free to use the Archimedean property in the exercises below.
%
%
%The Archimedean property is actually a consequence of the Completeness Axiom:
%
%\begin{theorem}
%The Archimedean property holds, that is, for any $x, y \in \mathbb{R}$ with $x,y>0$, there is $n\in\mathbb{N}$ so that $ny>x$.
%\end{theorem}
%
%\begin{proof}
%We will prove by contradiction: if $y>x$, we can just pick $n=1$. If $y\leq x$, consider the set $A=\{n\in\mathbb{N} \;\; | \; \; yn\leq x\}$. Since $ny\leq x$ for all $n\in A$, $n\leq \frac{x}{y}$ for all $n\in A$, so $A$ is bounded above, and the completeness axiom implies there is a least upper bound $L$. We claim $L\in A$. Indeed, since $L=\LUB(A)$, for any $t<L$, there is $n\in A$ with $t<n$

% Or: ETS $\mathbb{N}$ not bounded above. If it were, there would be a least upper bound $L$. Then there would be $N \in \mathbb{N}$ with $N > L-1/2$. But then $N+1$ > L + 1/2$.
%\end{proof}

\section{Exercises}%
\label{upperboundsexercises}

The exercises in Liebeck's book relevant to this section are in Chapter 22.

\begin{exercise}
Let $A,B\subseteq \mathbb{R}$ be nonempty sets that are bounded above. For each statement below, either prove it or provide a counterexample.
\begin{enumerate}[label=(\alph*)]
\item If $A$ is bounded above, then $\LUB(-A)=-\LUB(A)$, where $-A=\{-x \; | \; x\in A\}$.
\begin{solution}
This is not true: if $A=(-1,2)$, then $-A=(-2,1)$, so $-\LUB(A)=-2\neq \LUB(-A)=-1$. Another example is $A = (-\infty, 0)$ with $\LUB(A) = 0$, for which $-A$ is not even bounded above, and thus $-A$ has no LUB. Many more examples are possible.
\end{solution}
\item If $A$ is bounded above, then $-A$ is bounded below and $\GLB(-A)=-\LUB(A)$.
\begin{solution}
This is true: Let $L=\LUB(A)$, we claim $\GLB(-A)=-L$. First we show it is a lower bound: if $x\in -A$, then $-x\in A$, so $-x\leq L$, hence $x\geq -L$. Now we show it is the greatest lower bound: let $t>-L$, we will show there is $x\in -A$ with $x<t$. Note that $-t<L$, and since $L=\LUB(A)$, there is $y\in A$ with $y>-t$, so $-y<t$ and $-y\in -A$, thus $t$ is not a lower bound. This proves $\GLB(-A)=-L$.
\end{solution}
\item If $c\in \mathbb{R}$, then $\LUB(c+A)=c+\LUB(A)$ where $c+A=\{c+x \; | \; x\in A\}$.
\begin{solution}
This is true: let $L=\LUB(A)$, we claim $\LUB(c+A)=c+L$. First we show $c+L$ is an upper bound: if $x\in c+A$, then $x-c\in A$, so $x-c\leq L$, so $x\leq c+L$. Now we show that it is a least upper bound, i.e., that for $t<c+L$, there is $x\in c+A$ with $x>t$: Let $t<c+L$, then $t-c<L$, and since $L=\LUB(A)$, there is $y\in A$ with $y>t-c$, and so $y+c>t$ and $y+c\in A$, so $t$ is not an upper bound for $A$.
\end{solution}
\item $\LUB(A)=\max(A)$.
\begin{solution}
This is not true in general since $\max(A)$ may not be defined. A concrete conterexample is $A = (0,1)$ for which $\LUB(A) =1$, but which has no maximum. Many more examples are possible.
\end{solution}
\item If $c>0$, then $\LUB(cA)=c\LUB(A)$ where $cA=\{cx\; | \; x\in A\}$.
\begin{solution}
This is true, its proof is very similar to (c), so we omit it.
\end{solution}

\item $\LUB(AB)=\LUB(A)\cdot \LUB(B)$ where $AB=\{xy \;\; | \;\; x\in A,\; y\in B\}$.
\begin{solution}
Not true: let $A=(-2,1)$ and $B=(-2,1)$, then $AB=(-2,4)$ so $\LUB(AB)=4\neq \LUB(A)\cdot \LUB(B)=1$. Many more examples are possible.
\end{solution}
\item if $A\subseteq B$, then $\LUB(A)\leq \LUB(B)$.
\begin{solution}
This is true: note that $x\leq \LUB(B)$ for all $x\in B$, and so $x\leq \LUB(B)$ for all $x\in A$ since $A\subseteq B$. Hence, $\LUB(A)\leq \LUB(B)$.
\end{solution}
\end{enumerate}

\end{exercise}

\begin{exercise}
Suppose $A$ and $B$ are bounded above. Show that $\LUB(A+B)=\LUB(A)+\LUB(B)$, where $A+B=\{x+y\; | \; x\in A,\; y\in B\}$.
\begin{solution}
Let $L=\LUB(A)$ and $L'=\LUB(B)$, we'll show $\LUB(A+B)=L+L'$. First we show (a) that $L+L'$ is an upper bound for $A+B$: Note that if $z\in A+B$, then $z=x+y$ where $x\in A$ and $y\in B$, so
\[
z=x+y\leq L+L'
\]
and so $L+L'$ is an upper bound. Now we show (b) that if $t<L+L'$, then there is $z\in A+B$ with $z>t$. Recall that since $L=\LUB(A)$, for any $s<L$ there is $x\in A$ with $x>s$, and since $L'=\LUB(B)$, for any $s'<L'$ there is $y\in B$ with $y>s'$, and so $x+y>s+s'$ and $x+y\in A+B$. Hence, we just need to find values of $s<L$ and $s'<L'$ so that $s+s= t$, since then $x+y>s+s'= t$. Let's rewrite $s$ and $s'$ as $s=L-r$ and $s'=L-'r'$ where $r,r'>0$. Then we want to find $r,r'>0$ so that $t=s+s'=L+L'-r-r'$, i.e. so that $r+r'=L+L'-t$, so we can just choose $r=r'=\frac{L+L'-t}{2}$.
\end{solution}
\end{exercise}

\begin{exercise}
Determine which of the following sets are bounded or unbounded above.
\begin{enumerate}[label=(\alph*)]
\item $A=\{x \in \R\; | \; x^2-3x+2<2\}$.
\begin{solution}
If $x$ is in $A$, then
\[
x^2-3x=x(x-3)<0
\]
so exactly one of $x$ or $x-3$ is negative, which is only possible if $x-3< 0< x$. Thus for all $x \in A$, $0<x<3$, so $0$ and $3$ are lower and upper bounds for $A$ respectively. Therefore $A$ is bounded.
\end{solution}
\item $A= \{x \in \R \; | \; \cos x<1/2\}$. [You may assume any familiar properties of the cosine function.]
\begin{solution}
Note that $\cos \left(\frac{\pi}{2}+2\pi n\right)=0$ for all $n\in\mathbb{Z}$ and so $\frac{\pi}{2}+2\pi n$ is in $A$ for all $n$. In particular, $A$ is unbounded above and below: if $M\in\mathbb{R}$, by the Archimedean property, we can find $n$  so that $2\pi n>M-\frac{\pi}{2}$, i.e. so that $2\pi n+\frac{\pi}{2}>M$, hence $A$ is not bounded above. That it is not bounded below is similar.
\end{solution}
\item $A = \{n^2-n \; | \; n\in\mathbb{N}\}$.
\begin{solution}
If $n\in\mathbb{N}$, then $n\geq 1$ and so $n^2\geq n$, i.e. $n^2-n \geq 0$, hence $A$ is bounded below with $0$ being a lower bound. It is not bounded above. Indeed, let $M \in \R$. Note that for $n > 1$ we have $n^2-n=n(n-1)> n$, so by the Archimedean property, we may find $n>\max\{M,1\}$ such that $n^2-n=n(n-1)\geq n>M$. Thus $M$ cannot be an upper bound for $A$, and we deduce that $A$ is not bounded above.
\end{solution}
\item $A=\{ab \;\;| \;\;  a, b \in \R, \; a+b=1\}$.
\begin{solution}
Note that if $a+b=1$, then $b=1-a$, so that
\[
A = \{a(1-a)\;\; |\;\;a\in\mathbb{R}\} = \{- a^2 +a \;\; |\;\;a\in\mathbb{R}\}.
\]
Which real numbers $r$ are members of this set? It is precisely those $r \in \R$ such that the quadratic equation
\[
-a^2 + a =r
\]
has a real solution. To figure this out we complete the square:
\[
-a^2 + a =r \iff -\left(a-\frac{1}{2}\right)^2 = r -\frac{1}{4} \iff \left(a-\frac{1}{2}\right)^2 =\frac{1-4r}{4},
\]
and so long as $(1-4r)/4 \geq 0$ -- that is, $r \leq 1/4$ --  it has a real square root by Proposition 2.7. Thus $r \in A$ if and only if $r \leq 1/4$. Therefore
\[
A = (- \infty, 1/4]
\]
and this is bounded above (with $1/4$ as an upper bound) but not bounded below.

\end{solution}
\end{enumerate}

\end{exercise}

\begin{exercise}
Find the least upper bounds for the following sets:
\begin{enumerate}[label=(\alph*)]
\item $\{\frac{1}{n} \; | \; n\in \mathbb{N}\}$.
\begin{solution}
Note that for $n\geq 1$, $\frac{1}{n}\leq 1$, so $1$ is an upper bound. Since $1$ is in the set, it is in fact a maximum and thus equal to the least upper bound.
\end{solution}
\item $\{-\frac{1}{n^2} \; | \; n\in\mathbb{N}\}$.
\begin{solution}
We claim that the LUB is $0$. It is clearly an upper bound since all elements of the set are negative. Now we must show it is the least upper bound: let $t<0$, we'll show there is $-\frac{1}{n^2}>t$. This is the same as finding $n$ so that $n>(-t)^{-1/2}$, which is possible by the Archimedean property.
\end{solution}
\item $\{2-x^2 \; | \; x\in \R\setminus\{0\}\}$.
\begin{solution}
We claim the least upper bound is 2. Since $x^2\geq 0$, $2-x^2\leq 2$ for any $x\neq 0$, so $2$ is an upper bound. Now suppose $t<2$, we'll show there is $x$ so that $2-x^2>t$, i..e. so that $x^2<2-t$. Since $2-t>0$, we can just set $x=\sqrt{\frac{2-t}{2}}$ and this will work. This proves $2$ is the least upper bound.
\end{solution}
\item $\{x\in\mathbb{Q}: x^2<2\}$.
\begin{solution}
This is the same  as the set of $x\in\mathbb{Q}$ so that
\[
0>x^2-2=(x-\sqrt{2})(x+\sqrt{2})
\]
so exactly one of these terms must be negative, so we must have $-\sqrt{2}<x<\sqrt{2}$, that is,
\[
\{x\in\mathbb{Q}: x^2<2\}=(-\sqrt{2},\sqrt{2})
\]
We claim the least upper bound is $\sqrt{2}$: clearly $\sqrt{2}$ is an upper bound, so now we must show it is a least upper bound: let $t<\sqrt{2}$, we'll show there is $x$ in the set with $x>\sqrt{2}$. From Week 1, we saw that for any $a<b$, there is a rational $x$ with $a<x<b$, so in particular, there is $x$ with $t<x<\sqrt{2}$. This completes the proof that $\sqrt{2}$ is a LUB. The proof that $-\sqrt{2}$ is the GLB is similar.
\end{solution}
\end{enumerate}

\end{exercise}

\chapter{Limits}%
\label{limits}

\section{Motivation and the $\epsilon-N$ definition of a limit}%
\label{epsilondefinitionoflimit}

How does a computer compute $\sqrt{51}$? A computer can't possibly memorise all possible square roots, so it must compute it from scratch. Moreover, it can't possibly compute the entire decimal expansion (next week we will see that, because $\sqrt{51}$ is irrational, the decimal expansion never repeats), so it can only come up with an approximation. Thus, your computer requires an algorithm for computing $\sqrt{51}$ (or any root) to any degree of accuracy. But before implementing such an algorithm, we need to {\it justify} that it can indeed approximate $\sqrt{51}$ to any degree of accuracy. That is, if my algorithm spits out a sequence of numbers $x_1,x_2,...$, and I want an approximation that is within $10^{-10}$ of $\sqrt{51}$, I need to show that for all large enough $n$, $x_n$ is at most $10^{-10}$ away from $\sqrt{51}$, and I need this to happen for {\it every} degree of accuracy $\epsilon>0$ that I may require (not just $10^{-10}$). All this motivates the following:

\begin{definition}[ The $\epsilon-N$ definition of a limit]
Let $(x_{n})_{n=1}^{\infty}$ be a sequence of real numbers $x_{1},x_{2},...$ and let $L\in \mathbb{R}$. We say that $x_n$ {\it converges to $L$}, and we write $\lim_{n\rightarrow\infty} x_{n}=L$, or $x_{n}\rightarrow L$ as $n \rightarrow \infty$, if for all $\epsilon>0$, there is a real number $N$ so that for $n> N$, we have
\[
|x_{n}-L|<\epsilon
\]
or equivalently,\footnote{See Exercise~\ref{one}.}
\[
L-\epsilon<x_n<L+\epsilon.
\]
\end{definition}
\begin{definition}We say that a sequence $(a_n)$ is {\it convergent} if for some $L \in \R$, it converges to $L$.
\end{definition}
{\bf Exercise.} Give an example of a convergent sequence. Give an example of a non-convergent sequence.

Notice that here we are working with the notion of a {\bf sequence} $(x_n)_{n=1}^\infty$ -- this is just an {\em ordered} list of real numbers $x_n$, indexed by the natural numbers $\mathbb{N}$. Or, if you prefer, you can think of a sequence as a function $x:\mathbb{N}\to \mathbb{R}$ with domain $\mathbb{N}$ and codomain $\mathbb{R}$.

\medskip
Note that in Liebeck, the $N$ in Definition 6.1 is required to be an integer, but it actually makes no difference since we can just round $N$ up to an integer if we like, using the Archimedean principle. It is important to recognise that, in this definition, we expect $N$ to depend on $\epsilon$.

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

%The following notation will be convenient below: for a number $x$, we let $\floor{x}$ be $x$ rounded down, i.e. the largest integer at most $x$, so that
%\[
%\floor{x} \leq x<\floor{x}+1\;\;\; \mbox{ and }\;\;\; \floor{x}\in\mathbb{Z}.
%\]

\begin{example}
Show that $\lim_{n\rightarrow \infty} \frac{1}{n}=0$. \\

We need to show that for all $\epsilon>0$ there is $N$ so that $n> N$ implies $|\frac{1}{n}-0|<\epsilon$, which is the same as $\frac{1}{n}<\epsilon$. This is equivalent to finding $N$ so that $n>N$ implies $n>\frac{1}{\epsilon}$. Thus, if we pick $N=\frac{1}{\epsilon}$, we see that if $n> N$,
\[
\left|\frac{1}{n} -0\right|=\frac{1}{n} < \frac{1}{N}= \frac{1}{\frac{1}{\epsilon}}=\epsilon.
\]
\end{example}

\begin{protip}
{\bf Note on establishing limits:} Don't expect a nice looking proof to fall into your lap immediately when trying to establish a certain limit. It usually requires some rough work first, which you can then use to help you write your proof.
\end{protip}

\begin{example}
Show that $\lim_{n\rightarrow \infty} \frac{5n^2+2n}{n^2+4}=5$. \\
%
%{\bf Intuition:} For guessing the limit, note that in the numerator and denominator are polynomials in $n$ and the terms that grow fastest in $n$ are $5n^2$ and $n^2$ respectively. So as $n$ gets large, the other terms ($2n$ and $4$ respectively) will be insignificant in comparison to these terms. Thus, as $n$ gets larger, this should look approximately like $\frac{5n^2}{n^2}=5$. So my guess is that the limit is $5$.\\
%
%{\bf Note:} Intuition is not a proof! The above is just how we might go about hypothesizing an answer, but now we must justify our hypothesis. \\

{\bf Rough work:} We are going to need to show that $ \frac{5n^2+2n}{n^2+4}-5$ is small when $n$ is large, so it first makes sense to simplify this expression algebraically to see if we can identify when it might be small. Doing this,
\[\frac{5n^2+2n}{n^2+4}-5 =\frac{2n-20}{n^2+4},
\]
and therefore, using the triangle inequality and the fact that $4\geq0$,
\[\left|\frac{5n^2+2n}{n^2+4}-5\right|
\leq \left| \frac{2n-20}{n^2+4} \right|
\leq \frac{2n}{n^2 + 4} + \frac{20}{n^2 + 4}
\leq \frac{2}{n} + \frac{20}{n^2}.
\]
This is looking good because all we need to do to make this small is to make both $2/n$ and $20/n^2$ small, and this is very reasonable. Now we bring in the $\epsilon > 0$. If $n$ is such that both $2/n < \epsilon/2$ and $20/n^2 < \epsilon/2$, we can deduce that
\[\left|\frac{5n^2+2n}{n^2+4}-5\right| < \frac{\epsilon}{2}+ \frac{\epsilon}{2} = \epsilon,
\]
which is exactly what we need. What do we need $n$ to satisfy for this to be true? Well, for $\frac{2}{n} < \frac{\epsilon}{2}$ we need $n > \frac{4}{\epsilon}$, and for $\frac{20}{n^2} < \frac{\epsilon}{2}$ we need $n > \sqrt{\frac{40}{\epsilon}}$. To ensure both of these hold, we need $n$ to satisfy $n > \max\left\{\frac{4}{\epsilon}, \sqrt{\frac{40}{\epsilon}}\right\}.$ So we take $N = \max\left\{\frac{4}{\epsilon}, \sqrt{\frac{40}{\epsilon}}\right\}.$

(All the above was just rough work to help me formulate and write my formal proof. Below is what you want to do when writing your work for marking (and not the rough work). It's fine not to show how you chose your $N$, so long as it works. And there is no single answer for $N$ -- if $N$ works, then so does $N'$ for any $N'>N$.)\\

{\bf Now for the actual proof:} \\

{\bf Claim:} $\lim_{n\rightarrow\infty}\frac{5n^2+2n}{n^2+4}=5$.

\begin{proof}
Let $\epsilon$ be any positive real number. We need to show that there is $N\in\mathbb{R}$ so that $n> N$ implies $\left|\frac{5n^2+2n}{n^2+4}-5\right|<\epsilon$. We assert that $N=\max\left\{\frac{4}{\epsilon}, \sqrt{\frac{40}{\epsilon}}\right\} $ works. Indeed, if $n> N$, then
\[\left|\frac{5n^2+2n}{n^2+4}-5\right|
\leq \left| \frac{2n-20}{n^2+4} \right|
\leq \frac{2n}{n^2 + 4} + \frac{20}{n^2 + 4}
\leq \frac{2}{n} + \frac{20}{n^2} < \frac{2}{N} + \frac{20}{N^2} \leq \frac{\epsilon}{2}+ \frac{\epsilon}{2} = \epsilon.
\]
 Since we have shown we can find such an $N$ no matter what $\epsilon>0$ we started with, this proves the claim.

\end{proof}
\end{example}

\begin{example}\label{sqrootxn}
Suppose $x_{n}\geq 0$ and $x_{n}\rightarrow x>0$, then $\sqrt{x_{n}}\rightarrow \sqrt{x}$. \\

{\bf Discussion:} This is a little different from the previous example because now $x_n$ isn't given in a concrete form, and all we know about it is that $x_n \rightarrow x$, or, more informally, $|x_n - x|$ is small for all large $n$. Nevertheless the conclusion, $\sqrt{x_{n}}\rightarrow \sqrt{x}$, is similar to what we had before, and the tactic of manipulating the expression $\sqrt{x_{n}}-\sqrt{x}$ algebraically, and using some inequalities on it until it's apparent why it should also be small, is equally useful. Thus, we want to manipulate $|\sqrt{x_{n}}-\sqrt{x}|$ so that $|x_{n}-x|$ appears in the expression, since we hope that if we can make $|x_{n}-x|$ as small as we want, then we can make $|\sqrt{x_{n}}-\sqrt{x}|$ small as well.

A standard trick when working with roots is to multiply and divide by the conjugate, and we have
\[
|\sqrt{x_{n}}-\sqrt{x}|
=\left|(\sqrt{x_{n}}-\sqrt{x})\cdot \frac{\sqrt{x_{n}}+\sqrt{x}}{\sqrt{x_{n}}+\sqrt{x}}\right|
=\left|\frac{x_{n}-x}{\sqrt{x_{n}}+\sqrt{x}}\right|
=\frac{|x_{n}-x|}{\sqrt{x_{n}}+\sqrt{x}} \leq \frac{|x_{n}-x|}{\sqrt{x}}
\]
where in the last step we used the facts that $\sqrt{x_n} \geq 0$ and $x > 0$. This is looking good because we should be able to make
$\frac{|x_{n}-x|}{\sqrt{x}}$ small by taking $n$ large enough. Indeed, now let's bring in $\epsilon > 0$. Since $x_n \rightarrow x$, we know that there is some $N \in \mathbb{R}$ such that whenever $n > N$,
we have
\[ |x_n - x| <\epsilon\sqrt{x}.\]
(This maybe requires a pause for thought. The definition of $x_n \rightarrow x$ talks about ``$|x_n - x| < \epsilon$'' for all large $n$, it doesn't say anything about ``$ |x_n - x| <\epsilon\sqrt{x}$.'' But the definition of $x_n \rightarrow x$ applies to {\em every} positive $\epsilon$ -- and thus it applies just equally well to $\epsilon\sqrt{x}$
in place of $\epsilon$. Remember that $x$ is fixed in this discussion. The $N$ coming out depends on $x$ as well as $\epsilon$, but this is not a problem, as $x$ is fixed.)
So if $n > N$, we have
\[|x_n - x| \leq \frac{|x_n - x|}{\sqrt{x}} < \frac{\epsilon\sqrt{x}}{\sqrt{x}} = \epsilon
\]

\begin{proof}
We need to show that for any $\epsilon>0$ there is $N \in \R$ so that $n> N$ implies $|\sqrt{x_{n}}-\sqrt{x}|<\epsilon$.

Let $\epsilon>0$. Since $x_n\rightarrow x$, there is $N$ so that $n> N$ implies $|x_{n}-x|<\epsilon\sqrt{x}$, and we take this $N$. We then have, for $n > N$,
\[
|\sqrt{x_{n}}-\sqrt{x}|
=\left|(\sqrt{x_{n}}-\sqrt{x})\cdot \frac{\sqrt{x_{n}}+\sqrt{x}}{\sqrt{x_{n}}+\sqrt{x}}\right|
=\left|\frac{x_{n}-x}{\sqrt{x_{n}}+\sqrt{x}}\right|\]
\[=\frac{|x_{n}-x|}{\sqrt{x_{n}}+\sqrt{x}}
\leq \frac{|x_{n}-x|}{\sqrt{x}}
< \frac{\epsilon \sqrt{x}}{\sqrt{x}} = \epsilon.
\]
\end{proof}
\end{example}
{\bf Point to ponder:} What happens if $x = 0$ in this example?

\section{Rules for Limits}%
\label{rulesforlimits}
\begin{definition} We say that a sequence $(a_n)$ is {\it bounded} if the set of values $\{a_{1},a_{2},...\}$ is a bounded set, i.e. there are $m,M$ so that $m\leq a_n\leq M$ for all $n$.
\end{definition}

\begin{proposition}\label{cgtgivesbdd}
Suppose the sequence $(a_{n})$ converges. Then it is bounded.
\end{proposition}

\begin{proof}
Let $L=\lim_{n\rightarrow\infty}a_{n}$. By the definition of a limit (with $\epsilon=1$), there is $N$ so that $n> N$ implies $|a_{n}-L|<1$ (there was nothing special about our choice of $\epsilon=1$ here, any positive number will work in this argument). Thus, for $n> N$,
\[
|a_{n}|=|a_{n}-L+L|
\leq |a_{n}-L|+|L|
<1+|L|.
\]
For $n<N$, we have $|a_{n}|\leq \max\{|a_i| : |\; i=1,...,N-1\}$. Thus, for all $n$,
\[
|a_{n}|\leq \max\{1+|L|,|a_{1}|,...,|a_{n}|\}.
\]
By Exercise \ref{ex:bounded}, the sequence is bounded.
\end{proof}
\noindent
{\bf Point to ponder:} Does the converse to Proposition~\ref{cgtgivesbdd} hold?

\begin{proposition}
\label{p:limit-rules}
Suppose that $(a_{n})$ and $(b_{n})$ converge to $a$ and $b$ respectively. Then

\begin{enumerate}[label=(\alph*)]
\item $a_{n}+b_{n}\rightarrow a+b$
\item $ a_{n}b_{n} \rightarrow ab$
\item If $c\in\mathbb{R}$, then $ca_{n}\rightarrow ca$.
\item If $b\neq 0$ and $b_n\neq 0$ for all $n$, then $\frac{a_{n}}{b_{n}}\rightarrow \frac{a}{b}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item Let $\epsilon>0$. We need to show there is $N$ so that $n > N$ implies $|a_{n}+b_{n}-(a+b)|<\epsilon$. Note that by the triangle inequality
\[
|a_{n}+b_{n}-(a+b)|
=|(a_n-a)+(b_n-b)|
\leq |a_{n}-a|+|b_{n}-b|
\]
We can make $|a_{n}+b_{n}-(a+b)|<\epsilon$ by making $|a_{n}-a|$ and $|b_{n}-b|$ both less than $\frac{\epsilon}{2}$. Since $a_n\rightarrow a$, we know that there is $N_{1}$ so that $n > N_1$ implies $|a_{n}-a|<\frac{\epsilon}{2}$. Similarly, since $b_n\rightarrow b$, we know that there is $N_{2}$ so that $n > N_2$ implies $|b_{n}-b|<\frac{\epsilon}{2}$. If we set $N=\max\{N_1,N_2\}$, then for all $n > N$,
\[
|a_{n}+b_{n}-(a+b)|
\leq |a_{n}-a|+|b_{n}-b|
<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon.
\]
\item Let $\epsilon>0$.  Note that
\begin{align}
|a_{n}b_{n}-ab|
& =|a_{n}b_{n}-ab_{n}+ab_{n}-ab|
=|(a_{n}-a)b_{n} + a(b_n-b)| \notag \\
& \leq |(a_{n}-a)b_{n}| + |a(b_n-b)|
=|a_{n}-a|\cdot |b_{n}|+ |a|\cdot |b_{n}-b|.
\label{e:anbn-ab}
\end{align}
Thus, if we can pick $N$ large enough so that $n > N$ implies  $|a_{n}-a|\cdot |b_{n}|<\frac{\epsilon}{2}$ and $|a|\cdot |b_{n}-b|<\frac{\epsilon}{2}$, then the above will imply $|a_{n}b_{n}-ab|<\epsilon$ and we'll be done. So let's focus on proving these two things.

Let's first find $N_1$ so that $n > N_1$ implies $|a_{n}-a|\cdot |b_{n}|<\frac{\epsilon}{2}$. Since $b_n$ converges, it is bounded, and so there is $M$ so that $|b_{n}|\leq M$ for all $n$, thus
\[
|a_{n}-a|\cdot |b_{n}|\leq M|a_{n}-a|.
\]
Since $a_n\rightarrow a$, there is $N_1$ so that $n > N_1$ implies $|a_{n}-a|<\frac{\epsilon}{2M}$ and hence
\[
|a_{n}-a|\leq M|a_{n}-a|<M\cdot \frac{\epsilon}{2M}=\frac{\epsilon}{2}.
\]
Now let's find $N_2$ so that $n > N_2$ implies $|a|\cdot |b_{n}-b|<\frac{\epsilon}{2}$. If $a=0$, then this is always true and we can set $N_2=1$. If $a\neq 0$, then since $b_n\rightarrow b$, there is $N_2$ so that $n > N_2$ implies $|b_{n}-b|<\frac{\epsilon}{2|a|}$, and so that
\[
|a|\cdot |b_{n}-b|<|a|\cdot \frac{\epsilon}{2|a|}=\frac{\epsilon}{2}.
\]
Thus, if we set $N=\max\{N_{1},N_{2}\}$, then $n > N$ and  \eqref{e:anbn-ab} imply
\[
|a_{n}b_{n}-ab|
\leq |a_{n}-a|\cdot |b_{n}|+ |a|\cdot |b_{n}-b|
<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon.
\]
\item We leave this one as an exercise.
\item We claim that it suffices to show that if $b_n\rightarrow b$ and $b_n,b\neq 0$, then $b_{n}^{-1}\rightarrow b^{-1}$. Indeed, if this is the case, then  by part (b),
\[
\frac{a_{n}}{b_{n}}=a_{n}\cdot b_{n}^{-1} \rightarrow ab^{-1}=\frac{a}{b}.
\]
So now let's prove that if $b_n\rightarrow b$ and $b_n,b\neq 0$, then $b_{n}^{-1}\rightarrow b^{-1}$.  \\

Let $\epsilon>0$. We need to find $N$ so that $n > N$ implies $|b_{n}^{-1}-b^{-1}|<\epsilon$. Observe that
\[
\left|\frac{1}{b_{n}}-\frac{1}{b}\right|
=\left|\frac{b-b_{n}}{bb_{n}}\right|
=\frac{|b-b_{n}|}{|b|\cdot |b_{n}|}.
\]
Suppose we can show that for some number $s>0$ we have
\begin{equation}
\label{e:bn>m}
|b_{n}|\geq s \;\;\; \mbox{ for all }n\in\mathbb{N}.
\end{equation}
 Then since $b_{n}\rightarrow b$, we know that for any $\epsilon_1>0$ there is $N = N(\epsilon_1)$ so that $n > N$ implies $|b_{n}-b|<\epsilon_1$, and then we would have
\[
\left|\frac{1}{b_{n}}-\frac{1}{b}\right|\leq \frac{|b-b_{n}|}{|b|\cdot |b_{n}|}
\leq \frac{\epsilon_1}{|b| s}.
\]
Hence, if we pick $\epsilon_1=|b|s\epsilon$, then all this implies that for $n > N(|b|s\epsilon)$,
\[
\left|\frac{1}{b_{n}}-\frac{1}{b}\right|
\leq \frac{\epsilon_1}{|b| s}= \frac{|b|s\epsilon}{|b| s}=\epsilon.
\]
So now we focus on proving \eqref{e:bn>m}. Since $b_n\rightarrow b$, there is $N'$ so that $n > N'$ implies $|b_{n}-b|<\frac{|b|}{2}$. Thus, by the reverse triangle inequality,
\[
|b_{n}|
=|b_{n}-b+b|
\geq |b|-|b_{n}-b|
>|b|-\frac{|b|}{2} = \frac{|b|}{2}.
\]
For $n \leq N'$, we simply have $|b_{n}|\geq \min\{|b_{1}|,\dots,|b_{N'}|\}>0$. Thus, for all $n$,
 \[
|b_{n}|\geq \min\left\{\frac{|b|}{2},|b_{1}|,\dots ,|b_{N'}|\right\}.
\]
This finishes the proof of  \eqref{e:bn>m}, and thus of (d).

\end{enumerate}
\end{proof}

From these rules, we can get many more useful results:

\begin{proposition}
\label{p:power-lim}
Suppose that $x_n\rightarrow L$ as $n \to \infty$ and that $k \in \mathbb{N}$. Then $x_{n}^{k}\rightarrow L^{k}$ as $n \to \infty$.
\end{proposition}

\begin{proof}
We prove this by induction on $k$. The base case $k=1$ is a tautology. For the induction step, assume we have shown that whenever $x_n\rightarrow L$, then for some particular $k \geq 1$, $x_{n}^{k}\rightarrow L^{k}$. Now apply Proposition \ref{p:limit-rules}(b) with $a_n=x_{n}^{k}$ and $b_{n}=x_{n}$, to obtain
\[
\limn x_{n}^{k+1}=\limn x_{n}^{k}x_{n}
=\left(\limn x_{n}^{k}\right)\cdot\left(\limn x_{n}\right)
=L^{k}\cdot L = L^{k+1}
\] Thus, the proposition follows by induction.
\end{proof}

\begin{proposition}
\label{p:x_n<y_n}
Let $(x_{n})$ and $(y_{n})$ be sequences.
\begin{enumerate}[label=(\alph*)]
\item  If $x_{n}\rightarrow x$, $y_{n}\rightarrow y$, and $x_{n}\leq y_{n}$ for all $n\in\mathbb{N}$, then $x\leq y$.
\item If $x_n\rightarrow x$ and $x_{n}\leq y$ for all $n$, then $x\leq y$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item  Suppose for a contradiction that $x>y$. Take $\epsilon = (x-y)/2 > 0$. Since $x_{n}\rightarrow x$, there is $N_{1}$ so that $n > N_{1}$ implies
\[
x-\frac{x-y}{2}<x_{n}<x+\frac{x-y}{2}
\]
and in particular
\[
\frac{x+y}{2}<x_{n}.
\]
Similarly, since $y_{n}\rightarrow y$, there is $N_{2}$ so that $n > N_{2}$ implies
\[
y-\frac{x-y}{2}<y_{n}<y+\frac{x-y}{2}
\]
and in particular
\[
y_{n}<\frac{x+y}{2}.
\]
Therefore, for all $n>  \max\{N_{1},N_{2}\}$,
\[
\frac{x+y}{2}<x_{n} \leq y_n < \frac{x+y}{2}.
\]
which gives the desired contradiction.

\item This follows by (a) by letting $y_{n}=y$ for all $n$.
\end{enumerate}
\end{proof}

There is another important rule for limits, often called the ``Squeeze Theorem'':

\begin{proposition}\label{squeeze}
Suppose that $x_n \leq y_n \leq z_n$ for all $n$, and that $x_n \to L$ and $z_n \to L$. Then $y_n \to L$.
\end{proposition}

\begin{proof}
Let $\epsilon > 0$. There exist $N_1$ and $N_2$ such that for $n > N_1$ we have
\[ |x_n - L| < \epsilon\]
and for $n > N_2$ we have
\[ |z_n - L| < \epsilon.\]
Take $N = \max\{N_1, N_2\}$. For $n > N$ we have
\[ - \epsilon < x_n - L \leq y_n - L \leq z_n - L < \epsilon.\]
Therefore for $n > N$ we have $|y_n - L| < \epsilon$.
\end{proof}

\section{Application: the existence of roots.}%
\label{existenceofroots}

Recall that in Week 1 we asserted the existence of roots; now we can now prove it.

\begin{theorem}
\label{t:proof-roots-exist}
Let $x>0$ and $k\in\mathbb{N}$. Then there is a unique $y>0$ so that $y^{k}=x$.
\end{theorem}

\begin{proof}
Let $S=\{s \geq 0 \; | \; s^{k}<x\}$. Then $S$ is nonempty (since $0 \in S$) and bounded above since $\max \{x, 1\}$ is an upper bound for $S$. Therefore
\[
y=\LUB(S)= \LUB(\{s \geq0 \; | \; s^{k}<x\})
\]
exists by the Completeness Axiom. We claim that $y^{k}=x$.

Since $y$ is the least upper bound of $S$, this means that for all $n\in\mathbb{N}$, $y-\frac{1}{n}$ is not an upper bound, i.e. there is $y_{n}\in S$ so that
\[
y-\frac{1}{n}<y_{n}\leq y.
\]
This implies $y_n\rightarrow y$: indeed, for any $\epsilon>0$, if  $N=\epsilon^{-1}$, then for $n> N$, $\frac{1}{n}<\frac{1}{N}=\epsilon$ and so
\[
y-\epsilon < y-\frac{1}{n}<y_{n}\leq y<y+\epsilon,
\]
Thus, we can find for each $\epsilon$ an $N$ so that $n> N$ implies $y-\epsilon<y_n<y+\epsilon$, hence $y_n\rightarrow y$.

Since $y_n\in S$, $y_{n}^{k}<x$ for all $n$, so Proposition \ref{p:power-lim} and Proposition \ref{p:x_n<y_n} imply
\[
y^{k}=\limn y_{n}^{k}\leq x.
\]
Now we will show the reverse inequality, i.e. that $x\leq y^{k}$, and this will imply $y^k=x$. Since $y=\LUB(S)$, $y$ is an upper bound for $S$, and so for all $n$, $y+\frac{1}{n}\not\in S$. By the definition of $S$, this means
\[
\left(y+\frac{1}{n}\right)^{k}\geq x.
\]
Again, Proposition \ref{p:power-lim} and Proposition \ref{p:x_n<y_n}, together with the fact that $y + \frac{1}{n}\rightarrow y$ imply that
\[
y^{k} = \limn\left(y+\frac{1}{n}\right)^{k}\geq x.
\]
Thus, $y^k=x$. Finally, if $0 < y_1 < y_2$, then $y_1^k < y_2^k$ (see Proposition 4.3) and so there is a {\em unique} $y >0$ such that $y^k = x$.

\end{proof}

\section{Infinite limits}%
\label{infinitelimits}

\begin{definition}
Let $(x_{n})$ be a sequence of real numbers.
\begin{enumerate}[label=(\alph*)]
\item We say {\it $(x_{n})$ tends to $\infty$} or {\it diverges to} $\infty$ or $x_{n}\rightarrow \infty$ as $n \to \infty$ if, for all $M>0$, there is $N$ so that $n> N$ implies $x_{n}\geq M$.
\item Similarly, we say {\it $x_{n}$ tends or diverges to $-\infty$} or $x_{n}\rightarrow -\infty$ as $n \to \infty$ if, for all $M<0$, there is $N$ so that $n>N$ implies $x_{n}\leq M$.
\end{enumerate}
\end{definition}

\begin{example}
$\sqrt{n}\rightarrow\infty$: let $M>0$, we need to find $N$ so that $n> N$ implies $\sqrt{n}> M$. If we pick $N=M^2$, then for $n> N$,
\[
\sqrt{n}> \sqrt{N}=\sqrt{M^2}=M.
\]
Thus, we have shown that for any $M>0$ we can find $N$ so that $n> N$ implies $\sqrt{n}\geq M$, hence $\sqrt{n}\rightarrow\infty$.
\end{example}

\begin{example}
The sequence $x_{n} = n^2-n-1$ tends to infinity. To prove this, we must show that there is $N$ so that $n> N$ implies
\[
n^2-n-1>M,
\]
%We could use the quadratic formula to solve for such an $N$, but here is another way that could be used for other problems: it involves choosing $N$ large enough so that you can bound your sequence below by something simpler that you can then show is bigger than $M$.

Note that  for $n>1$,
\[
n^2-n -1= n(n-1)-1\geq n\cdot 1-1=n-1
\]
Thus, if $N=M+1$, then $n> N$ implies
\[
n^2-n-1 \geq n-1> N-1=M+1-1=M.
\]
\end{example}

%{\bf Note:} The symbol $\infty$ is {\it not} a number, so in particular, the limit rules we saw earlier do not hold for limits tending to infinity. If you ever catch yourself writing something like $\infty\cdot 3$, $\infty-2$, or $\infty^2$, you are writing something invalid, even if you may be getting the correct answer.

%
%\begin{protip}
%{\bf Simplifying sequences tending to infinity:} In the above exercise, we showed that $n^{2}-n+1$ tended to infinity by showing it was at least $\frac{n^{2}}{2}$ for $n$ large enough, and then it was easier to show that we could make $\frac{n^{2}}{2}$ as large as we wanted. In general, if you have a sequence you think tends to infinity of the form $a_n+b_n$ and you think that $a_n$ goes to infinity faster than $b_n$, try and first prove that there is $N_0$ so that $n\geq N_{0}$ implies $|b_{n}|<\frac{a_{n}}{2}$, because then
%\[
%a_{n}+b_{n}\geq a_{n}-|b_{n}|>a_{n}-\frac{a_{n}}{2}=\frac{a_{n}}{2}
%\]
%and then try and show that there is $N\geq N_{0}$ so that $n\geq N$ implies $\frac{a_{n}}{2}\geq M$.
%\end{protip}

\section{Exercises}%
\label{limitsexercises}

The exercises in Liebeck's book relevant to this section are in Chapter 23.

\begin{exercise}\label{one}
Show that $|x- L| < \epsilon$ if and only if $L - \epsilon < x < L+ \epsilon$.
\begin{solution}
 Suppose that $|x-L|<\epsilon$. Then $
x-L\leq |x-L|<\epsilon, \mbox{ and so } x < L + \epsilon;
$
moreover
$
L-x \leq |L-x|=|x-L|<\epsilon
\mbox{ and so } x>L-\epsilon.
$
Conversely, suppose that $L-\epsilon<x<L+\epsilon$. Then
$ - \epsilon < x -L < \epsilon$
and so $
|x-L|<\epsilon.$
 \end{solution}
 \end{exercise}

 \begin{exercise}
 Using the $\epsilon-N$ definition of a limit, prove the following:

\begin{enumerate}[label=(\alph*)]
\item $\limn \frac{1}{n^{3}}=0$.
\begin{solution}
Let $\epsilon>0$. If $N=\epsilon^{-1/3}$, then for $n> N$,
\[
\left|\frac{1}{n^3}-0\right|=\frac{1}{n^3}< \frac{1}{N^3}=\frac{1}{ (\epsilon^{-1/3})^3}=\frac{1}{\epsilon^{-1}}=\epsilon.
\]
\end{solution}
\item $\limn \frac{n^{4}}{n^{4}+1}=1$
\begin{solution}
Let $\epsilon>0$. We want to find $N$ so that $n> N$ implies
\[
\left|\frac{n^{4}}{n^{4}+1}-1\right|=\frac{1}{n^4+1}<\epsilon.
\]
If $\epsilon\geq 1$, then this always holds so we can just pick $N=1$. If $\epsilon<1$, this is equivalent to saying $n^4+1>\epsilon^{-1}$. This will certainly hold if $n^4>\epsilon^{-1}$, i.e. when $n>\epsilon^{-\frac{1}{4}}$. Thus, the above inequality holds if $n>N$ where $N=\epsilon^{-\frac{1}{4}}$.
\end{solution}
\item $\limn \frac{n^{2}+3}{n^{2}+n}=1$
\begin{solution}
Let $\epsilon>0$, we want to find $N$ so that $n> N$ implies
\[
\left|\frac{n^{2}+3}{n^{2}+n}-1\right|=\left|\frac{3-n}{n^2+n}\right|<\epsilon.
\]
Note that for $n> 3$, $n-3> 0$ and so
\[
\left|\frac{3-n}{n^2+n}\right|=\frac{n-3}{n^2+n}<\frac{n}{n^2+n}=\frac{1}{n+1}.
\]
Thus, if we pick $N=\max\{3,\frac{1}{\epsilon}\}$, we see that $n> N$ implies
\[
\left|\frac{n^{2}+3}{n^{2}+n}-1\right|=\frac{n-3}{n^2+n}<\frac{1}{n+1}< \frac{1}{N+1}\leq \frac{1}{\frac{1}{\epsilon}+1}<\frac{1}{1/\epsilon}=\epsilon.
\]
\end{solution}
\item $\limn \frac{2n^{2}-n}{n^{2}+n-1}=2$.

\begin{solution}
Let $\epsilon>0$, we want to find $N$ so that $n> N$ implies
\[
\left|\frac{2n^{2}-n}{n^{2}+n-1}-2\right|
=\left|\frac{-3n+2}{n^2+n-1}\right|<\epsilon.
\]
Since $n\geq 1$, $3n-2\geq 0$, so
\[
\left|\frac{-3n+2}{n^2+n-1}\right|=\frac{3n-2}{n^2+n-1}
<\frac{3n}{n^2-1}=\frac{3n}{(n-1)(n+1)}<\frac{3n}{(n-1)n}=\frac{3}{n-1}.
\]
We want $N$ so that $n> N$ implies $\frac{3}{n-1}<\epsilon$, i.e. that $n>3\epsilon^{-1}+1$. Thus, if we pick $N=3\epsilon^{-1}+1$, then $n> N$ implies
\[
\left|\frac{2n^{2}-n}{n^{2}+n-1}-2\right|=\frac{3n-2}{n^2+n-1}<\frac{3}{n-1}< \frac{3}{N-1} =\frac{3}{3\epsilon^{-1}+1-1}=\epsilon.
\]

\end{solution}

\end{enumerate}

 \end{exercise}

 \begin{exercise}
 For the following sequences, show that they tend to infinity.

\begin{enumerate}[label=(\alph*)]
\item $5n-2$
\begin{solution}
Let $M>0$. Let $N>\frac{M+2}{5}$, then for $n> N$,
\[
5n-2> 5N-2\geq 5\frac{M+2}{5}-2=M.
\]
\end{solution}
\item $n^{3}+1$
\begin{solution}
Note that $n^3+1>n^3$, so if $M>0$, we can pick $N=M^{\frac{1}{3}}$ and then $n> N$ will imply
\[
n^3+1>n^3> N^3=M.
\]
\end{solution}
\item $n^{4}-n+1$
\begin{solution}
Note that for $n>1$,
\[
n^4-n+1=n(n^3-1)+1\geq n+1
\]
and so if $N=M-1$, then for $n>N-1$, we have $n^4-n+1\geq n+1\geq N+1\geq M$.
\end{solution}
\item $n!$
\begin{solution}
Let $M>0$. If we set $N=M$, then $n> N$ implies
\[
n!\geq n> N=M.
\]
\end{solution}
\item $\sqrt{n}$
\begin{solution}
Let $M>0$ and $N=M^2$, then $n> N$ implies
\[
\sqrt{n}> \sqrt{N}=\sqrt{M^2}=M.
\]
\end{solution}
\end{enumerate}
\end{exercise}

\begin{exercise}
Show that the limit of a sequence is unique, that is, if $x_{n}\rightarrow L$ and $x_{n}\rightarrow M$, then $L=M$.
\begin{solution}
Suppose $L\neq M$. Let $\epsilon=\frac{|L-M|}{2}$. Since $x_{n}\rightarrow L$, there is $N_1$ so that $n> N_1$ implies $|x_{n}-L|<\epsilon$. Similarly, $x_{n}\rightarrow M$, there is $N_2$ so that $n> N_2$ implies $|x_{n}-L|<\epsilon$. Thus, for $n>N= \max\{N_1,N_2\}$,
\[
|L-M|=|L-x_{n}+x_{n}-M|\leq |L-x_{n}|+|x_{n}-M|<\frac{\epsilon}{2}+\frac{\epsilon}{2} = \epsilon = |L-M|
\]
which is a contradiction.
\end{solution}
\end{exercise}

 \begin{exercise}
For each $L$ below, find sequences $(x_{n})$ and $(y_{n})$ so that $x_{n}\rightarrow 0$, $y_{n}\rightarrow 0$ and $x_{n}\neq 0, y_n \neq 0$ for all $n$, and such that $\frac{x_{n}}{y_{n}}\rightarrow L$.
\begin{enumerate}[label=(\alph*)]
\item $1$
\item $0$
\item $\infty$
\end{enumerate}
\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item $x_n=y_n=\frac{1}{n}$.
\item $x_n=\frac{1}{n^2}, y_n=\frac{1}{n}$.
\item $x_n=\frac{1}{n}, y_n=\frac{1}{n^2}$.
\end{enumerate}
(Many other solutions are possible.)
\end{solution}
\end{exercise}

 \begin{exercise}
For each $L$ below, find sequences $(x_{n})$ and $(y_{n})$ so that $x_{n}\rightarrow 0$, $y_{n}\rightarrow \infty$ and $x_{n}y_{n}\rightarrow L$.
\begin{enumerate}[label=(\alph*)]
\item $1$
\item $0$
\item $\infty$
\end{enumerate}
\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item $x_n=y_n=n$.
\item $x_n=n, y_n=n^2$.
\item $x_n=n^2, y_n=n$.
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Show that if $x_n\rightarrow x$, then $|x_n|\rightarrow |x|$.

\begin{solution}
Let $\epsilon>0$. Since $x_n\rightarrow x$, there is $N$ so that $n> N$ implies $|x_{n}-x|<\epsilon$. By the reverse triangle inequality, for $n> N$,
\[
||x_{n}|-|x||
\leq |x_{n}-x|<\epsilon.
\]
Thus, $|x_n|\rightarrow |x|$.
\end{solution}

\end{exercise}

\begin{exercise}\label{68}
Suppose that $(x_{n})$ converges. Show that  for any $k\in\mathbb{N}$, $\limn x_{n}= \limn x_{n+k}$.

\begin{solution}
Let $L=\limn x_{n}$. We want to show $\limn x_{n+k}=L$, that is, for all $\epsilon>0$ there is $N$ so that $n> N$ implies $|x_{n+k}-L|<\epsilon$. Let $\epsilon>0$. Since $L=\limn x_{n}$, we know there is $N$ so that $n> N$ implies $|x_{n}-L|<\epsilon$. If $n> N$, then $n+k>N$, and so we also have $|x_{n+k}-L|<\epsilon$ for all $n> N$. This proves the claim.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose that $(x_{n})$ converges. Show that $x_{n}-x_{n+1}\rightarrow 0$.

\begin{solution}
Using the limit rules and the previous exercise, if $\limn =L$, then
\[
\limn(x_{n}-x_{n+1})
=\limn x_n - \limn x_{n+1} = L-L=0.
\]
\end{solution}
\end{exercise}
%
%\begin{exercise}
%Suppose $x_n\in \mathbb{N}$ for all $n$ and $x_n$ converges. Show that there is an integer $x,N\in\mathbb{N}$ so that $x_n=x$  for all $n\geq N$.
%
%
%\begin{solution}
%Let $x=\limn x_{n}$. Using the $\epsilon-N$ definition of a limit with $\epsilon=\frac{1}{2}$, we know that there is $N$ so that $n\geq N$ implies
%\[
%|x_{n}-x|<\frac{1}{2}.
%\]
%We claim that $x_{n}=x_{N}$ for all $n\geq N$. If there was $n\geq N$ so that $x_{n}\neq x_{N}$, then since $x_{n},x_{N}\in \mathbb{N}$, $|x_{n}-x_{N}|\geq 1$, but then
%\[
%1\leq |x_{n}-x_{N}| |x_{n}-x+x-x_{N}|\leq |x_{n}-x|+|x-x_{N}|
%< \frac{1}{2}+\frac{1}{2}=1,
%\]
%which is a contradiction.
%\end{solution}
%\end{exercise}

\begin{exercise} Using the $\epsilon-N$ definition of a limit, prove that if $x_n>0$ and $x_n\rightarrow x>0$ and $x_n\neq x$ for all $n$, then $x_{n}^{\frac{1}{3}}\rightarrow x^{\frac{1}{3}}$.

\begin{solution}
Note that
\[
|x^{\frac{1}{3}}-x_{n}^{\frac{1}{3}}|
=\left| \frac{ (x^{\frac{1}{3}}-x_{n}^{\frac{1}{3}})(x^{\frac{2}{3}}+x^{\frac{1}{3}}x_{n}^{\frac{1}{3}}+x_{n}^{\frac{2}{3}})}{x^{\frac{2}{3}}+x^{\frac{1}{3}}x_{n}^{\frac{1}{3}}+x_{n}^{\frac{2}{3}}} \right|
 = \frac{|x-x_{n}|}{x^{\frac{2}{3}}+x^{\frac{1}{3}}x_{n}^{\frac{1}{3}}+x_{n}^{\frac{2}{3}}}
 \leq \frac{|x-x_{n}|}{x^{\frac{2}{3}}}.\\
\]
Let $\epsilon>0$. Pick $N$ so that $n> N$ implies $|x-x_{n}|< \epsilon x^{\frac{2}{3}}$. Then the above implies that for $n> N$,
\[
|x^{\frac{1}{3}}-x_{n}^{\frac{1}{3}}|
\leq  \frac{|x-x_{n}|}{x^{\frac{2}{3}}}
<\frac{\epsilon x^{\frac{2}{3}}}{x^{\frac{2}{3}}}=\epsilon
\]
which thus proves $x_{n}^{\frac{1}{3}}\rightarrow x^{\frac{1}{3}}$.
\end{solution}

\end{exercise}

\begin{exercise}
Let $a_n = \left( 1- \frac{1}{n^2}\right)^n$. Show that $a_n \to 1$ as $n \to \infty$. What happens if we consider $\left(1-\frac{1}{n^s}\right)^n$ for $s > 1$? ({\bf Hint:} Bernoulli's inequality.)
\begin{solution}
By Bernoulli's inequality with $ h= -1/n^2$ we have
\[1 \geq \left( 1- \frac{1}{n^2}\right)^n \geq 1 - \frac{n}{n^2}
= 1 - \frac{1}{n}.\]
By the squeeze theorem we deduce that
\[ \left( 1- \frac{1}{n^2}\right)^n \to 1\]
as $n \to \infty$. A similar  argument works for $\left(1-\frac{1}{n^s}\right)^n$
whenever $s > 1$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $a_n = \left( 1+ \frac{1}{n^2}\right)^n$. Show that $a_n \to 1$ as $n \to \infty$.
\end{exercise}
\begin{solution}
We have
\[ \left( 1+ \frac{1}{n^2}\right)^n\left( 1- \frac{1}{n^2}\right)^n=\left( 1- \frac{1}{n^4}\right)^n.\]
Let $b_n = \left( 1- \frac{1}{n^2}\right)^n$ and $c_n = \left( 1- \frac{1}{n^4}\right)^n$ so that
$a_n b_n = c_n$. But by the previous exercise, $b_n \to 1$ and $c_n \to 1$. Therefore by the rules for limits, $a_n \to 1$ too.

\end{solution}

\begin{exercise}
Let $a_n = \left( 1+ \frac{1}{n}\right)^n$. Given that $a_n \to L$ as $n \to \infty$, what can you say about
the sequence $(b_n)$ where
$b_n = \left( 1- \frac{1}{n}\right)^n$?
\begin{solution}
By the argument for the previous exercise, $b_n \to 1/L$ (since $L \geq 1$).
\end{solution}
\end{exercise}

%
%
%\begin{exercise}
% Show that $|x_{n}+y_{n}|-|x_{n}-y_{n}|\rightarrow \infty$ if and only if $\lim_{n\rightarrow\infty} |x_{n}|=\lim_{n\rightarrow\infty} |y_{n}|=\lim_{n\rightarrow\infty} x_{n}y_{n} = \infty$.
% \end{exercise}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 4: More Analysis}

\chapterimage{Figures/blank.png}

\chapter{The Monotone Convergence Theorem}%
\label{monotoneconvergence}

Last week we studied how to formally show that a sequence $(x_{n})$ converges to a specific limit $L$, and we developed various rules for manipulating limits. What if we are given a sequence $(x_{n})$ and want to show it converges but we {\it don't} know what the limit is?

This week we will talk about an important convergence theorem which gives us a sufficient condition for a sequence to  converge. This is the monotone convergence theorem, which says that any increasing sequence which is bounded above must converge to something. %The next theorem is the Bolzano–Weierstrass Theorem, which doesn't say that a sequence converges, but says that whenever we have a bounded sequence, we can throw out some of the terms so that it does converge.
We'll then give some examples illustrating how this result can be useful. Then we will see how this result can shed light on something we are already familiar with -- decimal expansions -- and lastly we'll introduce the notion of infinite series.

%Before we begin, let's recall two facts which can be proved using induction, the binomial theorem and the formula for the sum of a geometric series. The binomial theorem states that for $a, b \in \mathbb{R}$ and $n \in \mathbb{N}$ we have
%\[ \left( a + b\right)^n = \sum_{j=0}^n {n \choose j} a^{n-j} b^{j} = a^n + {n \choose 1}a^{n-1}b + \cdots + {n \choose j} a^{n-j}b^j + \cdots + {n \choose n-1 } a b^{n-1} + b^n,\] where the binomial coefficient is defined by
%\[ {n \choose j} = \frac{n!}{j! (n-j)!}.\]
%The formula for the sum of a geometric series is
%\[ a + ar + ar^2 + \cdots + ar^{n-1} = \frac{a(1-r^{n})}{1-r}\]
%whenever $r \neq 1$. A consequence of this latter result is the useful formula
%\[ a^n - b^n = (a-b)(a^{n-1} + a^{n-2}b + \cdots + ab^{n-2} + b^{n-1})\]
%valid for $ a, b \in \mathbb{R}$ and $n \in \mathbb{N}$. These will all be helpful to us in our study of various examples.

\section{The Monotone Convergence Theorem}%
\label{monotoneconvergencesection}

\def\LUB{{\rm LUB}}
\def\ve{\epsilon}
\def\limn{\lim_{n\rightarrow\infty}}

\begin{definition}
A sequence $(a_{n})_{n=1}^{\infty}$ is {\it increasing} if $a_{n+1}\geq a_{n}$ for all $n$. It is {\it decreasing} if $a_{n+1}\leq a_{n}$ for all $n$. We say $(a_{n})$ is {\it monotonic} if it is either increasing or decreasing.
\end{definition}

\noindent
{\bf Exercise:} Give an example of an increasing sequence. Give an example of a sequence which is not monotonic.
Give an example of a sequence which is both increasing and decreasing.

\medskip
Monotonic sequences are especially important for us because we have:

\begin{theorem}[Monotone Convergence Theorem (MCT)]
Let $(a_{n})$ be an increasing sequence of real numbers that is bounded above (i.e. there is $M$ so that $a_{n}\leq M$ for all $n$). Then $(a_{n})$ converges to some limit. If $(a_{n})$ is decreasing sequence and is bounded below, then $(a_{n})$ converges to some limit.
\end{theorem}

\begin{proof}
We begin with the first part. Assume that $(a_n)$ is increasing and bounded above. Notice that we are not told what its limit is going to be. This has to come out of our argument somehow, and the first job is to find a candidate for it.

Since $(a_{n})$ is bounded above, this means that the set\footnote{Note the distinction between the sequence $(a_n)$, in which the order of members is important, and the set $\{ a_n\}$, in which it isn't important or even recorded.} $\{a_{n}\; | \; n\in\mathbb{N}\}$ is bounded above. Thus, by the Completeness Axiom, the set has an LUB, call it $L$. We claim that
\[
\lim_{n\rightarrow\infty}a_{n}=L.
\]
We begin the proof of the claim. Let $\ve>0$, we wish to show there is $N\in\mathbb{N}$ so that
\[
|a_{n}-L|<\ve
\]
for all $n> N$, or equivalently,
\begin{equation}
\label{e:L-e<a<L+e}
L-\ve<a_{n}<L+\ve.
\end{equation}
Since $L$ is an upper bound for $\{a_{n}\; | \; n\in\mathbb{N}\}$, we know $a_{n}\leq L<L+\ve$ for all $n$, thus dealing with the second inequality in \eqref{e:L-e<a<L+e}. Therefore, it suffices to show that there is $N\in\mathbb{N}$ so that for all $n> N$,
\[
L-\ve<a_{n}.
\]
Since $L$ is the LUB for $\{a_{n}\; | \; n\in\mathbb{N}\}$, this means that $L-\ve$ is not an upper bound for this set, so there exists an integer $N$ so that $a_{N}>L-\ve$. Since the sequence is increasing, this means that for all $n> N$,
\[
a_{n} \geq a_N > L - \epsilon.
\]
This shows \eqref{e:L-e<a<L+e} holds for all $n>N$ and completes the proof.

\medskip
We leave the proof of the second part to the reader. (Why is it an easy consequence of the first part?)
\end{proof}
\noindent
{\bf Question:} What if a sequence $(a_n)$ is bounded above but only satisfies $a_{n+1} \geq a_{n}$ for all $n > 100$. Can you still conclude that it converges?

\medskip
Thus, if you can show that a sequence is bounded and increasing, you can deduce that it has a limit, and in some cases this, together with the rules for limits that we discussed last week, allows you to figure out what the limit actually is.
%
%One nice thing about monotone sequences is that you can deduce their limit from a subsequence:
%%
%\begin{proposition}
%\label{p:sub-monotone}
%Let $(x_{n})$ be a monotone sequence and suppose there is a sequence of integers $(n_{k})$ so that $\lim_{k\rightarrow\infty} n_{k}=\infty$ and $\lim_{k\rightarrow\infty} x_{n_{k}}$ exists. Then $\limn x_{n}$ exists and has the same limit.
%\end{proposition}
%
%\begin{proof}
%Let $L=\lim_{k\rightarrow\infty} x_{n_{k}}$. We claim $\limn x_{n}=L$. Let $\ve>0$. We need to show that there is $N$ so that if $n\geq N$, then $|x_{n}-L|<\ve$. Again, since $x_n$ is monotone, it suffices to show that there is $N$ so that $n\geq N$ implies
%\begin{equation}
%L-\ve <x_{n}.
%\end{equation}
%Since $x_{n_{k}}\rightarrow L$, there is $K$ so that $k\geq K$ implies
%\[
%L-\ve <x_{n_{k}}<L+\ve.
%\]
%Thus, for $n\geq n_{K}$,
%\[
%L-\ve<x_{n_{K}}\leq x_{n}.
%\]
%Thus, (\theequation) holds for $N=n_{k}$.
%\end{proof}

\medskip
Let us look at some examples.
\begin{example}
Let $a_n= \frac{n-1}{n}$. Then $(a_n)$ is increasing since $\frac{n}{n+1} \geq \frac{n-1}{n} \iff n^2 \geq (n+1)(n-1) = n^2 -1$, which is true for all $n$. $(a_n)$ is also bounded above by $1$. The MCT tells us that $(a_n)$ converges to some $L \leq 1$. (In fact we don't need the MCT for this example because $ a_n = 1 - 1/n \to 1 - 0 = 1$ by the rules for limits, but it nevertheless illustrates the general point.)
\end{example}
The next example can also be done by bare hands, but we use it to introduce a general technique which will be very useful to us: {\em finding a simple equation which any potential limit {\bf must} satisfy.}

\begin{example}
\label{ex:power}
If $0<a<1$, show that the sequence $(a^n)_{n=1}^\infty$ converges and $\lim_{n\rightarrow\infty} a^{n}=0$. \\

\noindent
{\bf Solution:} Since $a^n$ is a product of positive numbers, it is positive, so the sequence $(a^{n})_{n=1}^{\infty}$ is bounded below by $0$. Moreover, since $a<1$, $a^{n+1}=a\cdot a^{n}<a^{n}$, so the sequence is decreasing. By the MCT, it has a limit $L$. To find $L$, we will find a simple equation which $L$ must satisfy -- in this case $aL = L$. Recall that if $(a_{n})$ is a convergent sequence, then $(a_{n+1})$ also converges, and indeed converges to the same limit (see the exercises from last week). Thus, by the rules for limits,
\[
L=\limn a^{n} = \limn a^{n+1}=\limn a^{n} =a\limn a^{n}=aL,\]
as we claimed. Therefore $L(a-1) =0$ and since $a \neq 1$ we must have $L =0$, and we are done.
\end{example}
In this example, we used the rules for limits, together with the fact that if $(x_n)$ converges, then $(x_{n+1})$
also converges to the same limit. In the next example we instead use the fact that if $x_n \to L$, then $x_{2n} \to L$ too (see Exercise~\ref{four}).

\begin{example}\label{2^{1/n}}
Let $a_n = 2^{1/n}$. Show that $(a_n)$ converges and find its limit.\\

\noindent
{\bf Solution:} We notice that $2^{1/n} \geq 1$ for all $n$ (since this is equivalent to $2 \geq 1^n = 1$) and also that $2^{1/(n+1)} \leq 2^{1/n}$
for all $n$ (since this is equivalent to
$2^n \leq 2^{n+1}$). Therefore $(a_n)$ is a decreasing sequence which is bounded below by $1$. Consequently, by the MCT, it converges to some number $L$ which satisfies $L\geq1$ (and $L< 2$). %Unfortunately in this case the rules for limits don't help us further and we need to do some more guesswork. There is no reason to suspect that $L$ is strictly bigger than $1$ so let's try to rule that out possibility. Indeed, if $L >1$, then we have $a_n \geq L$ for all $n$ (since $(a_n)$ is decreasing towards $L$), and therefore $2 \geq L^n$ for all $n$. But if $L>1$, $L^n \to \infty$ as $n \to \infty$, and therefore for all sufficiently large $n$ it will satisfy $L^n > 2$, which is a contradiction. The only remaining possibility is $L=1$.
Consider the sequence $b_n = 2^{1/2n}$. It runs through alternate values of $a_n$, and so $b_n \to L$ also as $n \to \infty$. But $b_n^2 = a_n$, so by the rules for limits we must have $L^2 = L$, or $L(L-1) =0$, meaning either $L=0$ or $L=1$. Since $1 \leq L < 2$ the only possibility is $L = 1$.
\end{example}
Sometimes it can be helpful to find the possible limits $L$ {\em before}
establishing monotonicity and boundedness. Indeed, knowing possible values of $L$ can help us to know what we are looking for in seeking to establish monotonicity and boundedness.

\section{Iteratively defined sequences}%
\label{iterativelydefinedsequences}
Iteratively or recursively defined sequences are those where the value of $x_{n+1}$ is specified in terms of previous values of the sequence $x_1, \dots, x_n$, (most usually just in terms of $x_n$).
\begin{example}
In high school you may have met recursively defined sequences of the form $x_1 = c$,
\[ x_{n+1} = a x_n +b \; \mbox{ for } \; n \geq 1\]
where $a, b, c \in \mathbb{R}$. Show that when $|a| < 1$, $(x_n)$ converges and find its limit. Show that when $|a|>1$, $(x_n)$ diverges. What happens when $a = \pm 1$?
\\

\noindent
{\bf Solution:} Let's first give ourselves a sneak preview of what $L$ might be in terms of $a,b$ and $c$. By the rules for limits, {\em if} the sequence $(x_n)$ converges to $L$, then $L$ {\em must} satisfy $L = aL + b$, or equivalently $L = \frac{b}{1-a}$. Since we cannot divide by $0$, this already shows us that the case $a=1$ is special.

\medskip
Indeed, if $a = 1$, then $L = aL + b$ becomes $L = L + b$, and unless $b=0$ this cannot hold. So when $a =1$, the sequence diverges in the case that $b \neq 0$, and is the constant sequence $x_n = c$ in the case that $b = 0$.

\medskip
Now suppose $a \neq 1$. Since we are guessing the limit to be $\frac{b}{1-a}$, it makes sense to set $y_n = x_n - \frac{b}{1-a}$ and to re-write the basic equation $x_{n+1} = a x_n +b$ in the equivalent form $y_{n+1} = a y_n$. This in turn the same as $y_n = (c - \frac{b}{1-a})a^{n-1}$, as is easily established by induction. If $|a|>1$, $|y_n|$ diverges to $+\infty$ and so $(x_n)$ also diverges; if $|a| < 1$, then $y_n \to 0$ as we have already seen in a previous example, and this is the same as $x_n \to
\frac{b}{1-a}$. The remaining case $a= -1$ we leave as an exercise.
\end{example}

The MCT is especially useful for proving convergence of some (but not all) iteratively or recursively defined sequences.
\begin{example}
In 60AD, the Greek mathematician Hero of Alexandria wanted to approximate $\sqrt{2}$ with rational numbers. %\footnote{This example is not too far off from how your calculator or computer is able to compute $\sqrt{2}$ to any degree of accuracy: it has pre-installed a recurrence relation that converges $\sqrt{2}$.}.
He came up with the following recurrence relation, now called {\it Heron's Method}, for computing $\sqrt{2}$. Let $x_{1}=2$ and for each $n\in\mathbb{N}$, let
\[
x_{n+1}=\frac{1}{2}\left(x_{n}+\frac{2}{x_{n}}\right).
\]
Let us imagine that we knew that $(x_n)$ converged to something, $L$. What must $L$ satisfy? Well, by the rules for limits, since $x_{n+1} \to L$ too, and assuming that $L \neq 0$, it must satisfy
\[
L=\frac{1}{2}\left(L+\frac{2}{L}\right),
\]
or $2L =L + \frac{2}{L}$, or $L = \frac{L}{2}$, which is the same as $L^2 = 2$, or $L = \pm\sqrt{2}$. We want to narrow this down so that the only possibility is $L = + \sqrt{2}$, and also so that we do indeed know that $(x_n)$ converges. Given that $x_1 = 2 > \sqrt{2}$, it makes sense to try to show that $(x_n)$ is decreasing, and is bounded below by $\sqrt{2}$, which is the same thing as $x_n > 0$ and $x_n^2 \geq 2$ for all $n$. Once we have this, the MCT will imply that $(x_n)$ does indeed converge to some number $L$, which must then satisfy $L \geq 0$ and $L^2 = 2$, thus indeed $L = + \sqrt{2}$.

Let's do a bit of experimentation to see whether $(x_n)$ is decreasing or not by computing a few values (which you can do on a calculator or writing a simple program):

\[
x_{1}=2,\;\; x_{2} = \frac{1}{2}\left(2+\frac{2}{2}\right) = \frac{3}{2}, \;\; x_{3}=\frac{1}{2}\left(\frac{3}{2}+\frac{2}{3/2}\right) = \frac{17}{12}, \;\; x_{4} = \frac{237}{408}.
\]
Since $ 2 > 3/2 > 17/12 > 237/408$, we see that
\[ x_1 > x_2 > x_3 > x_4, \]
and this suggests that the sequence might indeed be decreasing. Let's try to prove this.
This is the same thing as showing $x_{n}-x_{n+1}\geq 0$, so let's look at this difference:
\[
x_{n}-x_{n+1}=x_{n}-\frac{1}{2}\left(x_{n}+\frac{2}{x_{n}}\right) =\frac{x_{n}^{2}-2}{2x_{n}}.
\]
Thus, we just need to show that this last term is non-negative. The $x_{n}$ are always positive, this is established by induction: $x_1>0$, and if $x_n>0$, then we see that $x_{n+1}>0$ also straight from the definition. Hence, we just need to show that $x_{n}^{2}\geq 2$ for all $n\geq 1$. This we also prove by induction! If $n=1$ then $x_{1}=2$, and for $n>1$, it follows from the fact that for $x \neq 0$
\[
\left(\frac{1}{2}\left(x+\frac{2}{x}\right)\right)^2 \geq 2,
\]
which, finally, is something that we can just check directly:
\[
\left(\frac{1}{2}\left(x+\frac{2}{x}\right)\right)^2 \geq 2 \iff \left(x+\frac{2}{x}\right)^2 \geq 8 \iff x^2 + \frac{4}{x^2} + 4 \geq 8 \iff \frac{(x^2 - 2)^2}{x^2} \geq 0,
\]
and the last statement is clearly true. Hence, we conclude that the sequence $(x_n)$ is decreasing.

Notice that in proving that $(x_n)$ is decreasing we have already established that $x_n > 0$ and $x_n^2 \geq 2$ for all $n$! Thus we immediately conclude from the earlier remarks that $(x_n)$ converges to $L= +\sqrt{2}$!
\end{example}
{\bf Point to ponder:} How might you modify Heron's formula to construct a sequence of rational numbers converging to $\sqrt{7}$? or to $\sqrt{51}$?

Note from the above argument that induction is a really helpful tool to have at our disposal when we are dealing with recursively defined sequences. We see this again in the next example.

The previous example had the feature that the relevant boundedness of the sequence was immediate: the terms were all positive (hence bounded below by zero). Let's do an example where the relevant boundedness is not so immediate:

\begin{example}
\label{ex:1+sqrt(x)}
Let $x_{1}=1$ and $x_{n+1}=1+\sqrt{x_{n}}$ for $n \geq 1$. Prove that $(x_{n})$ converges and find its limit. \\

As before we first give ourselves a sneak preview of what the limit $L$ must satisfy if we assume it exists. By the rules for limits we must have
\begin{equation}
\label{e:L=sqrtL+1}
L=\lim_{n\rightarrow\infty}x_{n}
=\lim_{n\rightarrow\infty}x_{n+1}=\lim_{n\rightarrow\infty}(1+\sqrt{x_{n}})=1+\sqrt{L}.
\end{equation}
So $L$ must satisfy $(L-1)^2 = L$, i.e. $L^2 -3L +1=0$, i.e. $(L-\frac{3}{2})^2 = \frac{5}{4}$, and this equation has solutions $L = \frac{3 \pm \sqrt{5}}{2}$. Both of these are positive but $ \frac{3 -\sqrt{5}}{2} < 1 < \frac{3 + \sqrt{5}}{2}$. Now $x_1 = 1$ and it's easily proved by induction that $x_n \geq 1$ for all $n$. So {\em if} $(x_n)$ converges, it {\em must} converge to $\frac{3 + \sqrt{5}}{2}$.

We hope that $(x_n)$is increasing, so if we prove it is also bounded, the MCT will imply it converges, and by what we have said above, the limit must be $\frac{3 + \sqrt{5}}{2}$.

Let us prove it is increasing by using induction. We want that $x_{n+1}\geq x_{n}$ for all $n\in\mathbb{N}$. For the base case, we have $x_{2} = 1+\sqrt{1} > 1 = x_1$. For the inductive step, assume for a certain $n\in\mathbb{N}$ that $x_{n+1}\geq x_{n}$. Then
\[
x_{n+2}-x_{n+1}
=1+\sqrt{x_{n+1}}-(1+\sqrt{x_{n}})
=\sqrt{x_{n+1}}-\sqrt{x_{n}}
\]
and since $x_{n+1}\geq x_{n}$ by the inductive hypothesis, we then also have $\sqrt{x_{n+1}}\geq \sqrt{x_{n}}$ by Proposition 3.5. So the above displayed expression is non-negative, and thus $x_{n+2}\geq x_{n+1}$. This proves the inductive step and hence that $(x_n)$ is increasing.

Now we need show it is bounded above. Since we expect it to converge to $L = \frac{3 + \sqrt{5}}{2}$, it makes sense to try to show that this $L$ is an upper bound.
Let us prove that $x_{n}\leq L$ for all $n\in\mathbb{N}$ by induction.
We can verify the base case $x_{1}=1\leq L$ is already true. For the inductive step, it turns out that the actual value of $L$ won't be as useful as the identity $L=1+\sqrt{L}$ we derived in equation \eqref{e:L=sqrtL+1}. Assume we have proved $x_{n}\leq L$ for some $n$. Then
\[
x_{n+1}=1+\sqrt{x_{n}}\leq 1+\sqrt{L}=L.
\]
This proves the inductive step, and hence proves that $x_{n}\leq L$ for all $n$. Hence, $(x_n)$ is bounded above and is increasing, so the MCT implies it converges. As we showed above, it must converge to $L = \frac{3 + \sqrt{5}}{2}$.
\end{example}

\section{Exercises}%
\label{monotoneexercises}

The only relevant exercise from Liebeck's book is Problem 7 in Chapter 23, which we discussed as Heron's method already.

\begin{exercise} Suppose that $(x_{n})$ is increasing. Show that either $(x_{n})$ converges or else $(x_{n})$ tends to infinity.

\begin{solution}
If $(x_{n})$ is bounded above, then it converges. Otherwise, it is unbounded above, so for all $M>0$ there is $N$ so that $x_{N}\geq M$. Since $x_{n}$ is increasing, we know that for all $n\geq N$,
\[
x_{n}\geq x_{N}\geq M.
\]
Thus, we have shown that for all $M$, there is $N$ so that $n\geq N$ implies $x_{n}\geq M$, which means $x_{n}\rightarrow\infty$.
\end{solution}
\end{exercise}

\begin{exercise}
If $(x_n)$ is a bounded sequence and $(x_n+\frac{1}{n})$ is monotonic, does $(x_n)$ converge?
\begin{solution}
Yes. First note that $(x_n + \frac{1}{n})$ is bounded. By the MCT, the bounded monotonic sequence $(x_n+\frac{1}{n})$ converges. Since $\frac{1}{n}\rightarrow 0$, we know that $(x_{n}+\frac{1}{n})-\frac{1}{n}=x_{n}$ converges as well by the rules for limits.
\end{solution}
\end{exercise}

\begin{exercise}
Let $(a_{n})$ be a sequence of numbers such that $0\leq a_{n}\leq 1$ and let $s_{n}$ denote the product $a_{1}\cdot a_{2}\cdots a_{n}$. Does $(s_{n})$ converge?
\begin{solution}
Yes. Note that as $a_{n}\geq 0$ for all $n$, $s_{n}\geq 0$ as well. Moreover, $s_{n+1} = a_{n+1}s_{n}\leq s_{n}$ since $a_{n+1}\leq 1$ for all $n$. Thus, $s_{n}$ is decreasing and bounded below, and hence converges by the MCT.
\end{solution}
\end{exercise}

\begin{exercise}\label{four}
Using the definition of convergence, show that if $a_n \to L$ as $n \to \infty$ and if $b_n = a_{2n}$, then $b_n \to L$ as $n \to \infty$.
\begin{solution}
Let $\epsilon > 0$. There there is an $N$ such that $n> N$ implies $| a_n - L|< \epsilon$.
So $m > N/2$ implies $|a_{2m} - L|< \epsilon$.
\end{solution}

\end{exercise}

\begin{exercise}
Let $a_n = n^{1/n}$. Show that $(a_n)$ converges and find its limit. ({\bf Hint:} First prove that $(1+ \frac{1}{n})^n \leq 4$ for all $n$, and then try to use the MCT. Alternatively, use the formula for $a^n - b^n$ in Exercise~\ref{81} for well-chosen $a$ and $b$.)

\begin{solution}
We can establish the hint using the binomial theorem, see also the discussion of $e$ in Section 8. We try out whether $(a_n)$ is decreasing. We have
\[ a_{n+1} \leq a_n \iff (n+1)^{1/(n+1)} \leq n^{1/n}
\iff (n+1)^n \leq n^{n+1} \iff \left(1 + \frac{1}{n}\right)^n \leq n.
\]
We saw earlier that $\left(1 + \frac{1}{n}\right)^n \leq 4$ for all $n$, so we have $a_{n+1} \leq a_n$ at least when $n \geq 4$. Since $a_n \geq 1$ we have that $(a_n)$
is bounded below. This is enough for us to deduce from the MCT that it converges. Let $L = \limn n^{1/n} \geq 1$. Let $b_n = a_{2n} = (2n)^{1/2n} = 2^{1/2n} \sqrt{n^{1/n}}$. Since $b_n$ runs through alternate values of $a_n$ we have $b_n \to L$ too.
But $2^{1/2n} \sqrt{n^{1/n}} \to 1^{1/2} \times L^{1/2} = L^{1/2}$ by the rules for limits, Example 6.3 and Example~\ref{2^{1/n}}. Thus $L = L^{1/2}$ and so $L = 1$.

Alternatively, (assuming $n$ is even for simplicity)
\[ n - 1 = (n^{1/n} - 1)(1 + n^{1/n} + n^{2/n} + \dots + n^{(n-1)/n}) \]
\[\geq  (n^{1/n} - 1)(n^{n/2n} + n^{(n+2)/2n} + \dots + n^{(2n-2)/2n}) \geq (n^{1/n} - 1) \times \frac{n-1}{2} \times n^{1/2}\] so that
\[ 0 \leq  (n^{1/n} - 1) \leq \frac{2}{n^{1/2}}.\]
Now use the squeeze theorem.
\end{solution}
\end{exercise}

\begin{exercise}
 Let $x_{1}=1$ and $x_{n+1}=\sqrt{1+x_{n}}$ for $n \geq 1$. Show that $(x_n)$ converges and compute its limit.

\begin{solution}
We claim that $x_n$ converges. First, we show that it is increasing by induction, that is, that $x_{n+1}\geq x_{n}$ for all $n\geq 1$. For the base case, we have $x_{1}=1\leq \sqrt{1+1}=x_{2}$. Now suppose we have shown  $x_{n+1}\geq x_{n}$ for some integer $n\geq 1$. Then
\begin{align*}
x_{n+2} -x_{n+1}
& =\sqrt{1+x_{n+1}}-\sqrt{1+x_{n}}\\
& = \left(\sqrt{1+x_{n+1}}-\sqrt{1+x_{n}}\right)\cdot\frac{\sqrt{1+x_{n+1}}+\sqrt{1+x_{n}}}{\sqrt{1+x_{n+1}}+\sqrt{1+x_{n}}}\\
& = \frac{1+x_{n+1}-(1+x_{n})}{\sqrt{1+x_{n+1}}+\sqrt{1+x_{n}}}
 = \frac{x_{n+1}-x_{n}}{\sqrt{1+x_{n+1}}+\sqrt{1+x_{n}}}
\end{align*}
Since the denominator is always positive, we just need to check that the numerator is positive, but $x_{n+1}-x_{n}\geq 0$ by the inductive hypothesis, and so we have $x_{n+2} -x_{n+1} \geq 0 $. This proves the inductive step and hence the claim.

Now we need to show $x_n$ is bounded above. Note that if $x_n$ did converge to some number $L$, then
\begin{equation}
\label{e:L=sqrt(1+L)}
L=\lim_{n\rightarrow\infty}x_{n}
=\lim_{n\rightarrow\infty}x_{n+1}
=\lim_{n\rightarrow\infty}\sqrt{1+x_{n}}
=\sqrt{1+L}
\end{equation}
and squaring both sides and moving everything to the left gives $L^2-L-1=0$, so the only possible values for $L$ are $\frac{1\pm\sqrt{5}}{2}$. Since $L=\sqrt{1+L}> 0$, we must have $L=\frac{1+\sqrt{5}}{2}$. Thus, since $x_n$ is increasing, it is natural to guess that $x_n$ is bounded above by $L$. Let's prove this by induction. For $n=1$, we have $1\leq L$ already, so the base case is true. For the induction step, assume $x_{n}\leq L$ for some $n$. Then
\[
x_{n+1}=\sqrt{1+x_{n}}\leq \sqrt{1+L}\stackrel{\eqref{e:L=sqrt(1+L)}}{=}L.\]
This proves the induction step, and so $(x_n)$ is bounded above and is increasing, thus $(x_n)$ converges and its limit is $\frac{1+\sqrt{5}}{2}$.
\end{solution}

\end{exercise}

\begin{exercise}
 Let $x_1=\frac{1}{4}$ and $x_{n+1} = \sqrt{x_{n}(1-x_{n})}$ for $n \geq 1$. Show that $(x_n)$ converges and compute its limit. (Hint: First show that $0< x_{n}\leq \frac{1}{2}$ for all $n$. The AM--GM inequality may come in useful.)

\begin{solution}
We first show by induction that $0 < x_n \leq 1/2$ for all $n$. The case $n=1$ is trivial. Suppose inductively that for some $n$ we have $0 < x_n \leq 1/2$. Then $x_{n+1} =\sqrt{x_{n}(1-x_{n})}>0$ and, by the AM-GM inequality,
\[
x_{n+1}=\sqrt{x_{n}(1-x_{n})}\leq \frac{x_{n}+1-x_{n}}{2} = \frac{1}{2}.
\]
thus $x_{n}\leq \frac{1}{2}$ too, establishing the inductive step.

Now we claim that $(x_{n})$ is increasing:
\begin{align*}
x_{n+1}-x_{n}
& =\sqrt{x_{n}(1-x_{n})}-x_{n}
= \left(\sqrt{x_{n}(1-x_{n})}-x_{n}\right)\frac{\sqrt{x_{n}(1-x_{n})}+x_{n}}{\sqrt{x_{n}(1-x_{n})}+x_{n}}\\
& = \frac{x_{n}(1-x_{n})-x_{n}^{2}}{\sqrt{x_{n}(1-x_{n})}+x_{n}}
= \frac{x_{n}(1-2x_{n})}{\sqrt{x_{n}(1-x_{n})}+x_{n}} \geq 0
\end{align*}
since we've already established that $0 < x_n \leq 1/2$ for all $n$. Thus, $(x_{n})$ is increasing and bounded, and thus convergent. Let $L$ be its limit. Then
\[
L=\lim_{n\rightarrow \infty}x_{n}
=\lim_{n\rightarrow \infty}x_{n+1}
=\lim_{n\rightarrow \infty}\sqrt{x_{n}(1-x_{n})}
=\sqrt{L(1-L)}
\]
squaring both sides and moving everything to the left gives
\[
2L^2-L=0.
\]
The only solutions to this are $L=0$ and $L=\frac{1}{2}$. Since $(x_{n})$ is increasing, we have $x_{n}\geq x_{1} = \frac{1}{4}>0$, so $L\geq \frac{1}{4}$, so $x_{n}$ can't converge to zero, thus $L=\frac{1}{2}$.
\end{solution}
\end{exercise}

\begin{exercise} Let $x_{1}=\sqrt{2}$ and $x_{n+1}=\sqrt{2+\sqrt{x_{n}}}$ for $n \geq 1$. Show that $(x_n)$ converges and find the quartic equation of which the limit is a root. (When showing it is bounded above, you can try making an educated guess as to what an upper bound might be, try some values out!)

\begin{solution}
First we claim that $x_n >0$ for all $n$ (so that $\sqrt{x_n}$ makes sense). This is a simple exercise using induction. Next we claim that $(x_{n})$ is increasing. We prove this also by induction. For $n=1$, we clearly have $x_{1}=\sqrt{2}<\sqrt{2+\sqrt{2}}=x_{2}$. This proves the base case. Now suppose that $x_{n+1}\geq x_{n}$ for some integer $n\geq 1$. Then
\begin{align*}
x_{n+2}-x_{n+1}
& =\sqrt{2+\sqrt{x_{n+1}}}-\sqrt{2+\sqrt{x_{n}}}\\
& =\left(\sqrt{2+\sqrt{x_{n+1}}}-\sqrt{2+\sqrt{x_{n}}}\right)\cdot \frac{\sqrt{2+\sqrt{x_{n+1}}}+\sqrt{2+\sqrt{x_{n}}}}{\sqrt{2+\sqrt{x_{n+1}}}+\sqrt{2+\sqrt{x_{n}}}}\\
& = \frac{2+\sqrt{x_{n+1}}-2+\sqrt{x_{n}}}{\sqrt{2+\sqrt{x_{n+1}}}+\sqrt{2+\sqrt{x_{n}}}}\\
& = \frac{\sqrt{x_{n+1}}-\sqrt{x_{n}}}{\sqrt{2+\sqrt{x_{n+1}}}+\sqrt{2+\sqrt{x_{n}}}}\\
& \geq \frac{0}{\sqrt{2+\sqrt{x_{n+1}}}+\sqrt{2+\sqrt{x_{n}}}}=0,\\
\end{align*}
where in the last line we used the inductive hypothesis that $x_{n+1}\geq x_{n}$, and so $\sqrt{x_{n+1}}-\sqrt{x_{n}}\geq 0$. Thus, $(x_{n})$ is increasing.

Next, we need to show that it is bounded above. We claim that $x_{n}\leq 2$ for all $n$. We prove this by induction as well. For the base case, $x_{1}=\sqrt{2}\leq 2$. For the inductive step, suppose $x_{n}\leq 2$ for some $n$. Then
\[
x_{n+1}=\sqrt{2+\sqrt{x_{n}}}\leq \sqrt{2+\sqrt{2}}\leq \sqrt{2+2}=\sqrt{4}=2.
\]
This establishes the inductive step. Thus, $x_{n}$ is increasing and bounded above, so it converges. Let $L$ be the limit. Then, by the rules for limits,
\[L = \sqrt{2+\sqrt{L}}\
\]
and therefore $L$ is a root of the equation $x^4 - 4x^2 -x + 4 = 0$.
\end{solution}
\end{exercise}

%
% \item $x_{1}=3$ and $x_{n+1}=\frac{2}{x_{n}}+\frac{x_{n}}{2}$. (Hint: a useful inequality from Week 2 might help...)
%
%
%\begin{solution}
%After trying out a few values, it seems like this sequence is decreasing, and by induction we can show the terms are always positive (since $x_n>0$ implies $x_{n+1}>0$), so let's try to verify it is decreasing, for then it will converge by the MCT: for $n\geq 1$
%\[
%x_n-x_{n+1}=x_n-\frac{2}{x_{n}}-\frac{x_{n}}{2}=\frac{x_{n}}{2}-\frac{2}{x_{n}} = \frac{x_n^2-4}{2x_n}.
%\]
%Since the values of $x_n$ are always positive, it , we just need to show that $x_n^2-4\geq 0$ for all $n$,  but $x_1=3$ and for $n\geq 2$, by the AM-GM inequality $\frac{a+b}{2}\geq \sqrt{ab}$,
%\[
%x_{n+1} = \frac{2}{x_{n}}+\frac{x_{n}}{2}
%\geq 2\left(\sqrt{\frac{2}{x_{n}}}\sqrt{\frac{x_{n}}{2}}\right)^{\frac{1}{2}}
%=2.\]
%Thus $x_n$ is bounded below and decreasing and thus convergent. We can solve for the limit:
%\[
%L=\lim_{n\rightarrow\infty}x_{n}
%=\lim_{n\rightarrow\infty}x_{n+1}
%=\lim_{n\rightarrow\infty} \left(\frac{x_{2}}{2}+\frac{2}{x_n}\right)
%=\frac{L}{2}+\frac{2}{L}.
%\]
%Thus, we can rearrange this to get the formula
%\[
%L^2=4\]
%Since $x_n\geq 0$, $L\geq 0$, and so $L=2$.
%\end{solution}

%\item Let $a>1$, $x_1=1$ and for $n>1$ set
%\[
%x_{n+1}=\frac{2x_n^3+a}{3x_n^2}.
%\]
%%
%\begin{solution}
%Let's look at the differences to try and find out whether the sequence is increasing or decreasing:
%\begin{align*}
%x_{n+1}-x_{n}
%& = \frac{2x_n^3+a}{3x_n^2}=x_{n}\\
%& = \frac{-x_{n}^{3}+a}{3_{x_{n}^{2}}.
%\end{align*}
%Thus, we can determine whether $x_n$ is increasing or decreasing by determining the sign of $-x_{n}^{3}+a$. Observe that since $x_1=1$, we have that $-x_{1}^{3}+a=-1+a>0$. Now suppose we have shown $-x_{n}^{3}+a>0$ for some $n\geq 1$, so $a>x_{n}^{3}$ which means $a^{\frac{1}{3}}>x_{n}$. Then
%\[
%x_{n+1} =\frac{2x_n^3+a}{3x_n^2}
%<

%
%
%
%\begin{exercise}  Let $x_{n}=\sum_{i=1}^{n}\frac{1}{n+i}$. Show that $x_{n}$ converges.
%
%\begin{solution}
%Let's look at the differences:
%\begin{align*}
%x_{n+1}-x_{n}
%& =\sum_{i=1}^{n+1}\frac{1}{n+1+i}-\sum_{i=1}^{n}\frac{1}{n+i}\\
%& =\sum_{i=2}^{n+2}\frac{1}{n+i}-\sum_{i=1}^{n}\frac{1}{n+i}\\
%& = \frac{1}{2n+1}+\frac{1}{2n+2} - \frac{1}{n+1}
%=\frac{1}{2n+1}-\frac{1}{2n+2}
%=\frac{1}{(2n+1)(2n+2)}
%\end{align*}
%Thus,
%\[
%|x_{n+1}-x_{n}|
%\leq \frac{1}{(2n+1)(2n+2)}
%\leq \frac{1}{n^2}
%\]
%Since the differences are summable, $\sum (x_{n}-x_{n-1})$ is absolutely convergence, and so
%\[
%x_{n}-x_{0}=\sum_{i=0}^{n-1}(x_{i+1}-x_{i})
%\]
%converges, and so does $x_n$.
%
%
%\end{solution}
%
%\end{exercise}

\chapter{Decimals and Series}%
\label{decimalsseries}
We are all familiar with decimal representation of numbers. For example, given a rational number $x = \frac{p}{q}$ with $p, q \in \mathbb{N}$ and $0 < p < q$, we can go through the process of dividing $q$ into $p$ to represent it as
\[ x = 0.a_1 a_2 \dots a_n \dots\]
where each $a_n$ is an integer in the set $\{0,1,\dots, 9\}$. So we are used to writing \[ \frac{1}{2} = 0.5 = 0.50000\dots \mbox{
and } \frac{2}{3} = 0.66666\dots\]
and we are also familiar with such expressions as
\[ \sqrt{2} = 1.414\dots \mbox{ and } \pi = 3.14159 \dots. \]
There are also more random looking examples such as $0.7264082793684301...$ which have no discernible pattern nor immediate purpose in life. But what exactly do we mean by these expansions? In this chapter we explore this question, and give a precise meaning to decimal expansions. We will show that {\em every} real number has a decimal expansion, and conversely, every decimal expansion gives rise to a real number. Thus, in some sense, decimal expansions provide a complete description of the real numbers. In order to do this, we use the study of sequences which we have already begun, and, in particular, we will make crucial use of the Completeness Axiom for $\mathbb{R}$ (as indeed we must).

Then we will introduce infinite series and relate them to decimal expansions. You are probably familiar with certain infinite series already, such as the infinite geometric series
\[ \sum_{n=1}^\infty \frac{1}{2^n} = 1,\]
but we will examine series such as these from a more general perspective. Now is a good time to recall the formula for a finite geometric series:
\[ a + ar + ar^2 + \cdots + ar^{n-1} = \frac{a(1-r^{n})}{1-r}\]
whenever $r \neq 1$. You will learn more about infinite series if you are taking the course Calculus and its Applications. Finally, we introduce the number $e$ and study some of its basic properties.

\section{Decimals}%
\label{decimals}

Given a sequence $(a_n)$ with each $a_n \in \{0, 1, \dots , 9\}$, what exactly do we mean by the decimal
\[ 0.a_1 a_2 \dots a_n \dots?\]
We all know the convention that the number $a_1$ gives the number of tenths, $a_2$ gives the number of hundredths,
$a_3$ gives the number of thousandths etc. But what do we mean by the ellipses at the end of the expansion? A good way to make sense of this is to consider the {\em finite} decimal expansions
\[ x_n := 0.a_1 a_2 \dots a_n = \frac{a_1}{10} + \frac{a_2}{100} + \dots + \frac{a_n}{10^n}\]
and declare the infinite decimal expansion
\[ 0.a_1 a_2 \dots a_n \dots\]
to be the limit of the sequence $(x_n)$ as $n \to \infty$. But this naturally poses the question of whether the limit must even exist. Fortunately this is easy: the sequence
$(x_n)$ consists of positive numbers, and they satisfy $x_{n+1} \geq x_n$ so that $(x_n)$ is increasing. Finally, using the formula for a finite geometric progression,
\[ x_n = \frac{a_1}{10} + \frac{a_2}{100} + \dots + \frac{a_n}{10^n} \leq \frac{9}{10} + \frac{9}{100} + \dots + \frac{9}{10^n} =  \frac{9}{10}\frac{(1 - (\frac{1}{10})^{n+1})}{1 - \frac{1}{10}} \leq \frac{9}{10}\frac{1}{1 - \frac{1}{10}}=1,\]
so that $(x_n)$ is bounded above by $1$. By the MCT, the sequence $(x_n)$ converges to some $x \in \R$ satisfying $0 \leq x \leq 1$.
We have thus proved:
\begin{theorem}
Given a sequence $(a_n)$ with $a_n \in \{0,1, \dots ,9\}$ for all $n$, the decimal expansion $ x= 0.a_1 a_2 \dots a_n \dots$ given by the limit of the sequence
\[ x_n = \frac{a_1}{10} + \frac{a_2}{100} + \dots + \frac{a_n}{10^n} \]
defines a real number $x$ satisfying $0 \leq x \leq 1$.
\end{theorem}
What about the converse -- does every real number $x$ with $0 \leq x <1$ have a decimal expansion? Our intuition and experience tells us that this is so, but now we prove it more formally.

\begin{theorem}\label{decimal_existence}
Let $x\in\mathbb{R}$ satisfy $0 \leq x <1$. Then there is a sequence of integers $(a_n)$, with $a_{n}\in \{0,1,\dots,9\}$ for all $n$, so that if we let
\[ x_n = \frac{a_1}{10} + \frac{a_2}{100} + \dots + \frac{a_n}{10^n},\]
then $x_n \to x$ as $n \to \infty$.
\end{theorem}

\begin{proof}
We find the integers $a_{n}$ inductively. To find $a_1$, we split the interval $[0,1)$ into 10 equal subintervals
\[ \left[0,\frac{1}{10}\right),  \; \left[\frac{1}{10}, \frac{2}{10}\right), \; \dots , \; \left[\frac{9}{10},\frac{10}{10}\right).\]
Then $x$ must be in exactly one of these intervals, say $\left[\frac{a_1}{10}, \frac{a_1 +1}{10}\right)$, for a unique $a_1 \in \{0, 1, \dots , 9\}$.
Notice that
\[ 0 \leq x - \frac{a_1}{10} < \frac{1}{10}.\]
To find $a_2$, we split the interval $\left[\frac{a_1}{10}, \frac{a_1 +1}{10}\right)$ into 10 equal subintervals
\[
\left[\frac{a_{1}}{10},\frac{a_{1}}{10}+\frac{1}{10^2}\right),\;
\left[\frac{a_{1}}{10}+\frac{1}{10^2},\frac{a_{1}}{10}+\frac{2}{10^2}\right), \; \cdots \;
,\left[\frac{a_{1}}{10}+\frac{9}{10^2},\frac{a_{1}+1}{10}\right).
\]
Then $x$ must be in exactly one of these intervals as well, which can be written as $\left[\frac{a_{1}}{10}+\frac{a_{2}}{10^2},\frac{a_{1}}{10}+\frac{a_2+1}{10^2}\right)$ for some unique $a_{2}\in \{0,1, \dots ,9\}$. Notice that
\[ 0 \leq x - \left(\frac{a_1}{10} + \frac{a_2}{10^2}\right) < \frac{1}{100}.\]
\noindent
Continuing in this way, we find for each $n$ a unique integer $a_{n}\in \{0,1,\dots,9\}$ so that
\begin{equation*}
\label{e:xininterval}
x\in \left[\frac{a_{1}}{10}+\cdots +\frac{a_{n}}{10^{n}}, \frac{a_{1}}{10}+\cdots  +\frac{a_{n}+1}{10^{n}}\right)
\end{equation*}
and so that
\[ 0 \leq x- \left( \frac{a_{1}}{10}+\cdots +\frac{a_{n}}{10^{n}} \right) < \frac{1}{10^n}.\]
Then we have
\[ 0 \leq x - x_n < \frac{1}{10^n},\]
and since $1/10^{n} \to 0$ as $n \to \infty$, we have that $x_n \to x$ by the squeeze theorem.
\end{proof}

\medskip
What about decimal expansions $x = a_0.a_1 a_2 \dots$ for real numbers outside the interval $[0,1)$? We look at this in the exercises. If $a_0 \in \mathbb{Z}$ and $a_n \in \{0,1,2, \dots , 9\}$ for $n\geq 1$ we write
\[ x = a_0.a_1 a_2 \dots\]
to mean that the sequence $(x_n)$ defined by
\[x_n = a_0 + \frac{a_{1}}{10}+\cdots +\frac{a_{n}}{10^{n}}\]
converges to $x$.

What about uniqueness of decimal expansions? Can a real number $x$ be represented by two distinct decimal expansions? Unfortunately, yes: for example
\[ \frac{1}{2} = 0.5000000\ldots = 0.4999999\dots \]
since, by the formula for finite geometric series, $0.499999 \dots 99$ (with $n$ $9$'s)
\[= \frac{4}{10} + \frac
{\frac{9}{100}(1- (\frac{1}{10})^{n})}
{1- \frac{1}{10}} \rightarrow \frac{4}{10} + \frac{1}{10} = \frac{1}{2}\]
as $n \to \infty$. (Which of these two representations does the procedure of Theorem~\ref{decimal_existence} spit out?)
It turns out that situations like this are essentially the only ones when this happens (one expansion trailing $0$'s and the other trailing $9$'s), and there are never more than two decimal expansions that give the same number.

\begin{theorem}\label{83}
Let $0 \leq x <1$ and suppose that $x=0.a_{1}a_2\ldots =0.b_{1}b_{2}\dots$. Let $\ell$ be the smallest integer for which $a_{\ell}\neq b_{\ell}$ and suppose that $a_{\ell}<b_{\ell}$. Then $b_{\ell}=a_{\ell}+1$, and for all $k> \ell$ we have $b_{k}=0$ and $a_{k}=9$.
\end{theorem}

\begin{proof}
We have
\[ 0.b_{1}b_{2}\dots b_{\ell -1}b_{\ell}000 \ldots \leq x \leq 0.a_1 a_2 \dots a_{\ell - 1} a_{\ell} 999 \dots , \]
and since $a_j = b_j$ for $1 \leq j \leq \ell -1$ we have
\[0.a_1 a_2 \dots a_{\ell-1} b_{\ell}000\ldots \leq x \leq 0.a_1 a_2 \dots a_{\ell - 1} a_{\ell} 999 \dots \]
But
\[0.a_1 a_2 \dots a_{\ell - 1} a_{\ell} 999 \ldots = 0.a_1 a_2 \dots a_{\ell - 1} (a_{\ell} +1)000 \dots\]
(once again by the formula for finite geometric series).
Therefore $ b_{\ell} \leq a_{\ell} +1$. Since we are assuming $a_{\ell} < b_{\ell}$, this forces $b_{\ell} = a_{\ell} +1$.

Now we have
\[
x = 0.a_1 a_2 \dots a_{\ell - 1} a_{\ell} a_{\ell + 1} \ldots = 0.a_1 a_2 \dots a_{\ell - 1} (a_{\ell} + 1) b_{\ell + 1} \ldots
\]
and if for some $k > \ell$ we had either $a_k < 9$ or $b_k > 0$, this would be violated (see Exercise~\ref{877}). Hence we conclude that for all $k > \ell$ we have $a_k = 9$ and $b_k = 0$.
\end{proof}

When we find a decimal expansion of a rational number, it begins to repeat. Why is this? Because when doing long division, eventually you obtain some remainder a second time, and then you know the division process will repeat what you did from the beginning. This holds for all rational numbers.

\begin{lemma}
\label{l:rational->repeat}
If $x\geq 0$ is rational, then it has a periodic decimal expansion, that is,
\[
x=a_0.a_{1}a_{2}\dots a_{k} \overline{b_{1}b_{2}\dots b_{\ell}}.\]
Here, this notation means that the digits $b_{1}, \dots b_{\ell}$ repeat, so
\[
a_0.a_{1}a_{2}\ldots a_{k} \overline{b_{1}b_{2}\ldots b_{\ell}}
=a_0.a_{1}a_{2}\dots a_{k} {b_{1}b_{2}\dots b_{\ell}} {b_{1}b_{2}\dots b_{\ell}}\ldots
\]

\end{lemma}
\vspace{.5cm}
Before proving this, we will introduce a technique which is very useful throughout mathematics.
\vspace{.5cm}

%
\noindent

\begin{definition} {\bf - Pigeonhole Principle}\\
\begin{minipage}[c]{0.4\linewidth}
Suppose $k>n$, and we have $a_1,\ldots, a_k \in S$, with $|S|=n$.  Then there exists $i\neq j$ with $a_i=a_j$.
\end{minipage}
\hspace{.2\linewidth}
% no space if you would like to put them side by side
\begin{minipage}[c]{0.4\linewidth}
\begin{center}
\includegraphics[height=.8in]{Figures/pigeonhole.jpg}\\
10 pigeons, 9 boxes.
\end{center}
\end{minipage}
\end{definition}

\vspace{.5cm}

\begin{proof}[Proof of Lemma \ref{l:rational->repeat}]
Write $x=p/q$ where $p$ and $q$ are positive integers. Each step of the long division algorithm $q\overline{)p.0000\ldots}$ returns remainders in $\{1,\ldots, q\}$.  By the \emph{pigeonhole principle}, one remainder, call it $k$, must occur twice.  Because $p.0000\ldots$ has trailing zeroes, the algorithm starts repeating at that point.
\end{proof}

We define the {\it period} of a decimal expansion to be the smallest number of digits in a repeating sequence in the decimal expansion. For example, $0.121212...$ has period $2$, $0.34\overline{345}$ has period 3. If there are no repeating sequences we say that the decimal expansion is {\it aperiodic}.

\begin{corollary}[of the proof] If $p$ and $q$ are positive integers, the rational number $p/q$ has a decimal expansion with period at most $q$.\end{corollary}

The converse to Lemma~\ref{l:rational->repeat} is also true:

\begin{lemma}
If $x\geq 0$ has a periodic decimal expansion, then $x$ is rational.
\end{lemma}

\begin{proof}
Suppose that $x=a_{0}.a_{1}a_{2}\ldots a_{k} \overline{b_{1}b_{2}\ldots b_{\ell}}$. Let $
 y= 0.\overline{b_{1}b_{2}\ldots b_{\ell}}$.
 If we show that $y$ is rational, then so will $x$ be, because
 \[x = a_0 + \frac{a_1}{10} + \dots + \frac{a_k}{10^k} + \frac{y}{10^{k}}.
 \]
 Let $z = 0.{b_{1}b_{2}\ldots b_{\ell}}000\ldots$, which is certainly rational. Now
 $y = \limn y_n$ where
 \[ y_n = z\left(1 + \frac{1}{10^{\ell}} + \frac{1}{10^{2\ell}} + \dots + \frac{1}{10^{n\ell}}\right) = \frac{z\left(1 - 10^{-(n+1)\ell}\right)}{1- 10^{-\ell}} \]
 so that
 \[y = \frac{z}{1- 10^{-\ell}}\]
which is indeed rational.

% If we multiply this by $10^{\ell-1}$ this shifts the digits over by $\ell$ spaces, that is,\[10^{\ell} y = b_{1}b_{2}\cdots b_{\ell} . \overline{b_{1}b_{2}\cdots b_{\ell}}=b_{1}b_{2}\cdots b_{\ell} +y\]
 % where by $b_{1}b_{2}\cdots b_{\ell}$ we mean the integer with digits $b_{1}b_{2}\cdots b_{\ell}$, not the product, let's call this integer $b_{0}$. Then we can solve and get $y=b_{0}/(10^{\ell}-1)$.
%
% \begin{align*}
% 10^{\ell} y & =  10^{\ell} \left( \frac{b_{1}}{10}+\frac{b_{2}}{10}+ \cdots + \frac{b_{\ell}}{10^{\ell}} + \frac{b_{1}}{10^{\ell+1}}+ \frac{b_{2}}{10^{\ell+2}}+\cdots  \right) \\
%&   = 10^{\ell-1}b_{1}+10^{\ell-2}b_{2}+ \cdots + b_{\ell}10^{0} + \frac{b_{1}}{10}+ \frac{b_{2}}{10^{2}}+\cdots \\
% & = \underbrace{\left( 10^{\ell-1}b_{1}+10^{\ell-2}b_{2}+ \cdots + b_{\ell} \right)}_{\mbox{call this }b_{0}}. \overline{b_{1}b_{2}\cdots b_{\ell}}\\
%& = b_{0}.\overline{b_{1}b_{2}\cdots b_{\ell}} = b_{0}+y.
% \end{align*}
% Solving for $y$ gives $y=\frac{b_{0}}{10^{\ell}-1}$, and so $y$ is rational. Thus,
%Thus,
%\begin{align*}
%x & =a_{0}.a_{1}a_{2}\cdots a_{k} \overline{b_{1}b_{2}\cdots b\ell}
 %= a_{0}.a_{1}a_{2}\cdots a_{k} + 0.\underbrace{000....0}_{k {\mbox{ zeros}}} \overline{b_{1}b_{2}\cdots b_{\ell}} \\
%& = a_{0}.a_{1}a_{2}\cdots a_{k} + 10^{-k}  \cdot 0.\overline{b_{1}b_{2}\cdots b_{\ell}}
%= a_{0}.a_{1}a_{2}\cdots a_{k}  + 10^{-k}y,
% \\
%& = a_{0}.a_{1}a_{2}\cdots a_{k}  + 10^{-k}y,
%\end{align*}
%which is a sum of two rational numbers and is thus rational.

%Note that $ a_{0}.a_{1}a_{2}\cdots a_{k}   = a_{0}+\frac{a_{1}}{10}+ \cdots + \frac{a_{k}}{10^{k}}$ and so this is rational. Since $y$ was rational, so is $10^{-\ell}y$, and thus $x$ is rational since it is the sum of these two, and so we're done.

\end{proof}

We now summarise the results on rationality and periodicity:
\begin{theorem}\label{87}
A number $x  \geq 0$ is rational $\iff$ it has a periodic decimal expansion.
\end{theorem}
Noting that all the $x$ which have two distinct decimal expansions are necessarily rational, we obtain:
\begin{corollary}\label{88}
A number $x  \geq 0$ is irrational $\iff$ it has an aperiodic decimal expansion.
\end{corollary}
Note that we proved this without ever working with irrational numbers! This gives us a way of constructing many irrational numbers, not just ones that arise as roots like $\sqrt{2}$ or $3^{\frac{1}{2}}$.

\begin{example}
The number $0.10010001\cdots $ where the number of zeros between each $1$ is {\em strictly} increasing is irrational, because there is no way the decimal expansion can repeat.
\end{example}

\section{Infinite Series}%
\label{series}
We know how to add up a finite set of numbers $a_1 + a_2 + \cdots + a_n$. But what does it mean to add up an infinite sequence of numbers
\[ a_1 + a_2 + \cdots + a_j + \cdots ?\]
A good way to make sense of this is to form the {\em new sequence} $(s_n)$ where
\[ s_n := a_1 + a_2 + \dots + a_n\]
and to define the infinite sum
$$a_1 + a_2 + \cdots + a_j + \cdots = \sum_{j=1}^\infty a_j := \limn s_n$$
to be the limit of the sequence $(s_n)$ whenever it exists. The numbers $s_n$ are called the {\em partial sums} of $\sum_{j=1}^\infty a_j$. {\em If the sequence of partial sums $(s_n)$ does not converge, we assign no meaning to $\sum_{j=1}^\infty a_j$.} More formally:
\begin{definition}
Given a sequence $(a_{j})$, we say that the infinite series $\sum_{j=1}^\infty a_{j}$ {\it converges} if the sequence of partial sums
\[
s_{n}=\sum_{j=1}^{n}a_{j}
\]
converges as $n\rightarrow\infty$. When $(s_n)$ converges we denote its limit by $\sum_{j=1}^{\infty}a_{j}$.
\end{definition}
Alternatively, given the sequence $(a_n)$, we can equivalently define the sequence $(s_n)$ recursively by
$s_1 = a_1$ and
$s_{n} = a_n + s_{n-1}$
for $n \geq 2$.

\medskip
\noindent
{\bf Question:} Give an example of an infinite series $\sum_{n=1}^\infty a_n$ which converges, and an example of one which fails to converge.
%Notice that we have used precisely this formalism in our study of decimals. We are also familiar with it in the context of geometric progressions -- for example if $a_n = 2^{-n}$, then $s_n = \frac{1}{2}{\frac{1 - 2^{-n-1}}{1- 2^{-1}}} = 1 - 2^{-n-1} \to 1$ as $n \to \infty$. Thus we evaluate the sum of the infinite geometric series\[ \left(\frac{1}{2}\right) + \left(\frac{1}{2}\right)^2 + \left(\frac{1}{2}\right)^3 + \dots = 1.\]

\medskip
Decimal expansions can be thought of as special cases of infinite series: given a sequence $(b_j)$ with each $b_j \in \{0, 1, \dots , 9\}$, we can interpret the decimal expansion $0.b_1b_2 \dots$ as the infinite series $\sum_{j=1}^\infty \frac{b_j}{10^j}$.

\medskip
\noindent
{\em \bf \em A word of warning about a possible source of confusion.} Given a sequence $(a_n)$, we can consider two questions about it: convergence of the {\bf sequence} $(a_n)$, and convergence of the {\bf series} $\sum_{j=1}^\infty a_j$. {\bf It is important to realise that these are two completely different things.} Convergence of the sequence $(a_n)$ is just what it says, while convergence of the series $\sum_{j=1}^\infty a_j$
is defined as convergence of the {\em auxiliary} sequence $(s_n)$. So it's always important to be clear whether we are talking about convergence of the sequence $(a_n)$ or convergence of the series $\sum_{j=1}^\infty a_j$. Some texts use the word \emph{summable} to describe convergent series, and this goes some way towards relieving any potential confusion, but the term \emph{convergent series} is just too ingrained in mathematical language to be eradicated. (See also Exercise~\ref{eleven}.)

\medskip
Sometimes we will also work with series which start from a different position, like $\sum_{j=2}^{\infty} \frac{1}{j(j-1)}$, and the definition is the same: we say this series is convergent if the partial sums $\sum_{j=2}^{n} \frac{1}{j(j-1)}$ converge. Whether we use the dummy variable
$j$ or $k$ (or anything else) doesn't matter: $\sum_{j=2}^{\infty} \frac{1}{j(j-1)}$ has exactly the same meaning as $\sum_{k=2}^{\infty} \frac{1}{k(k-1)}$ or $\sum_{n=2}^{\infty} \frac{1}{n(n-1)}$.

\begin{example}
\label{ex:geometric-series}
The quintessential example of a convergent series is the {\it geometric series}. Recall that for any real number $r \neq 1$,
\begin{equation}
\sum_{j=0}^{n} ar ^{j}=\frac{a(1-r^{n+1})}{1-r}
\end{equation}
If $|r|<1$, then $|r^{n+1}|=|r|^{n+1}\rightarrow 0$ by Example \ref{ex:power}, and so $r^{n+1}\rightarrow 0$ as well. Thus,
\[
\limn \sum_{j=0}^{n}ar^{j} = \limn \frac{a(1-r^{n+1})}{1-r} = \frac{\limn a(1-r^{n+1})}{1-r}=\frac{a- a\limn r^{n+1}}{1-r}=\frac{a}{1-r}.\]
Thus, when $|r|<1$, $\sum_{j=0}^\infty a r^{j}$ is convergent, and $\sum_{j=0}^{\infty} a r^{j}=\frac{a}{1-r}$. We'll see below that this series converges if and only if $|r|<1$.\\
%
%{\bf WARNING:} Even though the formula \eqref{e:gs} holds for any number $x$, the series converges only when $|x|<1$. Famously, Liebniz (that is, one of the founders of calculus, so clearly someone clever) thought that
%\[
%\sum_{n=1}^{\infty}(-1)^{n} =-\frac{1}{2}.
%\]
%This is not true: the series $\sum (-1)^{n}$, and so the above equation is meaningless. This was part of the motivation for developing the $\epsilon$-$N$-style of analysis that we covered last week: to formalize what we mean by limits to prevent mathematicians from doing pseudomath.
\end{example}

One useful necessary condition for a series to converge is the following:

\begin{proposition}\label{divgcetest}
Suppose the series $\sum_{j=1}^\infty a_j$ converges. Then the sequence $(a_j)$ satisfies $a_j \to 0$.
\end{proposition}
\begin{proof}
Let $s_n = \sum_{j=1}^n a_j$. Then for some $s \in \mathbb{R}$, $s_n \to s$. Hence $a_n = s_n - s_{n-1} \to s-s =0$.
\end{proof}
As a consequence of this we see that the geometric series of Example~\ref{ex:geometric-series} diverges when $|r| \geq 1$ since the sequence $(ar^j)_{j=1}^\infty$ does not converge to $0$ when $|r| \geq 1$.

\medskip
Just as we had rules for manipulating limits of sequences, we also have rules for manipulating infinite sums. The one
which follows says that the operation of taking infinite sums is linear. Its proof  will use the rules for limits of sequences.

\begin{proposition}
\label{p:suman+bn}
Let $(a_{j})$ and $(b_{j})$ be sequences. If $\sum_{j=1}^\infty a_{j}$ and $\sum_{j=1}^\infty b_{j}$ are convergent, then so is $\sum_{j=1}^\infty (a_{j}+b_{j})$, and
\[
\sum_{j=1}^{\infty} (a_{j}+b_{j})=\sum_{j=1}^{\infty} a_{j}+\sum_{j=1}^{\infty} b_{j}.
\]
If $c\in\mathbb{R}$, then $\sum ca_{j}$ is convergent and
\[
\sum_{j=1}^{\infty} ca_{j}=c
\sum_{j=1}^{\infty} a_{j}.
\]

\end{proposition}

\begin{proof}
We observe that
\begin{align*}
\sum_{j=1}^{\infty} (a_{j}+b_{j}) & = \lim_{n\rightarrow\infty} \sum_{j=1}^{n}(a_{j}+b_{j})
& =\lim_{n\rightarrow\infty} \left(\sum_{j=1}^{n} a_{j}+\sum_{j=1}^{n}b_{j}\right)
=\lim_{n\rightarrow\infty} \sum_{j=1}^{n} a_{j}+\lim_{n\rightarrow\infty}\sum_{j=1}^{n}b_{j}\\
& =\sum_{j=1}^{\infty} a_{j}+\sum_{j=1}^{\infty}b_{j}.
\end{align*}
Similarly,
\[
\sum_{j=1}^{\infty} ca_{j}
=\lim_{n\rightarrow\infty}\sum_{j=1}^{n} ca_{j}
=c\lim_{n\rightarrow\infty}\sum_{j=1}^{n} a_{j}
=c\sum_{j=1}^{\infty} a_{j}.
\]
\end{proof}
\noindent
{\bf Point to ponder:} What about a rule for {\em products} of series?

\medskip
In Calculus and its Applications, you will learn about several tests for convergence of series. We will discuss only one here.

\begin{theorem}[Comparison Test]
If $b_j \geq 0$ for all $j$ and $\sum_{j=1}^\infty b_{j}$ is convergent, and if $|a_{j}|\leq b_{j}$ for all $j$, then $\sum_{j=1}^\infty a_{j}$ is also convergent, and
moreover $ |\sum_{j=1}^\infty a_j| \leq \sum_{j=1}^\infty b_j.$
\end{theorem}

\begin{proof}
The idea is to write $a_j = (a_j - b_j) + b_j$ and to use Proposition~\ref{p:suman+bn}. We know that $\sum_j b_j$ converges by hypothesis, and if we knew that $\sum_j(a_j - b_j)$ converged we would be in business.

Note first that, since $a_j \leq b_j$ for all $j$, we have $t_n \geq 0$ for all $n$, and moreover the sequence $(t_n)$ given by
\[
t_{n}=\sum_{j=1}^{n}(b_{j}-a_{j})
\]
is increasing. Next, note that $b_{j}-a_{j}\leq 2b_{j}$, and so
\[
t_{n}\leq \sum_{j=1}^{n}2b_{j}\leq \sum_{j=1}^{\infty}2b_{j}\]
and since this latter sum converges by Proposition \ref{p:suman+bn}, this means that $(t_{n})$ is bounded above. Thus, by the MCT, $(t_{n})$ converges
and $\limn t_n \geq 0$. Therefore, by Proposition~\ref{p:suman+bn}. $\sum_{j=1}^\infty(a_j - b_j)$
converges to some nonpositive number.
We conclude that $\sum_{j=1}^\infty a_{j}$ converges, and the sum is $\sum_{j=1}^\infty b_j + \sum_{j=1}^\infty (a_j - b_j)$, whose value is at most $\sum_{j=1}^\infty b_j$. Finally, applying the same reasoning with $-a_j$ in place of $a_j$, we conclude that
\[
|\sum_{j=1}^\infty a_j| \leq \sum_{j=1}^\infty b_j.
\]
\end{proof}

For example, since $\left|\frac{\sin j}{2^{j}}\right|\leq \frac{1}{2^{j}}$ and $\sum_{j=1}^\infty \frac{1}{2^{j}}$ converges by Example \ref{ex:geometric-series}, we can deduce that $\sum_{j=1}^\infty \frac{\sin j}{2^{j}}$ converges too.

\section{The number $e$}%
\label{numbere}
The number $e$, together with $0,1$ and $\pi$, is perhaps the most important of all the real numbers.
It arose in practical calculations in compound interest when one wants to calculate the interest on a ``continuous'' basis rather than on a discrete (annually, monthly, daily, hourly) basis. It is in this context that it arises as the limit of the sequence $(a_n)$, where
\[ a_n = \left(1 + \frac{1}{n}\right)^n.\]
In one of the exercises we shall establish that this sequence does indeed have a limit $L$ such that $2 < L < 4$.

It was also used as the base for natural logarithms. These were invented as early as 1618 by John Napier in the Merchiston area of Edinburgh.

Finally, it arises as the infinite sum
\[ e := \sum_{n=0}^\infty \frac{1}{n!}\]
which converges by the comparison test since for $n \geq 1$
\[
\frac{1}{n!}=\frac{1}{n\cdot (n-1)\cdots 2\cdot 1}\leq \frac{1}{2\cdot 2\cdots 2\cdot 1 } = \frac{1}{2^{n-1}},
\]
and the geometric series $\sum_{n=1}^{\infty}\frac{1}{2^{n-1}}$ converges. This is the definition of $e$ which we adopt officially in mathematics. In one of the exercises (quite hard!) we establish that
\[
\sum_{n=0}^\infty \frac{1}{n!} = \limn \left(1 + \frac{1}{n}\right)^n,\]
showing that the two definitions of $e$ which we have proposed are in fact the same.

What is not clear from either defintion of $e$ is whether it is rational or irrational: it is not clear whether $e$ has a repeating decimal expansion or not from its definition. In the Appendix below we establish that $e$ is in fact irrational.
In the Honours Analysis course you will study the more general exponential function which gives a rigorous framework for raising the number $e$ to an arbitrary (not just rational) real power $x$, and much else besides.

%
%\begin{theorem}[The $p$-test]
%The series $\sum \frac{1}{n^{p}}$ converges if and only if $p>1$.
%\end{theorem}
%
%\begin{proof}
%You may know a proof using the integral test, but we have not developed the theory of integrals in this course, so we cannot use them as justification. However, there is a simpler proof. First, suppose $p>1$, then
%\[
%\sum_{n=1}^{2^{N}-1}\frac{1}{n^{p}}
%=\sum_{j=0}^{N-1}\sum_{n=2^{j}}^{2^{j+1}-1}\frac{1}{n^{p}}
%\leq \sum_{j=0}^{N-1}\sum_{n=2^{j}}^{2^{j+1}-1}\frac{1}{(2^{j})^{p}}
%=  \sum_{j=0}^{N-1} \frac{2^{j+1}}{2^{jp}}
%=\sum_{j=0}^{N-1} 2\cdot (2^{1-p})^{j}
%\]
%and this converges since $2^{1-p}<1$ as $1-p<0$. If $s_{N}=\sum_{n=1}^{N}\frac{1}{n^{p}}$, we have now shown that $s_{2^{N}}$ converges. Since $s_{n}$ is increasing, $s_{n}$ now converges because of Proposition \ref{p:sub-monotone}.
%
%If $p\leq 1$, then
%\[
%\sum_{n=1}^{2^{N}-1}\frac{1}{n^{p}}
%=\sum_{j=0}^{N-1}\sum_{n=2^{j}}^{2^{j+1}-1}\frac{1}{n^{p}}
%\geq \sum_{j=0}^{N-1}\sum_{n=2^{j}}^{2^{j+1}-1}\frac{1}{(2^{j+1})^{p}}
%=  \sum_{j=0}^{N-1} \frac{2^{j+1}}{2^{(j+1)p}}
%=\sum_{j=0}^{N-1} 2^{1-p}\cdot (2^{1-p})^{j}
%\]
%which diverges by because $2^{1-p}\geq 1$ as $1-p\geq 1$, and so $\sum \frac{1}{n^{p}}$ diverges as well.
%\end{proof}
%

%
%
%\begin{definition}
%A series $\sum a_{n}$ is {\it absolutely convergent} if $\sum |a_{n}|$ is convergent.
%\end{definition}
%
%\begin{theorem}[Absolute convergence theorem (ACT)]
%If $\sum a_{n}$ is absolutely convergent, then it is convergent.
%\end{theorem}
%
%\begin{proof}
%Consider the series $\sum (|a_{n}|-a_{n})$. Then the terms of this series are nonnegative, and the partial sums are monotone increasing satisfy
%\[
%\sum_{n=1}^{N} (|a_{n}|-a_{n})
%\leq \sum_{n=1}^{N}|a_{n}|
%\leq \sum_{n=1}^{\infty} |a_{n}|<\infty,
%\]
%and so $s_{N}=\sum_{n=1}^{N} (|a_{n}|-a_{n})$ is a monotone increasing and bounded sequence, so it converges. Thus, by Proposition \ref{p:suman+bn}, the series $\sum (|a_{n}|-(|a_{n}|-a_{n}))=\sum a_{n}$ converges as well.
%\end{proof}
%
%
%
%\begin{example}
%The series $\sum \frac{(-1)^{n}}{n^2}$ is convergent because $\sum \left|\frac{(-1)^{n}}{n^2}\right|=\sum \frac{1}{n^2}$ is convergent by the $p$-test.
%\end{example}
%
%As an immediate corollary, we get the following useful test:
%
%\begin{theorem}[Comparison Test]
%If $|a_{n}|\leq b_{n}$ and $\sum b_{n}$ converges, then $\sum a_{n}$ converges.
%\end{theorem}
%
%\begin{proof}
%Note that the partial sum $\sum_{n=1}^{N}|a_{n}|$ is increasing since the terms are nonnegative, and so we just need to show it is bounded from above. But
%\[
%\sum_{n=1}^{N}|a_{n}|\leq \sum_{n=1}^{N}b_{n}\leq \sum_{n=1}^{\infty} b_{n}<\infty
%\]
%and so $\sum|a_{n}|$ converges, thus $\sum a_{n}$ is absolutely convergent.
%\end{proof}
%
%\begin{example}
%$\sum_{n=1}^{\infty} \frac{1}{n(n+1)}$ is convergent since $\frac{1}{n(n+1)}<\frac{1}{n^2}$ and $\sum_{n=1}^{\infty} \frac{1}{n^2}$ is convergent by the $p$-test.
%\end{example}
%
%\begin{example}
%If we consider $\sum_{n=2}^{\infty} \frac{1}{n(n-1)}$, we have to be a bit more careful using the comparison test, since $\frac{1}{n(n-1)}>\frac{1}{n^2}$. However, we can still try to bound it above by a multiple of $\frac{1}{n^2}$. Notice that for $n\geq 2$ that
%\[
%n-1\geq \frac{n}{2}.
%\]
%Hence,
%\[
%\frac{1}{n(n-1)}\leq \frac{1}{n\cdot \frac{n}{2}}=\frac{2}{n^2}
%\]
%and since $\sum_{n=1}^{\infty}\frac{2}{n^2}$ converges, $\sum_{n=2}^{\infty} \frac{1}{n(n-1)}$ by the comparison test.
%\end{example}
%
%
%
%Not all convergent series are absolutely convergent.
%
%\begin{example}
%Consider $s_{N}=\sum_{n=1}^{N}\frac{(-1)^{n}}{n}$. Then
%\[
%s_{2N}=\sum_{n=1}^{2N}\frac{(-1)^{n}}{n}
%=-1+\frac{1}{2}-\frac{1}{3}+\frac{1}{4}-\cdots -\frac{1}{2N-1}+\frac{1}{2N}
%=\sum_{n=1}^{N}\left(-\frac{1}{n}+\frac{1}{n+1}\right)
%=\sum_{n=1}^{N}\frac{1}{n(n+1)}
%\]
%We have already shown that $\lim_{N\rightarrow \infty} \sum_{n=1}^{N}\frac{1}{n(n+1)}$ converges in the previous example, so $\lim_{N\rightarrow\infty}s_{2N}$ exists. Moreover,
%\[
%\lim_{N\rightarrow\infty} s_{2N+1}=\lim_{N\rightarrow\infty} (s_{2N}-\frac{1}{2N+1})=\lim_{N\rightarrow\infty} s_{2N}+0,
%\]
%so $s_{2N+1}$ converges to the same limit.
%
%\end{example}
%
%
%Our study of series gives us another useful test for when a sequence converges:
%
%\begin{theorem}
%Suppose $(x_n)$ is a sequence so that $\sum |x_{n}-x_{n+1}|$ converges. Then $(x_n)$ converges.
%\end{theorem}
%
%\begin{proof}
%Since $\sum |x_{n}-x_{n+1}|$ converges, so does $\sum (x_{n}-x_{n+1})$, so
%\begin{align*}
%x_{n}
%& =x_{n}-x_1+x_1=(x_n-x_{n-1}) + (x_{n-1}-x_{n-2})+\cdots + (x_{1}-x_{1})+x_1\\
%& =\left(\sum_{k=1}^{n-1} (x_{k+1}-x_{k})\right) +x_{1}
%\end{align*}
%and since the sum converges, so does $x_n$.
%\end{proof}
%
%\begin{example}
%Suppose $x_1=1$ and $x_{n+1} = x_{n}  +(-1)^{n} \frac{x_{n}^{2}}{n^2}$. Show that $x_{n}$ converges.
%
%Note that because of the $(-1)^{n}$, the sequence is neither increasing or decreasing, so we can't use the MCT in a straightforward way. But let's look at the differences:
%\[
%|x_{n}-x_{n+1}|=\frac{x_{n}^{2}}{n^{2}}.
%\]
%If we show $x_{n}$ is bounded by some number $C$, say, then $|x_{n}-x_{n+1}|\leq \frac{C^{2}}{n^{2}}$ and the previous theorem tells us that $x_n$ converges since $\sum \frac{C^{2}}{n^{2}}$ converges.
%\end{example}
%
%
%
%We could show that it is alternate increasing-decreasing (that is, that $x_{1}\leq x_{3}\leq \cdots $ and $x_{2}\geq x_{4}\geq \cdots$ and then use the MCT to conclude these sequences and hence the whole s
%

%
%
%\subsection{Non-uniqueness of decimal representations}
%
%One drawback of using decimal numbers is that they don't give {\it unique} representations of real numbers, that is, a real number could have two distinct decimal expansions. For example,
%\[
%1=1.0000
%=.\overline{9}.
%\]
%
%%More generally,
%%
%%\[
%%0.\underbrace{00.......0}_{ k  \mbox{ zeros}} \overline{9} = 0.\underbrace{00.......0}_{k-1  \mbox{ zeros}} 1.\]
%
%
%
%It turns out that having a trail of 9's is the only way you can have non-unique expansions.
%
%
%\begin{theorem}
%If a real number $x$ has two distinct decimal expansions $a_{0}.a_{1}\cdots $ and $b_{0}.b_{1}\cdots $, then one of them must terminate in $0$'s and the other must terminate in $9$'s.
%\end{theorem}
%
%
%
%{\bf Note:} The proof given in Liebeck Proposition 3.3 is a little shorter, so you may want to read that proof first. If you'd like a bit more detail, you can read below.
%
%\begin{proof}
%First, let's assume $a_{0}=b_{0}=0$. We leave the general case as an exercise. Since the decimal expansions are distinct, there is an integer $k\geq 1$ so that $a_{k}\neq b_{k}$. Let $k$ be the smallest of these, so that
%\[
%a_{i}=b_{i} \;\; \mbox{ for }\;\; 1\leq i<k.\]
%We can also assume $a_{k}>b_{k}$ (since either this happens or $a_{k}<b_{k}$ and the proof will be similar). In particular, since $a_{k}$ and $b_{k}$ are integers, this means
%\[
%a_{k}\geq b_{k}+1.\]
%
%So now we will show that $a_{i}=0$ for all $i>k$ and $b_{i}=9$ for all $i>k$. Because $x=0.b_{1}b_{2}\cdots $  and $b_{j}\leq 9$ for all $j$,
%\begin{equation}
%x= 0.b_{1}...b_{k-1} b_{k}b_{k+1}...
%\leq 0.b_{1}...b_{k-1}b_{k}999...
%\leq 0.b_{1}...b_{k-1}(b_{k}+1)
%\label{e:x}
%\end{equation}
%In the last line, we used the fact that
%\[
%0.\underbrace{00.......0}_{ k  \mbox{ zeros}} \overline{9} = 0.\underbrace{00.......0}_{k-1  \mbox{ zeros}} 1\]
%so one gets added to $b_{k}$ term. Now recall that $a_{i}=b_{i}$ for $i<k$ and $b_{k}+1\leq a_{k}$. Applying this to the terms above, we get that $x\leq a_{1}...a_{k}$. Thus, now we know
%\[
%0.a_{1}\cdots \leq x\leq 0.a_{1}...a_{k}.
%\]
%The first equality is our main assumption (that $x$ has that decimal expansion) and the second is what we have just proved. If we subtract the last term from both ends of the inequality, we get
%\[
%0.{0...0}a_{k}a_{k+1}...\leq 0.
%\]
%Since the $a_i$ are nonnegative, this is only possible if $a_{i}=0$ for all $i\geq k$. Thus, $x=a_{0}.a_{1}\cdots ... a_{k}\bar{0}$. Similarly, from \eqref{e:x}, we have
%\[
%x=
%\leq 0.b_{1}...b_{k-1}b_{k}999...
% \leq 0.a_{1}...a_{k}=x=0.b_{1}\cdots b_{k-1}b_{k}b_{k+1}...
%\]
%and subtracting $0.b_{1}\cdots b_{k-1}b_{k}b_{k+1}$ from both ends of the inequality, we get
%\[
%0.b_{1}...b_{k-1}b_{k}(9-b_{k+1})(9-b_{k+2})...\leq 0.
%\]
%Since $b_{i}\leq 9$, $9-b_{i}\geq 0$, so the only way this is possible is if $b_{i}=9$ for all $i>k$.
%
%
%\end{proof}
%
%

\section{Exercises}%
\label{decimalsseriesexercises}

The relevant exercises in Liebeck are in Chapter 3.

\begin{exercise}\label{81}
Use the formula for a finite geometric series to show that for $ a, b \in \mathbb{R}$ and $n \in \mathbb{N}$ we have
\[ a^n - b^n = (a-b)(a^{n-1} + a^{n-2}b + \cdots + ab^{n-2} + b^{n-1}).\]
\end{exercise}

\begin{exercise}
Write each of the following decimal expansions in the form of a fraction $\frac{p}{q}$ with $p, q \in \mathbb{Z}$.
\begin{enumerate}[label=(\alph*)]
\item $0.\overline{123}$ .
\item $0.\overline{2}$ .
\item $0.\overline{2331}$ .
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove that if $a_{n},b_{n}\in \{0,1,...,9\}$ are so that $a_{n}+b_{n}\leq 9$ for all $n$, then
\[
0.a_{1}a_{2}\cdots + 0.b_{1}b_{2}\cdots = 0.(a_{1}+b_{1})(a_{2}+b_{2})...\]
\begin{solution}
We use Proposition \ref{p:suman+bn} and the definition of a decimal expansion to get
\begin{align*}
 0.a_{1}a_{2}\cdots + 0.b_{1}b_{2}\cdots
&  =\sum_{n=1}^{\infty}\frac{a_{n}}{10^{n}} + \sum_{n=1}^{\infty}\frac{b_{n}}{10^{n}} \\
& = \sum_{n=1}^{\infty}\left( \frac{a_{n}}{10^{n}} + \frac{b_{n}}{10^{n}} \right) \\
& = \sum_{n=1}^{\infty} \frac{a_{n}+b_{n}}{10^{n}}
=0.(a_{1}+b_{1})(a_{2}+b_{2})...
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{877}
Suppose that $0 \leq a_1 \leq 8$ and that $0.a_1 a_2 a_3\dots =
0.(a_1 +1) b_2 b_3 \dots$. Show that $a_j = 9$ and $b_j = 0$ for all $j \geq 2$.
\begin{solution}
By assumption, \[\frac{a_1}{10} + \sum_{j=2}^\infty \frac{a_j}{10^j} = \frac{a_1 + 1}{10} + \sum_{j=1}^\infty \frac{b_j}{10^j},\]
so that
\[ \sum_{j=2}^\infty \frac{a_j - b_j}{10^j} = \frac{1}{10}.\]
But if $a_j - b_j < 9$ for any $j$ we will have
\[ \sum_{j=2}^\infty \frac{a_j - b_j}{10^j} < \sum_{j=2}^\infty \frac{9}{10^j} = \frac{1}{10},\]
contradiction. So indeed $a_j = 9$ and $b_j = 0$ for all $j \geq 2$.

\end{solution}

\end{exercise}

\begin{exercise}
Suppose that $a_{n}\geq 0$ and the numbers $\sum_{j=1}^{n}a_{j}$ are bounded above.  Is $\sum_{j=1}^{\infty}a_{j}$ convergent?

\begin{solution}
Yes, note that $s_n=\sum_{j=1}^{n}a_{j}$ is an increasing sequence since
\[
s_{n+1}-s_{n}=\sum_{j=1}^{n+1}a_{j}-\sum_{j=1}^{n}a_{j} = a_{n+1}\geq 0.
\]
By assumption, $(s_{n})$ is bounded above, so it must converge by the MCT.
\end{solution}
\end{exercise}

%
%\begin{exercise} Show that $a_{n}\rightarrow L$ if and only if for all $k\in\mathbb{N}$ there is $N\in\mathbb{N}$ so that for all $n\geq N$, the first $k$ digits of the decimal expansions for $a_n$ and $a$ agree.
%
%\end{exercise}
%
%\begin{exercise}
% Show that $\sum_{n=1}^{N}\frac{1}{\sqrt{n}}-2\sqrt{N}$ converges as $N\rightarrow \infty$. Hint: Use monotone convergence theorem.
%\begin{solution}
%In the Week 2 workshop, we showed that $\sum_{n=1}^{N}\frac{1}{\sqrt{n}}\leq 2\sqrt{N}$ for all $N$, so $s_{N}=\sum_{n=1}^{N}\frac{1}{\sqrt{n}}-2\sqrt{N}$ is bounded above. Thus, it suffices to show that this sequence is increasing, since then it will converge by the MCT:
%\begin{align*}
%s_{N+1}-s_{N}
%& =\sum_{n=1}^{N+1}\frac{1}{\sqrt{n}}-2\sqrt{N+1}-\sum_{n=1}^{N}\frac{1}{\sqrt{n}}+2\sqrt{N}\\
%& = \frac{1}{\sqrt{N+1}}-2\sqrt{N+1}+2\sqrt{N}\\
%& =\frac{1}{\sqrt{N+1}}-2\left(\sqrt{N+1}-\sqrt{N}\right) \\
%& =\frac{1}{\sqrt{N+1}}-2\left(\sqrt{N+1}-\sqrt{N}\right)\frac{\sqrt{N+1}+\sqrt{N}}{\sqrt{N+1}+\sqrt{N}} \\
%& =\frac{1}{\sqrt{N+1}}-2\frac{(N+1)-N}{\sqrt{N+1}+\sqrt{N}} \\
%& =\frac{1}{\sqrt{N+1}}-2\frac{1}{\sqrt{N+1}+\sqrt{N}} \\
%\end{align*}
%
%\end{solution}
%\end{exercise}

%
%
%\begin{exercise}  Show that $\sum_{n=1}^{\infty}9^{-n^2}$ is irrational. (Hint: think about how we proved $e$ is irrational).
%
%\begin{solution}
%Let $N$ be an integer and $x=\sum_{n=1}^{\infty}9^{-n^2}$. Then
%\[
%x-\sum_{n=1}^{N}9^{-n^2}
%=\sum_{n=N+1}^{\infty}9^{-n^2}
%=9^{-(N+1)^2}\sum_{n=N+1}^{\infty} 9^{(N+1)^2-n^2}\]
%Note that this sum is of some negative powers of $9$, so it is at most the sum of {\it all} negative powers of $9$, and thus the above is
%\[
%\leq 9^{-(N+1)^2}\sum_{n=0}^{\infty} 9^{-n}
%= 9^{-(N+1)^{2}} \frac{1}{1-1/9}
%=9^{-(N+1)^{2}}\frac{9}{8}.
%\]
%
%Let $\frac{p}{9^{N^2}} = \sum_{n=1}^{N}9^{-n^2}$. Then,
%\[
%|x-\frac{p}{9^{N^2}}|
%=\sum_{n=N+1}^{\infty}9^{-n^2}
%\leq 9^{-(N+1)^{2}}\frac{9}{8}
%=9^{-N^2-2N-1}\frac{9}{8}
%=\frac{9^{-2N}}{9^{N^{2}}}
%\]
%and since $9^{-2N}\rightarrow 0$, we can pick $N$ so that this is at most $c$, so that $|x-\frac{p}{9^{N^2}}|<\frac{c}{9^{N^{2}}}$.
%\end{solution}
%
%\end{exercise}
%
%
\begin{exercise}\label{eleven}
In each case, either find a sequence $(a_n)$ such that $a_n \geq 0$ for all $n$ which has the desired property, or else explain why no such sequence exists. \\

(i) the sequence $(a_n)$ and the series $\sum_{n=1}^\infty a_n$ both converge\\

(ii) the sequence $(a_n)$ and the series $\sum_{n=1}^\infty a_n$ both fail to converge\\

(iii) the sequence $(a_n)$ converges and the series $\sum_{n=1}^\infty a_n$ fails to converge\\

(iv) the sequence $(a_n)$ fails to converge and the series $\sum_{n=1}^\infty a_n$ converges.\\

\begin{solution}
(i) $a_n = 2^{-n}$.

(ii) $a_n = 2^n$.

(iii) $a_n = 1/n$. Then $a_n \to 0$ but $\sum_{n=1}^\infty \frac{1}{n}$ diverges -- see his week's Lecture Quiz.

(iv) Not possible: by Proposition~\ref{divgcetest}, if $\sum_{n=1}^\infty a_n$ converges, then the sequence $(a_n)$ converges (in fact it converges to $0$).
\end{solution}

\end{exercise}

\begin{exercise}
If $\sum_n |a_n|$ converges, show that $\sum_n a_n$ converges. {\bf Harder:} Does the converse hold?
\begin{solution}
This follows directly from the comparison test. The converse is not true: take $a_n = \frac{(-1)^n}{n}$. The convergence of $\sum_{n=1}^\infty a_n$ for this case is treated in Calculus and its Applications.
\end{solution}
\end{exercise}

\begin{exercise} If $a_n \to 0$,
does $\sum_n a_n$ necessarily converge?
\begin{solution}
No: take $a_n = 1/n$.
\end{solution}
\end{exercise}

\begin{exercise}
Suppose that $|a_n - a_{n-1}| \leq 2^{-n}$ for all $n \in \mathbb{N}$. Show that the sequence $(a_n)$ converges. ({\bf Hint:} Consider the {\em series} $\sum_{n=1}^\infty (a_n - a_{n-1})$.)
\begin{solution}
By the comparison test and the geometric series, we have that $\sum_{n=1}^\infty (a_n - a_{n-1})$ converges.
But the partial sums for this series are
\[ s_n = \sum_{j=1}^n (a_j - a_{j-1}) = a_n - a_0.\]
Since $(s_n)$ converges, so does $(a_n)$.
\end{solution}
\end{exercise}

\begin{exercise}
 A fun fact you'll learn how to prove if you take the 4th year course {\it Fourier Analysis} is that
\[
\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^{2}}{6}.
\]
Given this fact, compute $\sum_{n=1}^{\infty}\frac{1}{(2n+1)^2}$.

\begin{solution}
Note that  for any integer $N$,
\begin{align*}
\sum_{n=1}^{2N+1}\frac{1}{n^{2}}
& =
\sum_{n=1}^{2N+1}\frac{1}{n^{2}}
=\frac{1}{1^{2}}+\frac{1}{2^{2}}+\frac{1}{3^{2}}+\cdots + \frac{1}{(2N+1)^2}\\
& =\left(\frac{1}{1^{2}}+\frac{1}{3^{2}}+\cdots + \frac{1}{(2N+1)^2}\right)
+\left(\frac{1}{2^{2}}+\frac{1}{4^{2}}+\cdots + \frac{1}{(2N)^2}\right)\\
& =\sum_{n=1}^{N}\frac{1}{(2n+1)^{2}}+ \sum_{n=1}^{N}\frac{1}{(2n)^{2}}
= \sum_{n=1}^{N}\frac{1}{(2n+1)^{2}}+\frac{1}{2^2} \sum_{n=1}^{N}\frac{1}{n^{2}}\\
\end{align*}
Now taking limits of both sides gives
\begin{align*}
\frac{\pi^{2}}{6} =
& \sum_{n=1}^{\infty}\frac{1}{n^2}
=\lim_{N\rightarrow\infty}\sum_{n=1}^{2N+1}\frac{1}{n^{2}}
 =\lim_{N\rightarrow\infty} \left(\sum_{n=1}^{N}\frac{1}{(2n+1)^{2}}+\frac{1}{2^2} \sum_{n=1}^{N}\frac{1}{n^{2}}\right) \\
& =\sum_{n=1}^{\infty}\frac{1}{(2n+1)^{2}}+ \frac{1}{2^2} \sum_{n=1}^{\infty}\frac{1}{n^{2}}
 = \sum_{n=1}^{\infty}\frac{1}{(2n+1)^{2}}+ \frac{1}{4} \cdot \frac{\pi^{2}}{6}
\end{align*}
and solving for the sum on the right gives
\[
\sum_{n=1}^{\infty}\frac{1}{(2n+1)^{2}}= \frac{\pi^{2}}{6} - \frac{1}{4} \cdot \frac{\pi^{2}}{6}=\frac{\pi^{2}}{8}.
\]

\end{solution}
\end{exercise}

%begin{exercise}
%Let $a_n = \left(1 + \frac{1}{n}\right)^n$.

%(i) Show that $a_{n+1} \geq a_n$ if and only if %$\left(1-\frac{1}{(n+1)^2}\right)^{n+1} \geq %\frac{n}{n+1}$.

%(ii) Use Bernoulli's inequality to show that $\left(1-\frac{1}{(n+1)^2}\right)^{n+1} \geq \frac{n}{n+1}$, and deduce that $(a_n)$ is increasing.

%(iii) Use the binomial theorem and finite geometric series to show that $a_n \leq 4$ for all $n$.

%(iv) Deduce that $(a_n)$ converges to some number $L$ satisfying $2 < L \leq 4$.

%\begin{solution}
%(i) We have
%\[ \left(1 + \frac{1}{n+1}\right)^{n+1} \geq \left(1 + \frac{1}{n}\right)^n\]
%if and only if
%\[ \left(\frac{(n+2)n}{(n+1)^2}\right)^{n+1} \geq %\frac{n}{n+1}
%\]
%if and only if
%\[\left(1-\frac{1}{(n+1)^2}\right)^{n+1} \geq \frac{n}{n+1}.\]

%(ii) By Bernoulli's inequality with $h = -1/(n+1)^2$ we %have \[\left(1-\frac{1}{(n+1)^2}\right)^{n+1} \geq 1 - %\frac{n+1}{(n+1)^2} = \frac{n}{n+1},\]
%and so $a_{n+1} \geq a_n$ for all $n$.

%(iii) By the binomial theorem,
%\[\left(1 + \frac{1}{n}\right)^n = \sum_{j=0}^n %{{n}\choose{j}} \frac{1}{n^j},\]
%and the definition of the binomial coefficients shows that
%\[{{n}\choose{j}} \frac{1}{n^j} \leq \frac{1}{j!} =\frac{1}{j\cdot (j-1)\cdots 2\cdot 1}\leq \frac{1}{2\cdot 2\cdots 2\cdot 1 } = \frac{1}{2^{j-1}}.\]
%Therefore
%\[\left(1 + \frac{1}{n}\right)^n \leq \sum_{j=0}^n \frac{1}{2^{j-1}} < 4.\]

%(iv) We conclude that $(a_n)$ is increasing and bounded above by $4$, and so by the MCT converges to a limit $L$ satisfying $ 2 < L \leq 4$.

%\end{solution}
%\end{exercise}

\begin{exercise}(Quite challenging.)
Let $a_n = \left(1 + \frac{1}{n}\right)^n$, and let $b_n = 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{n!}$. Show that $(a_n)$ and $(b_n)$ both converge to the same limit. (You may assume that $(a_n)$ is convergent.)

\begin{solution}
We have already seen in the previous exercise that
\[ \left(1 + \frac{1}{n}\right)^n \leq 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{n!}\]
so by the rules for limits we have
$\limn a_n \leq \limn b_n$. (Note that $\lim_n b_n$ exists by our discussion of the number $e$.) To show that $\limn b_n \leq \limn a_n$ we must control $1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{n!}$ by $\left(1 + \frac{1}{m}\right)^m$ for some $m$ (which will probably satisfy $m \gg n$). Indeed, note that for $m \geq n$ we have
\[ \left(1 + \frac{1}{m}\right)^m =
1 + 1 + \frac{m(m-1)}{2!m^2} + \frac{m(m-1)(m-2)}{3!m^3} + \dots
+\]
\[+ \dots
+ \frac{m(m-1) \dots (m-n+1)}{n! m^n} + \dots + \frac{m!}{m!m^m}
\]
\[ \geq 1 + 1 + \frac{1}{2!}\frac{m(m-1)}{m^2} + \frac{1}{3!}\frac{m(m-1)(m-2)}{m^3} + \dots
+ \frac{1}{n!}\frac{m(m-1) \dots (m-n+1)}{m^n}.
\]
The hope is that by taking $m$ sufficiently large we should be able to take all of the terms
\[
\frac{m(m-1)}{m^2}, \frac{m(m-1)(m-2)}{m^3}, \dots, \frac{m(m-1) \dots (m-n+1)}{m^n}
\]
very close to $1$.

Fix $n$ and fix $\epsilon>0$. Since
$\frac{m(m-1)}{m^2} \to 1$ as $m \to \infty$ there is an $M_1$ such that
for $m > M_1$ we have
\[ \frac{m(m-1)}{m^2} > 1- \epsilon.\]
Since
$ \frac{m(m-1)(m-2)}{m^3}\to 1$ as $m \to \infty$ there is an $M_2$ such that
for $m > M_2$ we have
\[ \frac{m(m-1)(m-2)}{m^3} > 1- \epsilon.\]
Continuing in the same way, we eventually obtain an $M_{n-1}$ such that for $m > M_{n-1}$ we have
\[\frac{m(m-1) \dots (m-n+1)}{m^n} > 1-\epsilon. \]
So for $m > M :=\max\{M_1, \dots, M_{n-1}\}$ we have
\[ \frac{m(m-1) \dots (m-j+1)}{m^j}
>1-\epsilon\]
for all $2\leq j \leq n$.
Hence, for $m >M$ we have
\[ \left(1 + \frac{1}{m}\right)^m \geq 1 + 1 +(1 - \epsilon)\left(
\frac{1}{2!} + \cdots + \frac{1}{n!}\right) \geq (1 - \epsilon)\left(1 + \frac{1}{1!} +
\frac{1}{2!} + \cdots + \frac{1}{n!}\right).\]
Therefore
\[ \lim_{m \to \infty} \left(1 + \frac{1}{m}\right)^m \geq (1 - \epsilon) \limn \left(1 + \frac{1}{1!} +
\frac{1}{2!} + \cdots + \frac{1}{n!} \right).
\]

\end{solution}
\end{exercise}

\begin{exercise}
Show that for every real number $x$ there is a unique integer $a_0 \in \mathbb{Z}$ such that $a_0 \leq x < a_0 +1$. (We sometimes call $a_0$ the {\em integer part} of $x$, or the ``floor'' or ``staircase'' of $x$, denoted by $a_0 = \lfloor x \rfloor$.) Deduce that every real number $x$ has a decimal expansion
$x = a_0.a_1 a_2 \dots a_n \dots$
where $a_0 \in \mathbb{Z}$ and $a_n \in \{0,1,2, \dots, 9\}$.
\begin{solution}
Let $x \in \mathbb{R}$. If $x \in \mathbb{Z}$ we simply take $a_0 = x$ and we are finished. So assume $x \notin \mathbb{Z}$. Let $A= \{ n \in \mathbb{Z} \; | \; n > x\}$. Then $A \neq \emptyset$ by the Archimedean property, and $x$ is a lower bound for $A$. Then $A$ has a minimum member $n_0$. Therefore $x < n_0$ and $ n_0 - 1 \leq x$. Take $a_0 = n_0 -1$. (Note that this argument works if and only if we are in the setting of an ordered field with the Archimedean property.)
%(because $x \neq n_0$ and if $x > n_0$, $x$ would be a lower bound greater than the GLB, contradiction). Moreover $x \geq n_0 -1$ (because if $x < n_0 -1$, then $n_0 -1$ would be in $A$, and so would have to be at least as big as the lower bound $n_0$, contradiction). Furthermore, $n_0 \in \mathbb{Z}$ (because if not, there will be a sequence $(x_n)$ with $x_n \in A \subseteq \mathbb{Z}$ for all $n$, such that $x_n \to a_0$, so $ x_n - x_{n-1} \to 0$, and this forces $x_n - x_{n-1}$ to be equal to zero for all sufficiently large $n$, i.e. $x_n$ is the constant $a_0$ for all sufficiently large $n$, and since $x_n \in \mathbb{Z}$ we must have $a_0 \in \mathbb{Z}$). [See also Workshop 3, Question 8.] Now take $a_0 = n_0 -1$.
The uniqueness of $a_0$ is easy.

(The fact that $A$ actually has a minimum member deserves more discussion. $A$ is nonempty and has some lower bound. By the Archimedean property we can assert that it has a lower bound in $\mathbb{Z}$. The well-ordering principle -- which states that a nonempty subset of $\mathbb{N}$ has a least member, and which follows from the principle of induction -- allows us to conclude that the minimum exists.)

\medskip
For the second assertion, let $a_{0}$ be the integer part of $x$. Then
$a_{0}\leq x<a_{0}+1$.
So $x\in [a_{0},a_{0}+1)$, i.e. $x - a_0 \in [0,1)$. Now apply Theorem~\ref{decimal_existence} to $x - a_0$.
\end{solution}
\end{exercise}

\begin{exercise}

\end{exercise} Formulate and prove results analogous to Theorems~\ref{83} and \ref{87} for arbitrary $x \in \mathbb{R}$.
\pagebreak

\section{*Appendix: irrationality of $e$}%
\label{eisirrational}
This section is optional, and is here to give you a taste of {\it analytic number theory}.

\medskip
In addition to Corollary~\ref{88}, there is another method for proving irrationality which involves this rather simple-looking lemma.

\begin{lemma}\label{irrattest}
Let $x \in \R$. Suppose that for every $c>0$ there is a rational number $\frac{p}{q}\neq x$ so that
\begin{equation}
\label{e:x-p/q}
\left|x-\frac{p}{q}\right| < \frac{c}{q}.
\end{equation}
Then $x$ is irrational.
\end{lemma}

\begin{proof}
We prove this using the contrapositive. Suppose $x$ is rational. Then $x=\frac{a}{b}$ for some integers $a$ and $b$ with $b >0$. Hence if $\frac{p}{q}$ is any other rational number not equal to $x$,
\[
\left|x-\frac{p}{q}\right|
=\left|\frac{a}{b}-\frac{p}{q}\right|
=\left|\frac{aq-pb}{bq}\right|
=\frac{|aq-pb|}{bq}.
\]
Note that  $aq-pb$ is an integer. Moreover $aq-pb\neq0$, since otherwise we would have $ x = p/q$.
Hence, $|aq-pb|\geq 1$, and if we set
\[
c=\frac{1}{b},
\]
then we have that for any rational number $\frac{p}{q}$,
\[
\left|x-\frac{p}{q}\right| \geq \frac{|aq-pb|}{bq}\geq \frac{1}{bq}=\frac{c}{q}.
\]
We have shown that, supposing $x$ is rational, there exists a $c>0$ such that for all rational numbers $\frac{p}{q}$ we have $\left|x-\frac{p}{q}\right| \geq \frac{c}{q}$ and we are done.
\end{proof}

As a corollary, we get the following:

\begin{corollary}
Suppose that $p_n, q_n \in \Z, q_n \neq 0$, and %$x=\lim_{n\rightarrow\infty}\frac{p_{n}}{q_{n}}$ and
$\left|x-\frac{p_{n}}{q_{n}}\right|q_n\rightarrow 0$. Then $x$ is irrational.
\end{corollary}

\begin{proof}
Let $c>0$. By assumption,
there is an $N$ so that $n> N$ implies
\[
\left|x-\frac{p_{n}}{q_{n}}\right|q_n<c \]
or equivalently
\[\left|x-\frac{p_{n}}{q_{n}}\right| < \frac{c}{q_n}.
\]
Thus, for all $c>0$, we can find $\frac{p_n}{q_n}$ rational so that \eqref{e:x-p/q} holds. Thus $x$ is irrational.
\end{proof}

\begin{theorem}
The number $e$ is irrational.
\end{theorem}

\begin{proof}
Let $x = e-1$. We aim to show that for every $c>0$, we can find a rational number $\frac{p}{q}$ so that \eqref{e:x-p/q} holds. Lemma~\ref{irrattest} will then imply that $e-1$, and hence $e$, is irrational.

So let $c>0$. Let $N\in\mathbb{N}$. Then
\[
x=\sum_{n=1}^{\infty} \frac{1}{n!}
=\sum_{n=1}^{N}\frac{1}{n!} + \sum_{n=N+1}^{\infty}\frac{1}{n!}
\]
Note that
\begin{align*}
\sum_{n=1}^{N}\frac{1}{n!}
& =1+\frac{1}{2!}+\frac{1}{3!}+\cdots + \frac{1}{N!}\\
& = \frac{N!}{N!}+\frac{N!/2!}{N!}+\frac{N!/3!}{N!}+\cdots + \frac{1}{N!}\\
& = \frac{N!+\frac{N!}{2!}+\frac{N!}{3!}+\cdots + 1}{N!} = \frac{p}{N!}
\end{align*}
where $p$ is an integer since $N!/k!$ is an integer for $k=0,1,...,N$. We will now show, for $N$ large enough, that
\[
\left| x-\frac{p}{N!}\right|<\frac{c}{N!}.\]
We know exactly what $x-\frac{p}{N!}$ is:

\[
x-\frac{p}{N!}
=x-\sum_{n=1}^{N}\frac{1}{n!}
=\sum_{n=N+1}^{\infty}\frac{1}{n!}
=\frac{1}{(N+1)!}\sum_{n=N+1}^{\infty}\frac{(N+1)!}{n!}
\]
If we can show that there is some constant $C$ such that \[\sum_{n=N+1}^{\infty}\frac{(N+1)!}{n!} \leq C\]
for all $N$, we are done, since then
\[
\left|x-\frac{p}{N!}\right|
=\frac{1}{(N+1)!}\sum_{n=N+1}^{\infty}\frac{(N+1)!}{n!}
\leq \frac{C}{(N+1)!}=\frac{C/(N+1)}{N!}
\]
and so if $\frac{C}{N+1}<c$ (which happens if we pick $N>\frac{C}{c}-1$), the above gives $\left|x-\frac{p}{N!}\right|<\frac{c}{N!}$.

\medskip
For the missing step, we have that for $n \geq N+1$,
\[ \frac{(N+1)!}{n!} =
\frac{1}{{n\choose N+1}}
\frac{1}{(n-N-1)!}
\leq \frac{1}{(n-N-1)!}
\]
since the binomial coefficients are all nonnegative integers, and we have already seen that
\[ \sum_{n = N+1}^\infty \frac{1}{(n-N-1)!} = \sum_{n=0}^\infty\frac{1}{n!}\]
converges.

To recap, for every $c>0$, we have shown that we can pick $N$ so that $\left|x-\frac{p}{N!}\right|<\frac{c}{N!}$, which implies that $e$ is irrational by Lemma~\ref{irrattest}.
\end{proof}

\section{Subsequences}%
\label{subsequences}

Consider the sequence $x_n=(-1)^{n}$, the simplest example of a bounded divergent sequence. Note that, while this sequence doesn't approach one value as $n\rightarrow\infty$, it concentrates on two values, and if you throw out all the odd numbered terms of the sequence, you get the new sequence $x_{2n}=(-1)^{2n}=1$, which does converge. This was quite a simple sequence, but it turns out this is more than just coincidence: for any bounded sequence, we can throw out some terms so that the sequence becomes convergent. Below we make more precise what we mean by ``throwing out'' terms.

\begin{definition}
Given a sequence $(x_{n})$, a {\it subsequence} is a sequence of the form $(x_{n_{k}})$ where $n_{1}<n_{2}<\cdots $ are positive integers. A real number $x$ is a {\it limit point} of a sequence $(x_{n})$ if there is a subsequence $(x_{n_{k}})$ so that $x_{n_{k}}\rightarrow x$.
\end{definition}

\begin{exercise}
Show that if $n_{1}<n_{2}<\cdots $ is a strictly increasing sequence of positive integers, then $n_k\geq k$ for all $k\in\mathbb{N}$.
\end{exercise}

The following gives a criterion for detecting limit points:

\begin{proposition}
\label{p:lim-points}
Given a sequence $(x_n)$ and a number $L$, $L$ is a limit point if and only if for all $\epsilon>0$ and for all integers $N$, we can find $n> N$ so that $|x_n-L|<\epsilon$.
\end{proposition}

\begin{proof}
If $L$ is a limit point, we will show  that for all $\epsilon>0$ and for all integers $N$, we can find $n> N$ so that $|x_n-L|<\epsilon$. Let $\epsilon>0$ and $N\in\mathbb{N}$. Since $L$ is a limit point, there is a subsequence $(x_{n_{k}})$ converging to $L$ with $n_{1}<n_{2}<\cdots$, so there is $K\in\mathbb{N}$ so that $k> K$ implies $|x_{n_{k}}-L|<\epsilon$. By the previous exercise, $n_{k}\geq  k$, so if $k> \max\{N,K\}$, then  $n_k$ is an integer bigger than  $N$ so that $|x_{n_{k}}-L|<\epsilon$. This proves the forward implication of the proposition.

Now we prove the converse. Assume $L$ is a number so that for all $\epsilon>0$ and for all integers $N$, we can find $n> N$ so that $|x_n-L|<\epsilon$. We will show $L$ is a limit point. \\

By our assumption (with $\epsilon=1$ and $N=1$), we can find $n_1\in\mathbb{N}$ so that
\[
|x_{n_{1}}-L|<1.
\]
By our assumption again (now with $\epsilon=\frac{1}{2}$ and $N=n_{1}+1$) we can find $n_2>n_1$ so that
\[
|x_{n_{2}}-L|<\frac{1}{2}.
\]
Now suppose we have picked $n_{k-1}$ for some integer $k\geq 2$. Using the same logic, we can find $n_{k}>n_{k-1}$ so that
\begin{equation}
\label{e:xn-L<1/k}
|x_{n_{k}}-L|<\frac{1}{k}.
\end{equation}
If we continue in this way, we get a sequence of integers $n_1<n_2<\cdots $ so that \eqref{e:xn-L<1/k} holds for all $k$. This means $x_{n_k}\rightarrow L$: For $\epsilon>0$, if $N=\frac{1}{\epsilon}$, then for $k> N$,
\[
|x_{n_{k}}-L|<\frac{1}{k}< \frac{1}{N}=\epsilon,
\]
which implies $x_{n_k}\rightarrow L$.
\end{proof}

\begin{example}
We have already seen that $x_n=(-1)^{n}$ has a subsequence $x_{2k}=1$ that converges to $1$. Also, $x_{2k+1}=(-1)^{2k+1}=-1$ is a subsequence that converges to $-1$. We claim that $\{-1,1\}$ are the only limit points:

Suppose $L$ is a limit point not equal to $-1$ or $1$. Then for all $n\geq 1$, $|x_{n}-L|=|(-1)^{n}-L|$ is either $|1-L|$ or $|-1-L|=|1+L|$, hence
\[
|x_{n}-L|\geq \min\{|1-L|,|1+L|\}.
\]
Let $\epsilon=\min\{|1-L|,|1+L|\}$ and $N=1$.Note $\epsilon>0$ since $L\neq \pm 1$. Then $\epsilon>0$ and $N$ so that for all $n\geq N$, $|x_{n}-L|\geq \epsilon$. This is the negation of Proposition \ref{p:lim-points}, and so $L$ is not a limit point. Thus, the only limit points are $\pm 1$.

%
%Suppose there is a subsequence $(x_{n_{k}})$ that converges to some number $L$. Thus, for all $\epsilon>0$, there is $N$ so that $k\geq N$ implies $|x_{n_{k}}-L|<\epsilon$. Note that $x_{n_{k}}$ equals $-1$ or $1$, but if $\epsilon$ is at most than half the distance between $1$ and $-1$ (so $\epsilon=1$, say), we can't have $x_{n_{k}}=1$  and $x_{n_{\ell}}=-1$ for some $k,\ell\geq N$, because otherwise the distance between $x_{n_{k}}$ and $x_{n_{\ell}}$ is
%\[
%|x_{n_{k}}-x_{n_{\ell}}|=|1-(-1)|=2,
%\]
%whereas since $|x_{n_{k}}-L|<\epsilon=1$ and $|x_{n_{\ell}}-L|<1$ (since $k,\ell\geq N$), we have
%\[
%2=|x_{n_{k}}-x_{n_{\ell}}|
%=|x_{n_{k}}-L+L-x_{n_{\ell}}|
%\leq |x_{n_{k}}-L|+|L-x_{n_{\ell}}| <1+1=2
%\]
%which is a contradiction. Thus, either $x_{n_{k}}=1$ for all $k\geq N$ or $x_{n_{k}}=-1$ for all $k\geq N$, which implies $\lim_{k\rightarrow\infty} x_{n_{k}}=1$ or $\lim_{k\rightarrow\infty} x_{n_{k}}=-1$.
\end{example}

Our next convergence theorem says that, while a sequence does not need to converge, if it is {\it bounded} then we can always find a convergent {\it subsequence}, no matter how wild the original sequence is.

%\begin{lemma}
%\label{l:x<y-lim}
%If $(x_{n})$ and $(y_{n})$ are both convergent sequences and $x_{n}\leq y_{n}$ for all $n\in\mathbb{N}$, then $\lim_{n\rightarrow\infty} x_{n}\leq \lim_{n\rightarrow\infty} x_{n}$. In particular, if $M$ is a number so that $x_{n}\leq M$ for all $n$, then $\lim_{n\rightarrow\infty} x_{n}\leq M$.
%\end{lemma}

\def\putgrid{\put(0,0){0}
\put(0,25){25}
\put(0,50){50}
\put(0,75){75}
\put(0,100){100}
\put(0,125){125}
\put(0,150){150}
\put(0,175){175}
\put(0,200){200}
\put(25,0){25}
\put(50,0){50}
\put(75,0){75}
\put(100,0){100}
\put(125,0){125}
\put(150,0){150}
\put(175,0){175}
\put(200,0){200}
\put(225,0){225}
\put(250,0){250}
\put(275,0){275}
\put(300,0){300}
\put(325,0){325}
\put(350,0){350}
\put(375,0){375}
\put(400,0){400}
{\color{gray}\multiput(0,0)(25,0){16}{\line(0,1){200}}}
{\color{gray}\multiput(0,0)(0,25){8}{\line(1,0){400}}}
}

\begin{theorem}[Bolzano–Weierstrass Theorem]
If $(x_{n})$ is a bounded sequence, then it has a limit point, that is, it has a convergent subsequence.
\end{theorem}

\begin{center}
\begin{figure}[h]
\includegraphics[width=420pt]{Figures/subsequence.pdf}
\begin{picture}(0,0)(420,0)
%\putgrid
\put(5,75){$x_{n}$}
\put(196,30){$n$}
\put(246,20){\color{red} $n_{1}$}
\put(265,20){\color{red} $n_{2}$}
\put(280,20){\color{red} $n_{3}$}
\put(305,20){\color{red} $n_{4}$}
\put(346,20){\color{red} $n_{5}$}
\put(380,20){\color{red} $n_{6}$}
\end{picture}
\caption{Above we have the graph of some bounded sequence, where the values $1,2,...$ are marked on the $x$ axis and the values $x_{1},x_{2},...$ are indicated by the dots. This sequence doesn't have any discernible pattern and is not converging. However, by the Bolzano–Weierstrass Theorem there is a subsequence $(x_{n_{k}})$ (denoted by the red points above the values $n_{1},n_{2},...$) that does converge.}
\end{figure}
\end{center}

\def\LUB{{\rm LUB}}
\begin{proof}
Let
\[
S=\{x \; | \; \mbox{ there are infinitely many $n\in\mathbb{N}$ so that }x_n\geq x\}\]
and set $L=\LUB (S)$. Since $(x_n)$ is bounded, $\LUB(S)$ exists. By Proposition \ref{p:lim-points}, $L$ is a limit point if we can verify the following:

{\bf Claim:} For all $\epsilon>0$ and $N\in\mathbb{N}$ there is $n>N$ so that $|x_{n}-L|<\epsilon$.

We prove by contradiction. Suppose instead that there exists $\epsilon>0$ and $N\in\mathbb{N}$ so that  $|x_{n}-L|\geq \epsilon$ for all $n> N$. In other words, there are at most $N$ many integers $n$ (hence only finitely many) for which $|x_{n}-L|<\epsilon$, or equivalently
\begin{equation}
\label{e:finitely-many-betweenL-eL+e}
L-\epsilon<x_n<L+\epsilon.
\end{equation}
Since $L+\epsilon>L=\LUB(S)$ and $\LUB(S)$ is an upper bound for $S$, we can't have $L+\epsilon\in S$, so by the definition of $S$, there are only finitely many $n\in\mathbb{N}$ so that
\begin{equation}
\label{e:finitely-many-aboveL+e}
x_n\geq L+\epsilon
\end{equation}
Thus, there are only finitely many $n$ satisfying \eqref{e:finitely-many-betweenL-eL+e} or \eqref{e:finitely-many-aboveL+e}, so there can only be finitely many $n$ so that $x_n>L-\epsilon$. But then
$L-\epsilon\geq \LUB(S)$, since if $L-\epsilon<\LUB(S)$, by definition of the LUB, $L-\epsilon$ is not an upper bound for $S$, so there is $t>L-\epsilon$ so that there are infinitely many $x_n\geq t$, which is impossible since there are only finitely many $x_n>\epsilon$. Thus, $L-\epsilon\geq \LUB(S)=L$, which is a contradiction. This proves the claim, and thus $L$ is a limit point.
\end{proof}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 5: Complex Numbers and Polynomials}

\chapterimage{Figures/blank.png}
\chapter{Complex Numbers}%
\label{complexnumbers}

%
%
%\begin{multicols}{2}
%\epigraph{\it     To divide 10 in two parts, the product of which is 40....It is clear that this case is impossible. Nevertheless, we shall work thus...
%%: We divide 10 into two equal parts, making each 5. These we square, making 25. Subtract 40, if you will, from the 25 thus produced, as I showed you in the chapter on operations in the sixth book leaving a remainder of -15, the square root of which added to or subtracted from 5 gives parts the product of which is 40. These will be $5 + \sqrt{- 15}$ and $5 - \sqrt{-15}$.
%Dismissing mental tortures, and multiplying $5 + \sqrt{- 15}$ by $5 - \sqrt{-15}$, we obtain $25 - (-15)$. Therefore the product is $40$...and thus far does arithmetical subtlety go, of which this, the extreme, is, as I have said, so subtle that it is useless.}{Cardan, {\it Ars Magna}, 1545}
%
%%\epigraph{\it For any equation one can imagine as many roots [as its degree would suggest], but in many cases no quantity exists which corresponds  to  what  one  imagines.}{Rene Descartes, {\it Discours de la M\'{e}thode Pour bien conduire sa raison, et chercher la v\'{e}rit\'{e} dans les sciences}, 1637}
%\end{multicols}
%
%
%The first of the above quotes comes from the inception of complex numbers. Here, Cardan is trying to find two numbers $x$ and $y$ so that $x+y=10$ but $xy=40$. This ends up being possible if you allow for taking square roots of negative numbers, since then $x=5+\sqrt{-15}$ and $y=5-\sqrt{-15}$ solves these equations. Mathematicians like Cardan considered these clever sophisms, but it was in this way that complex numbers come about. Rene Descartes called these numbers {\it imaginary}. It turns out that they aren't just sophisms, but they are fundamental to mathematics and physics. \\

%At this time, Italian mathematicians like Cardan and Tartaglia were researching how to solve polynomial equations. (In this day, there were even contests for solving cubic equations, for which Tartaglia achieved his fame).

%Given $z=a+bi \in \mathbb{C}$, with $a,b \in \mathbb{R}$ we define
%\[ \mathrm{Re}(z)=a \qquad \mathrm{Im}(z)=b \]
%to be the real and imaginary parts of $z$ respectively. A complex number written in this way (as $z=x+iy$) is said to be in {\em Cartesian form}.
%
%We can think of $(\mathrm{Re}(z),\mathrm{Im}(z))$ as a point on a plane using Cartesian coordinates.

\section{Complex Numbers}%
\label{complexnumberssection}

Complex numbers are an extension of the real numbers, defined by introducing a new special number called $i$ whose square is defined to be $-1$.

\begin{definition}[Complex Numbers]

Define $i$ to be a number such that $i^2=-1$.

The {\it complex numbers} are any number of the form
\[z=x+iy\mbox{ where }x,y \in \mathbb{R}.\]

The set of all complex numbers is denoted $\mathbb{C}$.

We define $\mathrm{Re}(z)=x$ and $\mathrm{Im}(z)=y$ to be the {\it real} and {\it imaginary parts} of $z$.
\end{definition}

Just like vectors, we represent complex numbers on an $xy$-plane, where the $x$-coordinate is the real part and the $y$ coordinate is the imaginary part. Using the plane to represent complex numbers in this way is sometimes called an {\it Argand diagram}.

\begin{center}
\includegraphics[width=150pt]{Figures/cartesian.pdf}
\begin{picture}(0,0)(150,0)
\put(-5,60){$iy$}
\put(80,5){$x$}
\put(85,65){$z=x+iy$}
\end{picture}
\end{center}

Multiplication, addition and subtraction of complex numbers maintains how addition works for real numbers, and treating \(i\) as an algebraic letter. In multiplication we proceed using an obvious approach:
consider any two complex numbers $z=a+i b$ and $w=c+i d$.
\begin{eqnarray*}
zw&=&(a+i b)(c+i d)\\
&=&ac+ai d+i
bc+i^2bd
\end{eqnarray*}
This is obtained in the usual way by multiplying all the
terms in one bracket by all the terms in the other
bracket. Use the defining property of \(i\), which is
$i^2=-1$ so that
\begin{eqnarray*}
zw&=&ac+ai d+i bc-bd\\
&=&ac-bd+i(ad+bc)
\end{eqnarray*}
where we have re-grouped terms with the  $i$ symbol and terms
without the $i$ symbol separately. These are the real and imaginary parts of the product $zw$ respectively.

For example:
\[
(5+6i) + (3+4i)=(5+3)+(6+4)i= 8+10i\]
and
\[
(1+3i)(2+i)=1\cdot 2 + 1\cdot i + 3i\cdot 2 + 3i\cdot i = 2+i+6i-3=-1+7i.
\]
For an integer $n$, we write  $z^{n}=z\cdot z\cdots z$ as we did with real numbers.

Complex addition can be interpreted as vector addition in the complex plane.
Consider any two complex numbers $z=a+i b$ and $w=c+i d$.
\[ z+x=(a+ib)+(c+id)=(a+c)+i(b+d).\]
This is identical to adding vectors
\[ \begin{pmatrix} a \\ b \end{pmatrix} + \begin{pmatrix} c \\ d \end{pmatrix}
= \begin{pmatrix} a+c \\ b+d \end{pmatrix} \]
What effect does multiplication have?

\begin{example}
Plot each of these complex numbers in the plane.
\begin{center}
\begin{tabular}{llll}
1. $1$    & 2. $i$     & 3. $-1$    & 4.  $-i$ \\
5. $4+3i$ & 6. $-3+4i$ & 7. $-3-4i$ & 8. $4-3i$\\
9. $a+ib$ & 10. $-b+ia$ & 11. $-a-ib$ & 12. $b-ia$
\end{tabular}
\end{center}
where you may choose any non-zero $a$ and $b$ you think are suitable.\\
(i) Multiply each of them by $i$ and plot the result on the same diagram.\\
(ii) Describe the geometric relationship between $z$ and $iz$
\end{example}
We will examine the geometry of complex multiplication fully in a short while.

Given $z\in\mathbb{C}$ not equal to zero, we let $z^{-1}=\frac{1}{z}$ denote the complex number such that $z\cdot z^{-1}=1$. Such a number (i) exists and (ii) is unique (a theorem we omit to save space).
In particular, we can divide complex numbers by each other, but before we explain how to do this, there are a couple of important quantities related to a complex number that will make this task easier.

%What if we want to do something more complicating, like $\frac{3+i}{1+i}$? How do we write it as $a+ib$?

The first is the {\it modulus}:

\begin{definition}[Modulus]
\begin{multicols}{2}
Given a complex number $z=x+iy$, the {\it modulus} of $z$ is
\[
|z|=|x+iy|=\sqrt{x^{2}+y^{2}}.
\]

Geometrically, the modulus of $z$ is its distance from the origin, which is the length of the hypotenuse of a triangle of base $x$ and height $y$, computed using the Pythagorean theorem.

\includegraphics[width=150pt]{Figures/modulus.pdf}
\begin{picture}(0,0)(150,0)
\put(-5,60){$iy$}
\put(80,5){$x$}
\put(86,65){$z=x+iy$}
\put(25,35){$|z|$}
\end{picture}

\end{multicols}

\end{definition}

This extends the definition of the absolute value to complex numbers, and in fact, if $x$ is real, the modulus of $x$ is equal to the absolute value.
This definition should remind you of the vector norm: for a vector $(x,y)$, its norm was $||(x,y)|| = \sqrt{x^2+y^2}$ as well. \\

A second important quantity is the conjugate.

\begin{definition}[Complex Conjugate]
\begin{multicols}{2}
Given a complex number $z=x+iy$, its {\em complex conjugate} is defined to be

\[\overline{z}=x-yi.\]
Geometrically, this is the complex number obtained by reflecting $z$  across the `$x$-axis'. \\

\includegraphics[width=150pt]{Figures/conjugate.pdf}
\begin{picture}(0,0)(150,0)
\put(80,0){$\bar{z}=x-iy$}
\put(80,125){$z=x+iy$}
\end{picture}
\end{multicols}
\end{definition}

Note: many books and resources use \(z^*\) as the complex conjugate, so please get used to using this notation.  But choose only one notation in your own writing and stick to it.

One useful thing about a conjugate is
\[z\overline{z} = (a+bi)(a-bi) = a^2 + b^2 = |z|^2.\]

We can use these concepts to show how to divide two complex numbers.  E.g.~let's look at $\frac{3+i}{1+i}$: we can make the denominator real by multiplying and dividing by the conjugate of $1+i$ (which is $\overline{1+i}=1-i$):

$$\frac{3+i}{1+i}=\frac{3+i}{1+i}\frac{1-i}{1-i} = \frac{(3+i)(1-i)}{(1+i)(1-i)} = \frac{5-2i}{2}=\frac{5}{2}-i.
$$

In general, for $w,z\in\mathbb{C}$ with $z\neq 0$, we have

\begin{equation}
\label{e:1/z}
 \frac{w}{z} = \frac{w}{z}\frac{\overline z}{\overline z} = \frac{w\overline z}{z\overline z} = \frac{w\overline z}{|z|^2}, \;\; \;\;\;\;\;\frac{1}{z} = \frac{\overline{z}}{|z|^2}
 \end{equation}

Here are some other properties that we will use throughout the chapter.

\begin{lemma}
\label{l:modulus-rules}
For all $u,v\in \mathbb{C}$:
\begin{itemize}
\item $\overline{u+v} = \overline{u}+\overline{v}$.
\item $\overline{uv} = \overline{u}\cdot\overline{v}$.
\item $|uv|=|u|\cdot|v|$.
\end{itemize}
\end{lemma}

We leave the proof as an exercise.\\

Note (a).  \(a^2+b^2\) is ``the difference of two squares''
\[ a^2+b^2 = a^2-(ib)^2 = (a+ib)(a-ib).\]
We can factor expressions over the complex numbers which have no factors over the reals, e.g. \(2=1^2+1^2=(1+i)(1-i)\).

Note (b). The process for finding \(z^{-1}\) is very similar to removing surds from the denominator of a rational expression.
E.g. compare multiplying \(\frac{1}{1+i}\) by the conjugate, \[ \frac{1}{1+i} = \frac{1}{1+i}\frac{1-i}{1-i}= \frac{1-i}{2}\]
with
\[ \frac{1}{\sqrt{3}+1} = \frac{1}{\sqrt{3}+1}\frac{\sqrt{3}-1}{\sqrt{3}-1}= \frac{\sqrt{3}-1}{2} .\]

\begin{solution}

\begin{proof}
These each follow by direct computation.  Let us denote $u=a+bi$, $v=c+di$, for $a,b,c,d\in\mathbb{R}$.
\begin{itemize}
\item We have:
$$\overline{u+v} = \overline{(a+c) + (b+d)i} = a+c - (b+d)i = (a-bi) + (c-di) = \overline{u} + \overline{v}.$$
\item We have:
\begin{align*}
\overline{u\cdot v}
& = \overline{(a+bi)(c+di)}\\
&  = \overline{(ac-bd) + (bc+ad)i} \\
& = ac-bd - (bc+ad)i \\
& = (a-bi)(c-di) \\
& = \overline{u}\cdot\overline{v}.
\end{align*}

\item We have:
$$|uv|^2 = (uv)(\overline{u}\overline{v}) = u\overline{u}v\overline{v} = |u|^2\cdot|v|^2,$$
where we have used (b) and (c) above.  Since both sides of the final equality are non-negatives, the inequality holds if, and only if, it holds after taking square roots (as we showed in Chapter 5, on inequalities)
\end{itemize}
Hence, we have established each of the four equalities.\end{proof}
\end{solution}

\section{Cartesian and Polar form}%
\label{cartesianandpolar}

Another useful representation of a complex number \(z=x+iy\) is {\it polar form}.  In polar form we represent a complex number by specifying an angle $\theta$ from the $x$ axis, and a distance from the origin which, recall, is $|z|$.

\begin{definition}
\begin{multicols}{2}
Let $z\neq 0$ be a complex number. Let $r = |z|$ and define the {\em argument} of $z$ be the angle $\theta\in [0,2\pi)$ between the line from $0$ to $z$ and the positive $x$-axis.
We can then write
$$z= r(\cos\theta+i\sin\theta).$$

This is the {\it polar form} of $z$.

\includegraphics[width=150pt]{Figures/modulus.pdf}
\begin{picture}(0,0)(150,0)
\put(87,65){$z$}
\put(35,40){$r$}
\put(35,20){$\theta$}
\put(85,30){$r\sin \theta$}
\put(25,0){$r\cos\theta$}
\end{picture}

\end{multicols}
\end{definition}
If we look at the triangle in the above figure, the hypotenuse is $r=|z|$, and so the base and height of the triangle are $r\cos \theta$ and $r\sin\theta$ respectively, and so these give the real and imaginary parts (i.e. the $x$ and $y$ coordinates) of $z$. \\

While we specify that the argument $\theta$ of a complex number $z$ to be in the range $[0,2\pi)$, there are infinitely many numbers $\phi$ so that $z= r(\cos\phi+i\sin\phi)$ (where $r=|z|$): we can just take $\phi = \theta+ 2n\pi$ for $n\in\mathbb{Z}$.
We have made a choice to only take $\theta\in [0,2\pi)$.  This is called the {\em principal argument}.
Other books and resources sometimes choose $\theta\in (-\pi,\pi]$.
Mathematicians are free to ``cut'' the complex plane where we choose, but be clear about which choice you make and stick to it.
For PPS only take $\theta\in [0,2\pi)$, unless otherwise stated clearly.

\section{Geometry and arithmetic in \(\mathbb{C}\)}%
\label{geometryandarithmeticofC}

There are at least three ways to treat complex numbers
\begin{enumerate}
  \item As a single number $z$,
  \item As a sum of {\em real} and {\em imaginary parts} $z=a+ib$,
  \item Using geometry, as a point in the {\em complex plane}.
\end{enumerate}
Algebra is applied to geometry by representing points in the plane using coordinates.
In Cartesian coordinates a point has coordinates $(a,b)$, so \(z=a+ib\).
In polar coordinates we have {\em modulus} $r$ (i.e.~length) and {\em argument} $\theta$ (i.e.~angle anti-clockwise from the $x$-axis, or real axis).
We can also think of complex numbers as {\em vectors} from the origin.
\begin{quote}
    {\em One of the most powerful techniques in mathematics is to describe something independently in two different forms and equate them.}
\end{quote}
Complex numbers have many representations and we will exploit this regularly.

Think about the geometric effect of multiplying \(z=x+iy\) by \(i\).
\[ i(x+iy)=-y+ix\]
What is the geometric transformation that multiplication by \(i\) creates?
In vector notation this transformation would be
\[ \begin{pmatrix} x \\ y \end{pmatrix} \rightarrow \begin{pmatrix} -y \\ x \end{pmatrix}\]
which is an anti-clockwise rotation by \(\frac{\pi}{2}=90^o\).

More generally multiplication of complex numbers can be understood by writing the two numbers in polar form.
%
Let $z=r(\cos \theta + i \sin \theta)$ and $w=s(\cos \phi + i \sin \phi)$ then $zw$
\begin{align*}
zw & = r(\cos \theta + i \sin \theta)\times  s(\cos \phi + i \sin \phi) \\
& = rs ( \cos \theta \cos \phi - \sin \theta \sin \phi)+ rsi (\cos \theta \sin \phi + \sin \theta \cos \phi) \\
                                                                                               & = rs (\cos(\theta + \phi) + i\sin(\theta + \phi)).
\end{align*}
Notice the overall geometric effect is to just multiply the moduli and add the arguments.

\begin{example}
Since $i=cos(\pi/2)+i\sin(pi/2)$, multiplying $z$ by $i$ corresponds to rotating counter clockwise by $\frac{\pi}{2}$, and multiplying by $-1=cos(\pi)+i\sin(pi)$ corresponds to rotating $z$ 180 degrees, i.e. flipping $z$ in the opposite direction.

\begin{center}
\includegraphics[width=300pt]{Figures/polar2.pdf}
\begin{picture}(0,0)(300,0)
\put(135,70){$z$}
\put(5,90){$iz$}
\put(170,15){$-z$}
\put(280,90){$z$}
\end{picture}
\end{center}
\end{example}

Given \(z^n\) is just \(z\) multiplied together \(n\) times we can prove De Moivre's Theorem.

\begin{theorem}[De Moivre's Theorem]
If we let $z = r(\cos \theta + i \sin \theta)$, and $n \in \mathbb{N}$ then
\begin{align*}
z^n & = r^{n} (\cos n\theta + i \sin n\theta) \\
z^{-n} &= r^{-n} (\cos (-n\theta) + i \sin (-n\theta)) = \frac{1}{r^n} (\cos (n\theta) - i \sin (n\theta))
\end{align*}
\end{theorem}

\begin{proof}
We prove the first statement by induction. Let \(P(n)\) be the statement
\[ z^n = r^{n} (\cos (n\theta) + i \sin (n\theta)).\]

The base case $n=1$ holds immediately by definition.

Assume \(P(n)\) holds for some integer $n\geq 1$.
\[ z^{n+1} = zz^n = z\times r^{n} (\cos (n\theta) + i \sin (n\theta))\]
using the induction hypothesis.  And so this
\[ =r(\cos \theta + i \sin \theta) \times r^{n} (\cos (n\theta) + i \sin (n\theta))
\]
\[ = r^{n+1} ( \cos \theta\cos (n\theta) - \sin \theta\sin (n\theta)
+i(\cos \theta\sin (n\theta) + \sin \theta\cos (n\theta)))
\]
\[ = r^{n+1} (\cos ((n+1)\theta) + i \sin ((n+1)\theta)),\]
which proves \(P(n+1)\) holds.

Since \(P(1)\) and \(P(n)\rightarrow P(n+1)\) it follows that the first equation in the theorem holds by the principle of mathematical induction.

For the second part, we just note that by \eqref{e:1/z},
\[
\frac{1}{\cos n \theta +i\sin n \theta} =\cos n \theta - i\sin n \theta,\]
and so
\[
z^{-n}=(z^{n})^{-1} = (r^{n}(\cos n\theta + i\sin n\theta))^{-1} = r^{-n} (\cos n \theta - i\sin n \theta) = r^{-n} e^{-i n\theta}.\]

\end{proof}

\section{Exponential form}%
\label{exponentialform}

De Moivre's Theorem allows us to calculate powers \(z^n\) for \(n\in\mathbb{N}\), but what about more a general exponential such as \(e^z\) when \(z\in\mathbb{C}\). Euler's formula provides the answer.
\begin{theorem}[Euler's formula]
\[e^{i \theta} = \cos(\theta)+i\sin(\theta).\]
\end{theorem}
Euler's formula is one of the most remarkable and important formulae in mathematics.  E.g. the special case \(e^{i\pi}=-1\) links together \(i\) (algebra), \(\pi\) (geometry) and \(e\) (calculus, since \(\frac{\mathrm{d}}{\mathrm{d}x}e^x=e^x\)) in a single equation.
Euler's formula justifies the definition of {\em exponential form} of a complex number \(z\) as.
\begin{equation}
\label{e:expform}
z= re^{i\theta} = r(\cos \theta + i \sin \theta).
\end{equation}
Exponential notation is a very convenient form for writing complex numbers.

There are many approaches to proving Euler's formula.
If you are studying Calculus and Taylor Series, is to show that $e^{i\theta} = \cos \theta + i \sin \theta$ by plugging $i\theta$ into the Taylor series for $e^{x}$ and then you can split the series into $\cos \theta + i\sin\theta$. We omit this working here, but it is provided as part of the online lecture quiz.

\begin{center}
\includegraphics[width=\textwidth]{Figures/polar.pdf}
\begin{picture}(0,0)(200,0)
\put(0,0){(a) Multiplying $z$ by $e^{i\phi}$.}
\put(45,50){$z=re^{i\theta}$}
\put(20,80){$e^{i\phi}z=re^{i(\theta+\phi)}$}
\put(20,30){$\theta$}
\put(165,30){$\theta$}
\put(311,30){$\theta$}
\put(20,55){$\phi$}
\put(190,50){$z=re^{i\theta}$}
\put(230,80){$sz=sre^{i\theta}$}
\put(150,0){(b) Multiplying $z$ by $s>0$.}
\put(300,0){(c) Multiplying $z$ by $se^{i\phi}$.}
\put(335,50){$z=re^{i\theta}$}
\put(375,80){$sz=sre^{i\theta}$}
\put(325,125){$se^{i\phi}z=sre^{i(\theta+\phi)}$}
\end{picture}
\end{center}

Exponential notation makes taking a power of a complex number much easier if we know it's polar form.

\begin{example}
What is $(1+i)^{6}$?\\

We could multiply this out by using the Binomial Theorem, but we'll use polar coordinates instead: To write $1+i$ in polar form $re^{i\theta}$, we first find $r$:
\[
r=|1+i|=\sqrt{1^2+1^2}=\sqrt{2}.
\]
Then
\[
e^{i\theta} = \frac{1+i}{r} =\frac{1+i}{\sqrt{2}} = \frac{1}{\sqrt{2}}+i\frac{1}{\sqrt{2}}=\cos \frac{\pi}{4} + i\sin\frac{\pi}{4}=e^{i\frac{\pi}{4}}.
\]
Finally,
\[
(1+i)^{6}= (\sqrt{2})^{6} e^{i\frac{6\pi}{4}} = 8\left(\cos \frac{6\pi}{4}+i\sin \frac{6\pi}{4}\right) =8\left(\cos \frac{3\pi}{2}+i\sin\frac{3\pi}{2}\right)= -8i.
\]
\end{example}

\section{Roots of Unity}%
\label{rootsof1}

\begin{center}
{\it How do we find all solutions to $z^{n}=1$?
}
\end{center}
We now have enough tools in place to answer this. We find them all in a few steps:

\begin{itemize}
\item First, using polar coordinates, we can find one root rather easily: $w=e^{\frac{2\pi i}{n}}$, since then
\[
w^{n} = e^{\frac{2\pi i}{n}\cdot n}=e^{2\pi i}=1.\]
\item In particular, this means that any power of $w$ is also a root, since if $j\in\mathbb{N}$,
\[
(w^{j} )^{n} = (w^{n})^{j}=1.
\]
\item Finally, we will show later that for any degree $n$ polynomial there are at most $n$ distinct roots. So if we show that the numbers
\[
1,w,w^{2},...,w^{n-1}
\]
are all distinct, then we will have all the roots. \\

\item Suppose for the sake of a contradiction that there are distinct integers $j$ and $k$ with $0\leq j,k<n$ so that $w^{j}=w^{k}$. Then
\[
1=w^{j-k}=e^{(j-k)\frac{2\pi i}{n}} = \cos \left((j-k)\frac{2\pi }{n}\right)+i\sin \left((j-k)\frac{2\pi }{n}\right).\]
The only way this can be $1$ is if the cosine is 1 and the sine is zero, so which only happens if their arguments are multiples of $2\pi$, that is, we must have
\[
(j-k)\frac{2\pi }{n} = 2\pi \ell \mbox{ for some integer }\ell\]
which implies $j-k=n\ell$ for some $\ell\in\mathbb{Z}$, but this is impossible since $0\leq j,k<n$, and hence $-n<j-k<n$, so $j-k$ must be zero, and so $j$ and $k$ cannot be distinct. Thus, the numbers $1,w,...,w^{n-1}$ are distinct and form all the roots.
\end{itemize}

We have thus shown the following.

\begin{theorem}[Roots of Unity]
The solutions to $z^{n}=1$ are $1,w,\cdots w^{n-1}$ where $w=e^{\frac{2\pi i}{n}}$. That is, they are $e^{\frac{2\pi k i}{n}}$ for $k=0,1,...,n-1$.
\end{theorem}

\begin{example}[The third roots of unity]
\hspace{5pt}

The third roots of unity are
\begin{multicols}{2}
\begin{align*}
z_1=&1\\
z_2=& e^{2\pi i/3} =  \cos \frac{2\pi}{3} + i\sin \frac{2\pi}{3}  = -\frac{1}{2}+i\frac{\sqrt{3}}{2} \\
z_3= & e^{4\pi i/3} = \cos \frac{4\pi}{3} + i\sin \frac{4\pi}{3}  = -\frac{1}{2}-i\frac{\sqrt{3}}{2}
\end{align*}
\vspace{10pt}
\begin{center}
\includegraphics[width=120pt]{Figures/3rdroots.pdf}
\begin{picture}(0,0)(120,0)
\put(20,115){$e^{\frac{2\pi i}{3}}$}
\put(5,5){$e^{\frac{4\pi i}{3}}$}
\put(120,55){$1$}
\end{picture}
\end{center}
\end{multicols}

Note: the cube roots of unity are used so frequently that writers often define \(\omega :=  e^{2\pi i/3}\).   Then we have \(\omega^2=\bar{\omega}\), and lots of other interesting and useful algebraic relationships.

\end{example}

\begin{center}
{\it How do we find all solutions to $
z^{n}=a$ where $a\in\mathbb{C}$?}
\end{center}

Again, we can use $n$th roots of unity. We showcase the method in an example:

\begin{example}

Suppose we set $a=16i$ and wish to find the fourth roots of $a$ (i.e., solve $z^4=16i$). We can find one quick root using polar coordinates: note that $a=16e^{i\frac{\pi}{2}}$, so we can spot one root as
\[
z=16^{1/4}e^{i\frac{\pi}{2}\frac{1}{4}} = 2e^{i\frac{\pi}{8}},\]
since then $z^{4} = a$.

Recall that there are at most $4$ distinct roots for a degree $4$ polynomial. Note that since $z$ is a solution, if $1,w,w^2,w^3$ (that is, $1,e^{i2pi/4}=e^{i\pi/2}, e^{i4\pi/4}=e^{i\pi}$, and $e^{i6\pi/4}=e^{i3\pi/2}$) are the $4$th roots of unity, then for $k=0,1,2,3$,
\[
(zw^{k})^{4}=z^{4}w^{4k}=a\cdot 1=a.
\]
Thus, the other solutions are $z,zw,zw^2,zw^3$. For $k \in \{0,1,2,3\}$ this gives the following (in exponential form)
\vspace{-1mm}
\[ 2e^{i\frac{\pi}{8}},2e^{i\frac{5\pi}{8}},2e^{i\frac{9\pi}{8}},2e^{i\frac{13\pi}{8}}\]
\vspace{-1mm}
or equivalently
\vspace{-1mm}
\[ 2e^{i\frac{\pi}{8}},2e^{i\frac{5\pi}{8}},2e^{i\frac{-7\pi}{8}},2e^{i\frac{-3\pi}{8}}\]
\end{example}

\section{Exercises}%
\label{complexnumbersexercises}

The relevant exercises in Liebeck's book are in Chapter 6.

\begin{exercise} Show that $\Re(z) = \frac{z+\bar{z}}{2}$ and $\Im(z) = \frac{z-\bar{z}}{2i}$.

\begin{solution}
If $z=x+iy$, then
\[
\frac{z+\bar{z}}{2} = \frac{z+iy+ z-iy}{2} = \frac{2x}{2} = x=\Re(z)
\]
and
\[
\frac{z-\bar{z}}{2i} = \frac{z+iy-( z-iy)}{2i} = \frac{2iy}{2i} = y=\Im(z).
\]

\end{solution}

\end{exercise}

\begin{exercise} Show that
\[
\Re(zw)\leq \frac{|z|^2+|w|^2}{2}.
\]
\begin{solution}

By Lemma \ref{l:modulus-rules},  and the AM-GM inequality,
\[
\Re(zw)
\leq |zw|
=|z|\cdot |w| =(|z|^2\cdot |w|^2)^{\frac{1}{2}}\leq \frac{|z|^2+|w|^2}{2}.
\]
\end{solution}

\end{exercise}

%
%\begin{exercise}  Below you see a graph of the complex plane and some points representing some complex numbers (the outer and inner circles have radii $1$ and $1/2$ for scale). They lie on lines through the origin making a 45 degree angle with the x-axis.
%
%
%\begin{multicols}{2}
%Draw the points in the plane if we plug these points into the functions
%\begin{itemize}
%\item $\left(\sqrt{2}+\sqrt{2}i\right)z$
%\item $z^2$
%\item $\frac{1}{z}$.
%\end{itemize}
%
%\begin{center}
%\includegraphics[width=100pt]{Figures/complex-diagram.pdf}
%\end{center}
%\end{multicols}
%
%
%\begin{solution}
%The portraits of (a), (b) and (c) are as follows:
%
%\begin{center}
%\includegraphics[width=300pt]{Figures/complex-soln.pdf}
%\end{center}
%\end{solution}
%%\vspace{1cm}
%
%
%\end{exercise}

\begin{exercise} Solve $z^2=i\overline{z}$.

\begin{solution}
Notice that if this equation holds, then
\[
|z|^2=|i\overline{z}|=|i|\cdot |\overline{z}| = |z|,\]
so either $|z|=0$ or $|z|=1$. In the latter case, this means that $\overline{z} = \frac{1}{z}$, and so
\[
z^2=i\overline{z} = \frac{i}{z}\]
which implies
\[
z^3=i.
\]
Hence, we just need to solve this equation now. Since $i=e^{\frac{\pi}{2}i}$, then $e^{\frac{\pi}{6}i}$ is one solution. Thus, to get all 3 solutions, we multiply this by all the 3rd roots of unity, so we get
\[
e^{\frac{\pi}{6}i}, \;\; e^{\frac{5\pi}{6}i}, \;\; e^{\frac{3\pi}{2}i}.
\]
Thus, all solutions to the original equation are
\[
0, \;\; \pm \frac{\sqrt{3}}{2}+\frac{i}{2}, \;\; -i.
\]

\end{solution}

\end{exercise}

\begin{exercise} Solve $|z|^2 - z|z| + z = 0$.

\begin{solution}
Note that by rearranging the equations so that the $z's$ are on one side and $|z|'s$ are on the other, we get if $|z|\neq 1$ that
\[
z  =\frac{|z|^{2}}{|z|-1},\]
and so $z\in\R$. If $z\geq 0$, then our original equation becomes $z^2-z^2+z=0$, hence $z=0$. If $z<0$, then $|z|=-z$, and our original equation is
\[
z^2+z^2+z=0\]
So $0=z(2z+1)$, hence $z=0$ or $z=-\frac{1}{2}$.

If $|z|=1$, then the original equation is
\[
1-z+z=0,\]
which is impossible, so there are no solutions in this case. Thus, $z=0,-\frac{1}{2}$ are the only solutions.
\end{solution}

\end{exercise}

\begin{exercise} Show that if $|z| = 1$, then $\Re\frac{1}{1-z} =
\frac{1}{2}$.

\begin{solution}
\[ \frac{1}{1-z} = \frac{1}{1-z}\frac{1+\bar{z}}{1+\bar{z}} =
\frac{1+\bar{z}}{1-z+\bar{z} + z\bar{z}}\]
\[ =
\frac{1+\bar{z}}{1-2iy-|z|^{2}}=\frac{1+\bar{z}}{-2iy}=i\frac{1+\bar{z}}{2y}\]
\[\Re \frac{1}{1-z} = \Re i\frac{1+\bar{z}}{2y} = \Re
i\frac{1+x-iy}{2y} = \Re\left(\frac{y}{2y} + i\frac{1+x}{2y}\right) =
\frac{y}{2y}\]
\[=\frac{1}{2}\]
\end{solution}

\end{exercise}

\begin{exercise} Describe geometrically the points $z\in\mathbb{C}$ so that $|z-1|=|z+i|$.

\begin{solution}
These are all points that lie on the $x=y$ line in the $xy$-plane (so the line that makes a 45 degree angle with the axes. To see this, let $z=x+i$ and observe that
\begin{align*}
|z-1|=|z+i| & \;\;\; \Longleftrightarrow \;\;\; |z-1|^2=|z+i|^2\\
& \;\;\; \Longleftrightarrow \;\;\; (z-1)\overline{(z-1)} = (z+i)\overline{(z+i)} \\
& \;\;\; \Longleftrightarrow \;\;\;  (z-1)(\bar{z}-1)=(z+i)(\bar{z}-i) \\
& \;\;\; \Longleftrightarrow \;\;\; z\bar{z}-z-\bar{z}+1 = z\bar{z} -zi+i\bar{z}+1\\
& \;\;\; \Longleftrightarrow \;\;\;  -z-\bar{z}= -zi+i\bar{z}\\
& \;\;\; \Longleftrightarrow \;\;\;  -(x+iy)-(x-iy) = -(x+iy)i+i(x-iy) \\
& \;\;\; \Longleftrightarrow \;\;\;  -2x = -2y\\
& \;\;\; \Longleftrightarrow \;\;\;  x=y.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} Find all solutions to $(z+1)^4=z^4$.

\begin{solution}
First, we write this equation as $(1+1/z)^4=1$, so this implies that $1+1/z$ is one of the $4$th roots of unity $\pm1,\pm i$. However, there are no solutions to $1+1/z=1$, so the only solutions are when $1+1/z$ is $-1$ or $\pm i$, in which case
\[
z=-\frac{1}{2}, \frac{1}{-1\pm i}.
\]
\end{solution}

\end{exercise}

\begin{exercise} Let $\mathbb{D}=\{z\in \mathbb{C}: |z|<1\}$, that is, the set of complex numbers with modulus strictly less than $1$. \(\mathbb{D}\) is for ``disc''. For a complex number $z\in \mathbb{D}$ and a real number $-1< r<1$, define\footnote{The function $f_{r}$ is called a {\it M\"obius transformation} and is usually defined for $r\in \mathbb{D}$ as well, not just $-1<r<1$. They are the only functions with the property that, if $A$ is a circle or straight line in $\mathbb{D}$, then $f_{r}(A)$ is also a line or circle (this is not part of the problem, it's just cool).}
\[
f_{r}(z) = \frac{z-r}{1-zr}.\]
 Show $f_{r}(z)\in \mathbb{D}$ for all $z\in \mathbb{D}$ and $-1<r<1$.

\begin{solution}
If $-1<r<1$, then
\begin{align*}
\left|\frac{z-r}{1-rz}\right|<1
& \Leftrightarrow \;\;|z-r|<|1-rz|\\
&  \Leftrightarrow \;\; |z-r|^{2}<|1-rz|^{2}\\
& \Leftrightarrow \;\; |z|^{2}+|r|^{2}-2Re(zr)<1+|rz|^{2}-2Re(zr)\\
& \Leftrightarrow \;\; |z|^{2}+|r|^{2}-|rz|^{2}<1\\
& \Leftrightarrow \;\; |z|^{2}(1-|r|^{2})+|r|^{2}<1\\
\end{align*}

But this inequality is true since $|z|<1$, so
\[
|z|^{2}(1-|r|^{2})+|r|^{2}<1\cdot (1-|r|^{2})+|r|^{2}=1.
\]

\end{solution}

\end{exercise}

\chapter{Polynomials}%
\label{polynomials}
%
%
%\epigraph{\it  The mathematicians have been very much absorbed with finding the general solution of algebraic equations, and several of them have tried to prove the impossibility of it. However, if I am not mistaken, they have not as yet succeeded. I therefore dare hope that the mathematicians will receive this memoir with good will, for its purpose is to fill this gap in the theory of algebraic equations.}{Niels Henrik Abel, 1824, having shown there are no formulas for roots to polynomials degree $5$ and higher.}
%
%

\section{Introduction}%
\label{introtopolynomials}

\begin{definition}
For $n\in\mathbb{N}$, an {\it $n$-degree complex polynomial} \(p\) is a function of the form
\begin{equation}
\label{e:pz}
p(z)=a_nz^n + a_{n-1}z^{n-1} + \cdots + a_0
\end{equation}
where $a_n \neq 0$ and $a_i \in \mathbb{C}$ for all $i$.

\(n\) is called the {\em degree} of the polynomial.

A {\it root} of $p$ is a complex number  $\alpha\in\mathbb{C}$ such that $p(\alpha)=0$.
\end{definition}

If the degree of a polynomial is \(2\) then the polynomial is called a {\em quadratic}.   Any quadratic $az^2+bz+c$ has two roots which can be found using the familiar {\it quadratic formula}:
\[\frac{-b\pm \sqrt{b^2-4ac}}{2a}.\]

{\bf Note:} One needs to be careful here, since we are allowing $a,b,c\in\mathbb{C}$, so  $b^2-4ac$ could be complex. In this case, $\sqrt{b^2-4ac}$ is interpreted to mean any number $z$ so that $z^2=b^2-4ac$, and then the solutions are $\frac{-b\pm z}{2a}$.

There are also formulae for the roots of cubic or quartic (i.e. degree 3 or 4) polynomials, although these are much less convenient to write down. It is natural to ask whether there are convenient formulae for higher order polynomials, but this is not the case:

\begin{theorem}[Abel-Ruffini Theorem] There is no formula (like the quadratic formula) for the roots of a polynomial of degree $\geq 5$.
\end{theorem}

This doesn't mean degree $5$ polynomials don't have roots, we will see below that they must have at least one root. This also doesn't mean it is impossible to solve higher order polynomials for their roots, it just means we do not have an expression (i.e. formula) for the roots of the polynomial in terms of the coefficients.

\section{Factoring Polynomials}%
\label{factoringpolynomials}

The following theorem is stated without proof in PPS (although we prove it later in the degree):

\begin{theorem}[Fundamental Theorem of Algebra]
Any complex polynomial has at least one root in $\mathbb{C}$.
\end{theorem}

Using this, we can prove that any polynomial can be factored completely.

\begin{theorem}[Factorization Theorem]
If $p$ is a degree $n$ polynomial, then there are $n$ roots $r_{1},...,r_{n}\in\mathbb{C}$ and a number $a\in\mathbb{C}$ so that
\[
p(z) = a(z-r_{1})(z-r_{2})\cdots (z-r_{n}).\]
\end{theorem}
In the above theorem some roots may repeat. If a root appears $m$ times in $r_{1},...,r_{n}$, then we say it has {\it multiplicity} $m$.

Notice that if we multiply out the above polynomial, then we get a polynomial like $p(z)=az^{n}+\cdots $, so if $p$ was as in \eqref{e:pz}, then we actually know that $a=a_n$.

The above theorem tells us that an $n$-degree polynomial always has $n$ roots if we also take into account the multiplicity of the roots (that is, how often it appears in the factorization above). For example, $(z-1)^2$ is a $2$-degree polynomial with only one root, but that root has multiplicity $2$.

\begin{proof}
We prove this by induction on \(n\), the degree of the polynomial \(p\).
Let \(P(n)\) be the statement
\begin{quote}
A degree $n$ polynomial \(p\) has $n$ roots $r_{1},...,r_{n}\in\mathbb{C}$ and there is a number $a\in\mathbb{C}$ so that
\[
p(z) = a(z-r_{1})(z-r_{2})\cdots (z-r_{n}).\]
\end{quote}

{\bf Base Case:} Assume \(p\) is any degree \(1\) polynomial.
If we write \(p\) as $p(z)=az+b$, we can re-write $p(z) = a(z-\frac{-b}{a})$.
Then \(p\) has precisely one root \(r_1=\frac{-b}{a}\) and \(p\) can be written in the form \(p(z)=a(z-r_1)\). This proves \(P(1)\).

{\bf Induction Step:}
Suppose the theorem holds true for some integer $n\geq 1$, i.e. \(P(n)\) is true.
Let $p$ be a degree $n+1$ polynomial.
By the Fundamental Theorem of Algebra there is a root $w$ of $p$, i.e.~\(p(w)=0\).

Consider the polynomial $q(z):=p(z+w)$.
Then \(q\) is also degree \(n+1\) and has a root at $0$.
Write \(q\) out as a polynomial then because it has a root at \(z=0\) the constant term is also zero and we can write
\[
q(z) = q_{1}z+\cdots + q_{n+1}z^{n+1}
= z\underbrace{(q_{1}+q_{2}z+\cdots + q_{n+1}z^{n})}_{=g(z)}.
\]
where \(g\) is a polynomial of degree \(n\).
By our induction hypothesis \(P(n)\), there are $a,z_{1},...,z_{n}\in\mathbb{C}$ so that
\[
g(z) = a(z-z_{1})(z-z_{2})\cdots (z-z_{n}).
\]
Note if \(q(z):=p(z+w)\) then
\[
p(z)=q(z-w)=(z-w)g(z-w)
\]
and so if \(r_{i}:=z_{i}+w\) for \(i=1,\cdots, n\) and \(r_{n+1}=w\)
then
\[
p(z) = a(z-w-z_{1})(z-w-z_{2})\cdots (z-w-z_{n})(z-w)
=a(z-r_{1})(z-r_{2})\cdots (z-r_{n+1}).
\]
This proves \(p\) has $n+1$ roots and there is a number $a\in\mathbb{C}$ so that we can write
\[
p(z) = a(z-r_{1})(z-r_{2})\cdots (z-r_{n+1}).\]

Since \(P(1)\) is true, and \(P(n)\Rightarrow P(n+1)\) it follows that \(P(n)\) is true for all \(n\in\mathbb{N}\) by the principle of mathematical induction.
\end{proof}

\section{Real Polynomials}%
\label{realpolynomials}

If we are given a {\it real polynomial}, by which we mean a polynomial $p(z)=a_{0}+\cdots + a_{n}z^{n}$ such that each $a_i \in \mathbb{R}$, then we have a bit more information about the roots:

\begin{theorem}[Real Polynomials have conjugate roots] If $p(x)$ has {\it real} coefficients and $r$ is a root, so is $\bar{r}$.
\end{theorem}

\begin{proof}
If $p(x)=a_0+a_{1}x+\cdots +a_{n}x^n$ with $a_{i}$ real, and $p(r)=0$, then
\begin{align*}
0 =\overline{p(r)}
 & =\overline{a_0+a_{1}r+\cdots +a_{n}r^n} \\
{\color{magenta} (\overline{z+w}=\bar{z}+\bar{w})} & = \overline{a_0}+\overline{a_1 r}+\cdots + \overline{ a_n r^n} \\
{\color{magenta} (\overline{zw}=\bar{z}\bar{w})} & =\overline{a_0}+\bar{a_1} \bar{ r}+\cdots + \bar{ a_n}\bar{ r^n} \\
{\color{magenta} (\overline{a_{i}}=a_{i}\;\; \mbox{ since }\;\;a_{i} \;\; \mbox{are real})} &  ={a_0}+{a_1} \bar{ r}+\cdots + { a_n}\bar{ r}^n = p(\bar{r}).
\end{align*}

\end{proof}

%Here we've used that conjugation commutes with both sums and products, and that real numbers (the $a_i$) are invariant under conjugation.

\noindent {\bf Note:} If any of the coeffients is not a real number then all bets are off! That is, we won't be able to factor into conclude that conjugates of roots are also roots. \\

\begin{example}
Find the roots of $x^4+2x^3-7x^2+2x-8$ given that one of them is $i$. \\

Recall that the roots come in conjugate pairs, and so $-i$ is also a root, hence $(x-i)(x+i)=x^2+1$ is a factor in the above polynomial, so we can do polynomial long division to see how it factors: first, since the leading term of the above polynomial is $x^4$, we subtract a multiple of $x^2+1$ that will eliminate the $x^4$, so we subtract $x^2(x^2+1)$ from the polynomial to get
\[
2x^3-8x^2+2x-8\]
Now the leading term is $2x^3$, so we remove $2x(x^2+1)$ from this to get
\[
-8x^2-8=-8(x^2+1).
\]
Finally, we can eliminate this term by subtracting $-8(x^2+1)$. Thus, adding together all the multiples of $(x^2+1)$ we subtracted, we get that
Thus, we see that
\[
x^4+2x^3-7x^2+2x-8=(x^2+1)(x^2+2x-8)
\]
Now we just need to solve $x^2+2x-8=0$. Using the quadratic formula, we find that the other two roots are $2$ and $-4$. Thus, all the roots are $\pm i, 2,$ and $-4$.
\end{example}

Notice how in that example, we started off just knowing one root and from that the polynomial collapsed and we could find the other 3, thus, even with partial information about the roots of a polynomial, we can use reasoning like this to solve for them all.

\section{Root-Coefficient Theorem}%
\label{rootcoefficienttheorem}

Another useful tool is the following theorem which shows how the coefficients of a polynomial relate to the roots.

\begin{theorem}[Root-Coefficient Theorem] If $p(x)=x^{n}+a_{n-1}x^{n-1}+\cdots + a_{1}x+a_{0}$, has roots $r_{1},...,r_{n}$ (counting multiplicities), then
\[
r_{1}+\cdots + r_{n} = -a_{n-1} \]
\[
r_{1}\cdots r_{n} = (-1)^{n}a_{0}.\]
In general, if $s_{j}$ denotes the sum of all products of $j$-tuples of the roots (e.g. $s_{2} = r_{1}r_{2}+r_{1}r_{3}+r_{2}r_{3}+\cdots $), then
\[
s_{j} = (-1)^{j}a_{n-j}.
\]
\end{theorem}

A ``tuple'' is a finite ordered list of items, here the roots.
The phrase ``$j$-tuples'' means a list of items of length \(j\).

\begin{proof}
%We prove by induction. The case when $n=1$ can easily be verified. Suppose now that any degree $n$ polynomial satisfies the conclusions of the above theorem. Let
%\[
%p(x)=x^{n+1}+a_{n}x^{n}+\cdots + a_{1}x+a_{0}.
%\]
%Let $r_{n+1}$ be a root of $p(x)$, so we can factor
%\[
%p(x) = (x-r_{n+1})q(x)\]
%where
%\[
%q(x) = x^{n}+(a_n-r_{n+1})x^{n-1}+\cdots +

First, factorize
\[
p(x)=x^{n}+a_{n-1}x^{n-1}+\cdots + a_{1}x+a_{0}=a(x-r_{1})\cdots (x-r_{n}).\]
Note that as the coefficient of $x^{n}=1$, we know $a=1$ (since otherwise the right side, when multiplied out, wouldn't equal the left). We can establish the formulas in the theorem now by multiplying out the product on the right.
\end{proof}

This theorem looks complicated, but in fact it just encodes a pattern.
\[ (x-\alpha)=x-\alpha\]
\[ (x-\alpha)(x-\beta)=x^2-(\alpha+\beta)x + \alpha\beta\]
\[ (x-\alpha)(x-\beta)(x-\gamma)
=x^3-(\alpha+\beta+\gamma)x^2 +(\alpha\beta+\beta\gamma +\alpha\gamma)x^2 - \alpha\beta\gamma\]
In the language of the theorem, a single \(2\)-tuple from the set \(\{\alpha,\beta,\gamma\}\) would be a two of these items.  The set of all different \(2\)-tuples is
\[ \alpha\beta,\beta\gamma,\alpha\gamma\]

\begin{example}
Suppose $x^{3}+ax^{2}+bx+c$ has roots $\alpha,\beta,$ and $\gamma$. Find a polynomial with roots $\alpha\beta$, $\beta\gamma$, and $\gamma\alpha$ in terms of $a,b,c$ (that is, the coefficients of your polynomial should only be described using $a,b,$ and $c$, not $\alpha,\beta$, and $\gamma$).

By the Root Coefficient Theorem,
\[
\alpha+\beta+\gamma = -a,
\]
\[
\alpha\beta+\beta\gamma+\gamma\alpha = b\]
and
\[
\alpha\beta\gamma = -c.\]
Let $x^{3}+Ax^{2}+Bx+C$ be a polynomial with roots $\alpha\beta$, $\beta\gamma$ and $\gamma\alpha$. Then we know
\[
-A=\alpha\beta+\beta\gamma+\gamma\alpha = b\]
\[
B=\alpha\beta^{2}\gamma+\alpha\beta\gamma^{2}+\alpha^{2}\beta\gamma=-c(\alpha+\beta+\gamma)=ac
\]
and
\[
-C=\alpha\beta \cdot \beta\gamma\cdot \gamma\alpha
 = (\alpha\beta\gamma)^{2}=c^{2}
 \]
 Hence, the polynomial is
 \[
 x^{3}-bx^{2}+acx-c^{2}.
 \]
\end{example}

\begin{example}
Are all the roots of $x^{3}+11x^2+7$ integers? \\

Suppose they were. Notice that by the Factorization Theorem, this polynomial has three roots $a,b,c$ (where some of these roots could repeat). By the Root-Coefficient Theorem, $abc=-7$, so $7=|abc|=|a|\cdot|b|\cdot |c|$. Since $7$ is prime and we are assuming the roots are integers, the only way this is possible is if one of these absolute values is $7$ and the others are $1$, say $|a|=7$ and $|b|=|c|=1$. The Root-Coefficient Theorem also says $a+b+c=-11$, so by the triangle inequality,
\[
11=|-11|=|a+b+c|\leq |a|+|b|+|c|=1+1+7=9,
\]
which is impossible. Thus, at least one of the roots is not an integer. \end{example}

\section{Exercises}%
\label{polynomialsexercises}

\begin{exercise} Find the (complex) roots of the following polynomials:\\

(a) $x^2-5x+7-i=0$.

\begin{solution}
Recall that the roots of this polynomial are
\[x= \frac{5\pm z}{2}\]
where $z$ are the solutions to
\[z^2 = 5^2-4\cdot (7-i)\cdot 1  = 25-28+4i = -3+4i.\]
If we set $z=a+ib$, this gives
\[a^2-b^2+2abi = -3+4i.\]
So in particular, $2ai = 4i$ and $a^2-b^2 = -3$. Moreover,
\[|z^2|= |-3 + 4i| = 5\]
Thus,
\[
5=|z^2|=|z|^2 = \sqrt{a^2+b^2}^2 = a^2+b^2
\]
Adding this to  $a^2-b^2=-3$ implies $2a^2 = 2$, so $a^2=1$, and so $a = \pm 1$.
Similarly, subtracting $a^2-b^2=-3$ from the above equation gives $2b^2 = 8$, so $b=\pm 2$.
Finally, $2ab = 4$ implies that $a$ and $b$ have the same algebraic sign, so we must have $a+ib$ is $1+2i$ or $-1-2i$ as our two solutions for $z$. Hence,
\[
x=\frac{5 \pm (1+2i)}{2} \]
so $x$ is either $3+i$ or $2-i$.

We can substitute these in to check, e.g. (just the first)
\[ (3+i)^2-5(3+i)+7-i = (8+6i)-5(3+i)+7-i =0 \]
\end{solution}

(b)  $ x^4 -x^2 - 1 = 0$.

\begin{solution}
Let $y=x^2$. Then $y^2+y+1=0$, and the solutions to this are
\[
x^2=y=\frac{1\pm \sqrt{5}}{2}
\]
Thus, we see that $x=\pm \sqrt{\frac{1+\sqrt{5}}{2}}$ are two solutions, we just need to find the other two. They will be solutions to $x^2 = \frac{1-\sqrt{5}}{2} = - \frac{\sqrt{5}-1}{2}$, which are $\pm i \sqrt{\frac{\sqrt{5}-1}{2}}$. Thus, all 4 solutions are

\[
\pm \sqrt{\frac{1+\sqrt{5}}{2}}, \;\; \pm i \sqrt{\frac{\sqrt{5}-1}{2}}.
\]
\end{solution}

(c) $2x^4-4x^3+3x^2+2x-2$, given that one of the roots is $1+i$.

\begin{solution}
Since this is a real polynomial, the conjugate of $1+i$ is also a root, so two of the root are $1\pm i$. Hence, the polynomial contains
\[
(x-(1+i))(x-(1-i))=
x^2-2x+2
\]
as a factor. Now let's do polynomial long division to find the other factor. The leading factor of our original polynomial is $2x^{4}$, which we can eliminate by subtracting $2x^2(x^2-2x+2)$ to get
\[
-x^2+2x-2=-(x^2-2x+2)
\]
Then finally we can just subtract $-1\cdot (x^2-2x+2)$ to eliminate this.
Thus, we can factor the above polynomial as
\[
2x^4-4x^3+3x^2+2x-2=(2x^2-1)(x^2-2x+2)=2(x-2^{-1/2})(x+2^{1/2})(x-(1+i))(x+(1-i))
\]
so the roots are finally $\pm 2^{-1/2}, 1\pm i$.
\end{solution}

\end{exercise}

\begin{exercise} Factor the following polynomials into products of real polynomials that are linear and/or quadratic:

\begin{itemize}
\item $x^3-1$.
\begin{solution}
We first need to find the roots of unity of $x^3$, which are $1, w=e^{2\pi i/3}=-\frac{1}{2}+\sqrt{3}{2}i$ and $w^2=e^{4\pi i/3} = -\frac{1}{2}-\sqrt{3}{2}i=\overline{w}$.  Thus,
\begin{align*}
x^3-1 & = (x-1)(x-w)(x-\overline{w}) = (x-1)(x^2-wx-\overline{w}x+\overline{w}w) \\
& = (x-1)(x^2+x+|w|^2) = (x-1)(x^2+x+1).
\end{align*}
\end{solution}
\item $x^3+1$.

\begin{solution}
We need to find the roots of $x^3+1=0$, which are solutions to $x^3=-1=e^{\pi i}$. One solution is clearly $1$. Another we can get by taking 1/3 the exponent of $e^{\pi i}$, which is $w=e^{\pi i/3} = \frac{1}{2}+\frac{\sqrt{3}}{2}$. We know that roots of real polynomials come in conjugate pairs, and so the other root is $\overline{w}$. Thus,
\begin{align*}
x^3+1 & = (x-(-1))(x-w)(x-\overline{w}) = (x+1)(x^2-wx-\overline{w}x+\overline{w}w) \\
& = (x+1)(x^2-x+|w|^2) = (x+1)(x^2-x+1).
\end{align*}
\end{solution}

\item $x^4-1$.

\begin{solution}
There is no need to find complex roots here:
\[
x^4-1 = (x^2)^2-1 = (x^2-1)(x^2+1).
\]
\end{solution}

\item $x^4+1$.

\begin{solution}
To find the roots of $x^4+1$, we need to find solutions to $x^4=-1=e^{\pi i}$. One solution is $z=e^{\pi i/4}=\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}i$. To find all 4 roots, we just multiply this by the 4th roots of unity, which are $\pm 1, \pm 1$, thus all solutions which will be
\[
\pm \frac{1}{\sqrt{2}} \pm \frac{1}{\sqrt{2}}i
\]
where we consider all 4 possible combinations of $+$'s and $-$'s. Let $z=\frac{1}{\sqrt{2}}+i\frac{1}{sqrt{2}}$ and $w=-\frac{1}{\sqrt{2}}+i\frac{1}{\sqrt{2}}$. Then the roots are $z,w,\overline{z},\overline{w}$. Thus,
\begin{align*}
x^4+1 &
= (x-z)(z-\overline{z})(x-w)(x-\overline{w})\\
& =(x^2 -zx-\overline{z}x+z\overline{z})(x^2 -wx-\overline{w}x+w\overline{w})\\
& = (x^2-\sqrt{2}x+|z|^2)(x^2+\sqrt{2}x+|w|^2)\\
& = (x^2-\sqrt{2}x+1)(x^2+\sqrt{2}x+1).
\end{align*}
\end{solution}

\item $x^5+1$. {\it Hint: $\cos \frac{2\pi}{5}= \frac{-1+\sqrt{5}}{4}$ and $\cos \frac{4\pi}{5}= \frac{-1-\sqrt{5}}{4}$.}

\begin{solution}
We note that this is easily completely factorized over the complex numbers.  Let $\xi = e^{\frac{2\pi i}{10}}$, then the $5$th roots of unity are $1,\xi,\xi^2, \xi^3,\xi^4$. By examining these numbers, we can see that $\xi^{4}=\overline{\xi}$ and $\xi^{3} = \overline{\xi}^{2}$.  Thus,

$$x^5+1 = (x+1)\underbrace{(x-\xi)(x-\bar{\xi})}_{\textrm{conjugate}}\underbrace{(x-\xi^2)(x-\bar{\xi}^2)}_{\textrm{conjugate}}.$$

If we collect into pairs the terms which involve a root and its conjugate, we obtain real polynomials.

We compute:
$$(x-\xi)(x-\bar{\xi}) = x^2 - (\xi+\bar{\xi})x + \xi\bar{\xi} = x^2 - 2\cos(\frac{2\pi}{5})x+1
= x^2 +\frac{1-\sqrt{5}}{2}x+1.
$$
and

$$(x-\xi^2)(x-\bar{\xi}^2) = x^2 -(\xi^2 + \bar{\xi}^2)x + \xi^2\bar{\xi}^2 = x^2 - 2\cos(\frac{4\pi}{5})x+1
= x^2 +\frac{1+\sqrt{5}}{2}x+1.
,$$
Hence,
\[
x^5 = \left(x^2 +\frac{1-\sqrt{5}}{2}x+1\right) \left(x^2 +\frac{1+\sqrt{5}}{2}x+1\right).
\]
\end{solution}

%\item $x^6+1$.
%
%\begin{solution}
%
%The roots of $x^6+1$ are $e^{i\pi/6}\omega^{j}$ where $\omega= e^{i2\pi/6}=e^{i\pi/3}$ and $j=0,1,2,3,4,5$. Hence,
%\[x^{6}+1
% =(x-e^{i\pi/6})(x-e^{i\pi/6}w)(x-e^{i\pi/6}w^2)(x-e^{i\pi/6}w^3)(x-e^{i\pi/6}w^4)(x-e^{i\pi/6}w^5).\]
%We need to group the terms into conjugate pairs, so that when we multiply the pairs out they become real numbers.
%Note that
%\[
%e^{i\pi/6}w^{5}=e^{i\pi/6+i5\pi/3}=e^{i11\pi/6}=e^{-i\pi/6}=\overline{e^{i\pi/6}},
%\]
%\[
%e^{i\pi/6}w^{4}=e^{i\pi/6+4\pi/3}=e^{i3\pi/2}=-i=\overline{i}=\overline{e^{i\pi/6}\omega}
%\]
%\[
%e^{i\pi/6}w^{3}=e^{i\pi/6+i\pi}=e^{i7\pi/6}=\overline{e^{5\pi/6}}
%=\overline{e^{i\pi/6}w^{2}}
%\]
%Thus,
%\begin{align*}
%x^{6}+1
%&  =(x-e^{i\pi/6})(x-e^{i\pi/6}w)(x-e^{i\pi/6}w^2)(x-e^{i\pi/6}w^3)(x-e^{i\pi/6}w^4)(x-e^{i\pi/6}w^5)\\
%& = (x-e^{i\pi/6})(x-e^{i\pi/6}w)(x-e^{i\pi/6}w^2)(x-e^{i\pi/6}w^3)(x-e^{i\pi/6}w^4)(x-e^{i\pi/6}w^5)
%\end{align*}
%Hence, rearranging the terms in our product for $x^6+1$, we get
%\begin{align*}
% x^6+1
%&  =(x-e^{i\pi/6})(x-e^{i\pi/6}w^5)(x-e^{i\pi/6}w^2)(x-e^{i\pi/6}w^3)(x-e^{i\pi/6}w^4)(x-e^{i\pi/6}w)\\
%& =(x-e^{i\pi/6})(x-e^{-i\pi/6})(x-e^{i\pi/6}w^2)(x-e^{-i\pi/6}w^{-2})(x-e^{-i\pi/6}w^{-1})(x-e^{i\pi/6}w)\\
%& =(x^2-2xRe(e^{i\pi/6})+1)(x^2-2xRe(e^{i\pi/6}w^2)+1)(x^2-2xRe(e^{i\pi/6}w)+1)\\
%& = (x^2-2x\cos\pi/6+1)(x^2-2x\cos(7\pi/6)+1)(x^2-2x\cos \pi/2+1)\\
%& = (x^2-\sqrt{3}x+1)(x^2+\sqrt{3}x+1)(x^2+1).
%\end{align*}
%
%
%\end{solution}
\end{itemize}
\end{exercise}

%2019/20 Exam problem
%\begin{exercise} Prove that for complex numbers $z$ and $w$
%\[
%|z+w|^2+|z-w|^2=2|z|^2+2|w|^2.
%\]
%
%
%\end{exercise}

\begin{exercise} If $x^3+15x^2+74x+120$ has roots of the form $a,a+1,a+2$, find $a$.

\begin{solution}
We see that by the Root-Coefficient Theorem,
\[
-15=a+a+1+a+2=3a+3\]
and so $a=-6$.
\end{solution}

\end{exercise}

\begin{exercise}
 Let $a\in \mathbb{R}$. If $x^{3}-x+a$ has three integer roots, solve for $a$.

\begin{solution}

Note that if the integer roots are $r_{1},r_{2}$, and $r_{3}$, then by Proposition 7.1,
\[
r_{1}+r_{2}+r_{3}=0\]
and
\[
r_{1}r_{2}+r_{2}r_{3}+r_{3}r_{1}=-1
\]
Hence,
\[
r_{1}^{2}+r_{2}^{2}+r_{3}^{2} = (r_{1}+r_{2}+r_{3})^{2}-2(r_{1}r_{2}+r_{2}r_{3}+r_{3}r_{1})=2.
\]
Since the $r_{i}$ are integers, the $r_{i}^{2}$ are nonnegative integers, and so one of them has to be zero. Thus, $0=0^{2}-0+a$, hence $a=0$.

\end{solution}

\end{exercise}

\begin{exercise} Show that if $z_1,z_2,z_3\in \C$ are so that $z_1+z_2+z_3=0$, and $z_1^2+z_2^2+z_3^2=0$ then $|z_1|=|z_2|=|z_3|$

\begin{solution}
Let $f(z)=(z-z_1)(z-z_2)(z-z_3)$. Then $f(z)=z^3-az^2+bz-c$ where
\begin{align*}
a&=z_1+z_2+z_3\\[4pt]
b&=z_1z_2+z_2z_3+z_3z_1\\[4pt]
c&=z_1z_2z_3\\[4pt]
\end{align*}
By assumption, we get that $a=z_1+z_2+z_3=0$.  Also, since $z_{1}^2+z_{2}^2+z_{3}^2=0$, we have
\[
(z_1+z_2+z_3)^2=z_1^2+z_2^2+z_3^2+2(z_1z_2+z_2z_3+z_3z_1)
\]
and so $b=0$. Thus, $f(z)=z^3-c$, so in particular, since $z_{1},z_{2}$ and $z_{3}$ are roots, we have $z_{1}^3=z_{2}^3=z_{3}^{3}=c$. Thus
\[
|c|=|z_{i}^3|=|z_{i}|^3
\]
so $|z_{i}|=|c|^{1/3}$ for $i=1,2,3$.
\end{solution}

\end{exercise}

%
%
%\begin{exercise} Show that the solutions of $z^3=c$ where $|c|=1$ are the corners of an equilateral triangle.
%
%\end{exercise}
%

\begin{exercise} Suppose $|z_{1}|=|z_{2}|=|z_{3}|=1$ and $z_{1}+z_{2}+z_{3}=0$. Show that $z_{1}^3=z_{2}^3=z_{3}^3$. {\it Hint: First think about when $z_{1}=1$, then use that to prove the general case.}

\begin{solution}
First let's assume $z_{1}=1$. Then $z_{2}+z_{3}=-1$. In particular, this means that the imaginary parts of $z_{2}$ and $z_{3}$ cancel, that is, if $z_{j}=x_{j}+iy_{j}$, then $y_{2}=-y_{3}$. Also, $x_{2}+x_{3}=-1$, and we can't have $x_{2}>0$, since then $x_{2}+x_{3}>x_{3}\geq -1$. Similarly, we can't have $x_{3}>0$, thus $x_{2},x_{3}\leq 0$. Also,
\[
-x_{2} = \sqrt{1-y_{2}^2} = \sqrt{1-y_{3}^2}= -x_{3},\]
hence $z_{3} = -x_{2} -iy_{2} = \overline{z_{2}}$, and $-1=z_{2}+z_{3}=-x_{2}-x_{2}=-2x_{2}$, thus $x_{2}=-\frac{1}{2}$. Thus, we must have that $z_{3} = -\frac{1}{2}\pm i\frac{\sqrt{3}}{2}$.

For the general case, let $w_{i}=\overline{z_{1}}z_{i}$, then $w_{1}=1$ and $w_{1}+w_{2}+w_{3}=0$. We now apply the previous case to conclude that $w_{i} = w^{i}$ where $w=e^{2\pi i/3}$. Thus,
\[
z_{j}^3 = (z_{1} w_{j})^3 = z_{1}^3 w_{j}^3 = z_{1} (w^{3j})=z_{1}.
\]
\end{solution}
\end{exercise}

\begin{exercise} Suppose $|z_{1}|=|z_{2}|=|z_{3}|=1$ and $z_{1}+z_{2}+z_{3}=0$. Show that $z_{1}^{2^{n}}+z_{2}^{2^{n}}+z_{3}^{2^{n}}=0$ for all $n\in \mathbb{N}$.

\begin{solution}
By the previous problem, $z_1,z_2,z_3$ are roots of $z^3-c$ for some complex number $c$ with $|c|=1$. In particular, by the Root-Coefficient Theorem,
\[
z_{1}z_{2}+z_{1}z_{2}+z_{2}z_{3}=0.
\]
Thus,
\[
0=(z_{1}+z_{2}+z_{3})^2 = z_{1}^2+z_{2}^2+z_{3}^2 + 2(z_{1}z_{2}+z_{1}z_{2}+z_{2}z_{3})=z_{1}^2+z_{2}^2+z_{3}^2 .
\]
Now we can repeat the process by induction using $z_{1}^2,z_{2}^2,z_{3}^2$ instead of $z_1,z_2,z_3$.
\end{solution}

\end{exercise}

%
%\begin{exercise} {\bf Challenging:} We all know what $\cos \theta$ and $\sin\theta$ are when $\theta$ is a multiple of $\pi/6$, $\pi/4$, $\pi/3$, or $\pi/2$, but what about $\pi/5$, $\pi/7$ and $\pi/8$?  In the following problems, we will use complex numbers to find other values of cosine and sine, and other interesting facts about these trigonometric functions.
%
%
%\begin{itemize}
%
%\item (Liebeck 6.7)  Here we will find $\cos \pi/5$.
%\begin{itemize}
%\item Let $w=e^{2\pi i/5}$. Show that
%\[
%1+w+w^2+w^3+w^4=0.
%\]
%\begin{solution}
%Note that
%\[
%1+w+w^2+w^3+w^4 = \frac{1-w^5}{1-w}=\frac{1-1}{1-w}=0.
%\]
%\end{solution}
%
%\item Let $\alpha = 2\cos 2\pi/5$ and $\beta = \cos 4\pi/5$. Show that $\alpha = w+w^4$ and $\beta = w^2+w^3$.
%
%\item Find a polynomial whose roots are $\alpha$ and $\beta$, solve it to find $\cos 2\pi /5$.
%
%\begin{solution}
%Let
%\begin{align*}
%p(x)
%& = (x-\alpha)(x-\beta)\\
%& =x^2 - (\alpha + \beta)x+\alpha \beta \\
%& =x^2 -(w+w^2+w^3+w^4)x+(w^3+w^4+w^6+w^7) \\
%& = x^2 +x + w^3 (1+w+w^3+w^4)\\
%& = x^2 + x - w^3w^2 = x^2 + x -1 .
%\end{align*}
%The roots of this polynomial are
%\[
%\frac{-1 \pm \sqrt{5}}{2}.
%\]
%Since $\alpha>0>\beta$, we see that
%\[
%2\cos \frac{2\pi}{5} = \alpha  = \frac{-1+\sqrt{5}}{2}.
%\]
%Thus,
%\[
%\cos \frac{2\pi}{5} = \frac{-1+\sqrt{5}}{4}.
%\]
%Finally, since
%\[
%\cos \frac{2\pi}{5} = 2\cos^2 \frac{\pi}{5} -1,\]
%we see that
%\[
%\cos \frac{\pi}{5} = \sqrt{\frac{3 + \sqrt{5}}{8}}.
%\]
%
%
%
%\end{solution}
%\end{itemize}
%
%%
%\item Repeat the argument using $7$th roots of unity to find a polynomial with integer coefficients whose roots are $2\cos 2\pi/7$,  $2\cos 4\pi/7$, and $2\cos 6\pi/7$.
%
%\begin{solution}
%Again, if $w=e^{2\pi i/7}$, we have
%\[
%1+w+w^2+\cdots + w^6=0.
%\]
%Moreover, notice that we can match these terms as conjugate pairs: $\overline{w} = w^6$, $\overline{w^2}=w^5$, $\overline{ w^3} = w^{4}$. Thus, when we add these together, we get
%\[
%\alpha = w+w^6 = 2\cos \frac{2\pi}{7}, \;\; \beta = w^2+w^5 = 2\cos \frac{4\pi }{7}, \;\; \gamma = w^{3} + w^{4} = 2\cos\frac{6\pi }{7} .
%\]
%Let's look at the polynomial that has these numbers as roots:
%
%\[
%p(x)
% = (x-\alpha)(x-\beta)(x-\gamma)
%= x^3+ax^2+bx+c
%\]
%where
%\[
%-a=\alpha +\beta + \gamma=1+w+\cdots + w^6=0,
%\]
%\begin{align*}
%b & = \alpha \beta + \beta \gamma + \gamma\alpha\\
%& = w^3+w^6+w^8+w^{11} + w^5+w^6+w^8+w^9 + w^4+w^5+w^9+w^{10}\\
%& =w^3(1+2w^3+2w^5+w^8+2w^2+2w^6+w+w^7) \\
%& w^3(1+2w^3+2w^5+w+2w^2+2w^6+w+1)  \\
%& = 2w^3(1+w^3+w^5+w^2+w^6+w)\\
%& =2w^3\cdot 2^4 = 2.
%\end{align*}
%and finally,
%\begin{align*}
%-\alpha\beta\gamma
%& = (w+w^6)(w^2+w^5)(w^3+w^4)\\
%& =w(1+w^5)w^2(1+w^3)w^3(1+w) \\
%& =w^6(1+w^5)(1+w^3)(1+w) = w^6(1+w+w^3+w^4+w^5+w^6+w^8+w^9)\\
%& = w^{6}(-w^2+w^8+w^9) =w^6(-w^2+w+w^2) = w^7=1.
%\end{align*}
%Hence,
%\[
%p(x) = x^3+2x-1.
%\]
%
%
%\end{solution}
%
%
%\item Find $\cos \pi/8$.
%
%\begin{solution}
%We can just do this via the double angle formula:
%\[
%\frac{1}{\sqrt{2}} = \cos \pi/4 = 2\cos^2 \pi/8-2
%\]
%and so
%\[
%\cos \frac{\pi}{8} = \sqrt{\frac{2\sqrt{2}+1}{2\sqrt{2}}}.
%\]
%
%\end{solution}
%
%
%
%%
%%\begin{align*}
%%\cos \frac{\pi}{3}
%%& =\cos \frac{\pi}{9}\cos\frac{2\pi}{9}
%%-\sin \frac{2\pi}{9}\sin\frac{\pi}{9}\\
%%& =\cos\frac{\pi}{9}(2\cos^2\frac{\pi}{9}-1)-2\sin^2\frac{\pi}{9}\cos\frac{\pi}{9}\\
%%& =2\cos^3\frac{\pi}{9} - \cos\frac{\pi}{9} -2\cos\frac{\pi}{9} +2\cos^2\frac{\pi}{9}
%%=4\cos^3\frac{\pi}{9} -3\cos\frac{\pi}{9}
%%\end{align*}
%%
%%Thus, $\cos^3\frac{\pi}{9}$ is a root of $0=4x^3-3x-\cos \frac{\pi}{3}=4x^3-3x-\frac{1}{2}$.
%
%
%\item Similar to what we did with $7$th roots of unity, use $9$th roots of unity to find a three degree polynomial with integer coefficients whose roots are $ 2 \cos \frac{2\pi}{9}$, $ 2\cos \frac{4\pi}{9}$, and $ 2\cos \frac{8\pi }{9}$.
%
%\begin{solution}
%Consider the numbers $\alpha = 2 \cos \frac{2\pi}{9}$, $\beta = 2\cos \frac{4\pi}{9}$, $\gamma = 2\cos \frac{8\pi }{9}$.
%
%\[
%\alpha = w+w^8, \;\; \beta = w^2+w^7, \;\; \gamma = w^4+w^5.
%\]
%Thus,
%\[
%p(z)
%=(z-\alpha)(z-\beta)(z-\gamma)
% = z^3 -(\alpha+\beta+\gamma)z^2 +(\alpha\beta+\beta\gamma+\alpha\gamma)z-\alpha\beta\gamma.
% \]
% Note that if $u=e^{2\pi i/3}$, then $w^3=u$, and so
% \[
% \alpha+\beta+\gamma=w+w^2+w^4+w^5+w^7+w^8
% =-1-w^3-w^6 = -1-w-w^2
% =0.
% \]
% Also,
% \begin{align*}
% \alpha\beta & +\beta\gamma+\alpha\gamma\\
%& =w^3+w^8+w^{10}+w^{15} +w^6+w^7+w^{11}+w^{12} + w^5+w^6+w^{12}+w^{13}\\
%& = w^3+w^8+w+w^{6} +w^6+w^7+w^{2}+w^{3} + w^5+w^6+w^{3}+w^{4}\\
%& = 3w^3+w^8+w+3w^6+w^7+w^2+w^5+w^4 \\
%& = 2w^3+2w^6=2(u+u^2)=-2.
% \end{align*}
% Finally,
% \begin{align*}
% \alpha\beta\gamma
% & = (w^3+w^8+w^{10}+w^{15})(w^4+w^5)
% =w^7+w^{12}+w^{14}+w^{19}+w^8+w^{13}+w^{15}+w^{20}\\
% & = w^7+w^3+w^5+w+w^8+w^4+w^6+w^2
% -1.
% \end{align*}
%
% Thus,
% \[
% p(z) = z^3-2z+1.
% \]
%
%
%
%
%\end{solution}
%\end{itemize}
%
%
%\end{exercise}
%
%\begin{exercise} ({\bf Challenge!}) Show that
%\[
%\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}.
%\]
%
%\begin{solution}
%Let $P$ denote the product above and $w=e^{2i\pi/n}$ be the $n$th root of unity. Then
%\begin{align*}
%P
%& =\prod_{k=1}^{n-1}\sin(k\pi/n)=(2i)^{1-n}\prod_{k=1}^{n-1}(e^{ik\pi/n}-e^{-ik\pi/n})\\
%& =(2i)^{1-n}e^{-i\pi n(n-1)/(2n)}\prod_{k=1}^{n-1}(e^{2ik\pi/n}-1)\\
%& =(-2)^{1-n}\prod_{k=1}^{n-1}(w^k-1)=2^{1-n}\prod_{k=1}^{n-1}(1-w^k),
%\end{align*}
%Now note, that $x^n-1=(x-1)\sum_{k=0}^{n-1}x^k$ and $x^n-1=\prod_{k=0}^{n-1} (x-w^k)$, thus cancelling $x-1$ we have $\prod_{k=1}^{n-1} (x-w^k) =\sum_{k=0}^{n-1}x^k$. Substituting $x=1$ we have $\prod_{k=1}^{n-1} (1-w^k)=n$. Therefore $P=n2^{1-n}$.
%
%\end{solution}
%
%\end{exercise}
%

%Interesting open problems:
%
%
%\begin{question}[Bocard's problem]
%Are there infinitely many $n\in\mathbb{N}$ so that $n!+1$ is a square? Only 3 integers are known: $4!+1=5^2$, $5!+1=11^2$, and $7!=71^2$.
%\end{question}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\chapterimage{Figures/blank.png}

\part{Week 5: Integers}

\chapter{Integers}%
\label{integers}

%\epigraph{\it It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second, into two like powers. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain.}{Pierre de Fermat, {\it Arithmetica}, 1637...\\ Proof not found until 1994.}

%\epigraph{\it God may not play dice with the universe, but something strange is going on with the prime numbers.}{Paul Erd\"{o}s}

This week we will study the integers, particularly the techniques of studying divisibility, the highest common factor, and prime factorization.

These techniques are especially useful for solving {\it diophantine equations}, which are polynomial equations where we seek {\it integer} solutions. The most famous diophantine equation is
\[
x^{n}+y^{n}=z^{n}\]
It was famously claimed by Fermat (without proof) that there are solutions  $x,y,z,n\in\mathbb{N}$ only if $n=1$ or $n=2$. It wasn't until 1994 when Andrew Wiles actually gave a proof.

This week's set of tools are also useful when we are trying to show at a diophantine {\it can't} be solved. You have seen one such proof before: that $\sqrt{2}$ is irrational. Let's recall the proof briefly:

Suppose there was a rational number $\frac{m}{n}$ (where $m$ and $n$ are reduced in the sense that they have no common divisors apart from $1$) so that $\left(\frac{m}{n}\right)^2=2$, then $m^2=2n^2$. This means that $m^2$ is even. Then $m$ must be even since, if instead $m=2k+1$ for some integer $k$, then $m^2=(2k+1)^2=4(k^2+k)+1$, which is odd, a contradiction. Hence, $m=2k$ for some integer $k$, and so
\[
2n^2=m^2=(2k)^2=4k^2\]
and so $n^2=2k^2$, and using a similar reasoning, we get that $n$ must also be even, which is a contradiction since we assumed $m$ and $n$ had no common factors, but now we have shown they are both divisible by $2$.

The important feature of the proof was to use the {\it coprimality} of $m$ and $n$, that is, that they share no common divisors, as well as the fact that if $m^2$ is divisible by $2$ (that is, if $m^2$ was even, then so was $m$). If we want to generalize this proof, we will need to develop  these tools about divisibility and coprimality. Later, we will in fact show that $\sqrt{n}$ is rational exactly when $n=m^2$ for some integer $m$.

As motivation for the techniques we develop along the way, we will show how they can be used to solve various diophantine equations. As a special final application of these techniques, we'll also classify all {\it Pythagorean Triples}, that is, integers $x,y,z$ so that
\[
x^2+y^2=z^2.\]

If you like the material from this week, you might enjoy taking {\it Introduction to Number Theory} (4th year).

\section{Remainders and divisibility }%
\label{remainders}

The following theorem is the starting point for our studies on divisibility.

\begin{theorem}[The Remainder Theorem]
Let  $a\in\mathbb{N}$ and $b\in\mathbb{Z}$. There are unique integers $q\in \mathbb{Z}$ and $0\leq r<a$ such that
\[
b=qa+r.
\]
\end{theorem}

\begin{proof}
We prove existence and uniqueness separately. \\

\noindent {\bf Existence:} Let $q$ be the largest integer for which $qa\leq b$. Let $r=b-qa\geq 0$. Then we must also have $r<a$, since if $r\geq a$, then $b-qa\geq a$, and so $b-(q+1)a\geq 0$, which contradicts $q$ being the largest integer for which $qa\leq b$. This shows the existence of a pair $(q,r)$ satisfying the theorem. \\

\noindent {\bf Uniqueness:} Assume that $(q',r')$ is a pair of integers such that $b=q'a+r'$ and $0\leq r'<a$.
Then
\[
0=b-b=q'a+r'-qa-r = (q'-q)a+(r'-r),
\]
and so
\[
r'-r=(q-q')a.
\]
Since $0\leq r,r'<a$, we know $|r-r'|<a$. Thus
\[
a>|r-r'|=|(q-q')a|=|q-q'|\cdot a,
\]
which implies that $1>|q-q'|$.
Since $|q-q'|$ is a nonnegative integer, it therefore follows that $q=q'$.
\end{proof}

\begin{definition}
For two integers $a$ and $b$ we say {\it $a$ divides $b$} if there is an integer $c$ so that $b=ac$.

Some authors will write $a | b$ for the sentence ``$a$ divides $b$.''
(This isn't great notation.)
\end{definition}

\noindent So for example, $2$ divides $4$, but $2$ does not divide $3$.

\begin{lemma}
\label{l:abba}
If $a$ divides $b$ and $b$ divides $a$, then $a=\pm b$.
\end{lemma}

\begin{proof}
Exercise.
\end{proof}

%\begin{proof}
%Since $a|b$, we know $b=na$ for some integer $n$, and similarly, since $b|a$, we know $a=mb$ for some integer $m$. Hence, $a=mb=mna$, and so $1=mn$. The only way this can happen is if either $m=n=1$ or $m=n=-1$.
%\end{proof}

\section{The GCD and the Euclidean Algorithm}%
\label{euclideanalgo}

\begin{definition}
Let $a$ and $b$ be two nonzero integers.
A \emph{common divisor} or \emph{common factor} of $a$ and $b$ is an integer $d \in \N$ such that $d$ divides both $a$ and $b$.
The {\it greatest common divisor} (AKA the {\it highest common factor}) of $a$ and $b$ is the \emph{largest} common divisor of $a$ and $b$.
We denote this integer $d$ by $\gcd(a,b)$ (you may also see the notation $\mathrm{hcf}(a,b)$.

If $\gcd(a,b)=1$, then we say that $a$ and $b$ are said to be {\it coprime} or {\it relatively prime}.
\end{definition}

Thus $a$ and $b$ are coprime if and only if their only (positive) common divisor is $1$.

\begin{example} $\gcd(15,45)=15$,
$\gcd(6,15)=3$, and $\gcd(17,91)=1$.
\end{example}

\begin{example}
We allow ourselves the flexibility to speak of the gcd of negative integers as well as positive ones, but we notice that
\[
\gcd(a,b) = \gcd(a,-b) = \gcd(-a,b) = \gcd(-a,-b).
\]
(So for instance $\gcd(-6,-27) = \gcd(6, 27) = 3$.)
As a result, we will mostly focus on the gcd of natural numbers.
\end{example}

For a prime $p>1$ and any integer $n$, we have:
$$
\gcd(p,n) = \left\{\aligned%
&1\ \text{if $p$ does not divide $n$},\\
&p\ \text{if $p$  divides $n$}.\\
\endaligned
\right.
$$

\begin{definition}
Let $a$ and $b$ be integers.
An \emph{integer linear combination} of $a$ and $b$ is an integer of the form $ma + nb$ for some $m,n \in \Z$.
\end{definition}

Notice that a linear combination of linear combinations is a linear combination.
That is, if $c$ and $b$ are each linear combinations of $a$ and $b$, then for some integers $m$, $n$, $p$, and $q$, we have $c = m a + n b$ and $d = p a + q b$.
If we form a linear combination of $c$ and $d$, we get
\[
r c + s d = (rm+sp)a + (rn+sq)b,
\]
which is a linear combination of $a$ and $b$.

A little trick that we will use repeatedly is the observation that common divisors of two integers are also divisors of linear combinations of those integers:

\begin{lemma}[Linear Combination Lemma]
\label{l:easy-lemma}
Let $a$, $b$, and $d$ be integers. If $d$ divides both $a$ and $b$, then $d$ divides every integer linear combination of $a$ and $b$.
In other words, for every pair of integers $m,n\in\mathbb{Z}$, the integer $d$ also divides $ma + nb$.
\end{lemma}

\begin{proof}
Since $d$ divides $a$ and $d$ divides $b$, there are integers $s$ and $t$ so that $a=sd$ and $b=td$.
Hence
\[
ma+nb=msd+ntd=(ms+nt)d\]
and so $d$ divides $ma+nb$.
\end{proof}

\def\easylemma{\hyperref[l:easy-lemma]{the Linear Combination Lemma}}

\begin{example}
Let $a, b \in \Z$.
If there is a linear combination of $a$ and $b$ that gives $1$, then $a$ and $b$ are coprime.
Indeed, if $m,n \in \Z$ have the property that $ma+nb = 1$, then any common divisor $d$ of $a$ and $b$ must also divide $1$. But then $d=1$, so $\gcd(a,b)=1$.

Here we'll show $n$ and $n+1$ are coprime: By \easylemma, $\gcd(n,n+1)$ divides $(n+1)-n=1$, so $\gcd(n,n+1)$ divides $1$. Thus $\gcd(n,n+1)=1$.
\end{example}

We can also use it to narrow down the gcd of two numbers:

\begin{example}
For $n\in\mathbb{N}$, what is $\gcd(4n^2-2,2n)?$ Observe that by \easylemma, $\gcd(4n^2-2,2n)$ divides $(2n)(2n)-(4n^2-2)=2$, so the highest common factor of these numbers is either $1$ or $2$. Since $2$ divides both $4n^2-2$ and $2n$, it also divides the $\gcd$, so Lemma \ref{l:abba} implies $\gcd(4n^2-2,2n)=2$.
\end{example}

How do we find the gcd for very large numbers? For this we use the {\it Euclidean Algorithm} which exploits the Remainder Theorem and \easylemma.

Let $a,b\in\N$. Let us also assume $a<b$.
We are going to construct a sequence of nonnegative integers
\[
0 = r_{n+1} < r_n < \cdots < r_2 < r_1 = a < r_0 = b
\]
such that for each $k \in \N$, if $k\leq n$, then:
\begin{enumerate}
    \item $r_{k-1}$ is an integer linear combination of $r_k$ and $r_{k+1}$, and
    \item $r_{k+1}$ is an integer linear combination of $r_k$ and $r_{k-1}$.
\end{enumerate}

If we manage to construct a sequence with these properties, then $\gcd(a,b)$ will be the natural number $r_n$.
Before we perform the construction, let's understand why.
The key is \easylemma.
\begin{itemize}
\item We first claim that $\gcd(a,b)$ divides $r_{n}$.
Note that $r_0=b$ and $r_1=a$ are certainly linear combinations of $a$ and $b$.
Let $k\in \N$ with $1\leq k \leq n$.
If $r_{k-1}$ and $r_k$ are each integer linear combinations of $a$ and $b$, so is $r_{k+1}$.
By induction, we conclude that $r_n$ is a integer linear combination of $a$ and $b$.
Hence it follows from \easylemma\ that $\gcd(a,b)$ divides $r_n$.
\item On the other hand, we claim that $r_{n}$ divides $\gcd(a,b)$.
This is equivalent to the claim that $r_n$ is a common divisor of $a$ and $b$.
Note that since $r_{n+1}=0$, it follows that $r_{n-1}=q_{n+1}r_{n}$ for some $q_{n+1}\in\Z$, so $r_{n}$ divides $r_{n-1}$.
Let $k\in \N$ with $k \leq n$.
Assume that $r_n$ divides $r_k$ and $r_{k+1}$; since $r_{k-1}$ is a linear combination of $r_k$ and $r_{k+1}$, it follows from \easylemma\ that $r_n$ divides $r_{k-1}$ as well.
By induction, we conclude that $r_n$ divides $r_1=a$ and $r_0=b$.
\end{itemize}
This proves (using Lemma \ref{l:abba}) that $\gcd(a,b)=r_n$.

So our goal now is to present an iterative construction of a sequence of nonnegative integers
\[
0 = r_{n+1} < r_n < \cdots < r_2 < r_1 = a < r_0 = b
\]
such that for each $k \in \N$, if $k\leq n$, then
\begin{enumerate}
    \item $r_{k-1}$ is an integer linear combination of $r_k$ and $r_{k+1}$, and
    \item $r_{k+1}$ is an integer linear combination of $r_k$ and $r_{k-1}$.
\end{enumerate}

To begin the iteration, we set $r_0 = b$ and $r_1 = a$.
To construct the rest of the sequence, assume that for some $k\in\N$ we have constructed nonnegative integers $r_k < r_{k-1} < \dots < r_1 < r_0$ with the two properties above.
Now use the Remainder Theorem to find an integers $q_{k+1}$ and $r_{k+1}$ such that
\[
r_{k-1} = q_{k+1}r_k + r_{k+1}.
\]
Thus $r_{k-1}$ is a linear combination of $r_k$ and $r_{k+1}$, but also $r_{k+1} = r_{k-1} - q_{k+1} r_k$ so $r_{k+1}$ is a linear combination of $r_k$ and $r_{k-1}$.

This process must terminate with $r_{n+1}=0$ for some $n$, since the $r_{j}$ are nonnegative and strictly decreasing.
This completes the construction of the sequence
\[
0 = r_{n+1} < r_n < \cdots < r_2 < r_1 = a < r_0 = b
\]
with the desired properties.

Let's watch this algorithm in action.
\begin{example}
Let $a = r_1 = 6$ and $b = r_0 = 15$.
\begin{itemize}
    \item We divide $15$ by $6$ to get $2$ with a remainder of $r_2=3$.
    \item We divide $6$ by $3$ to get $2$ with a remainder of $r_3 = 0$.
\end{itemize}
Thus $r_2 = 3$ is the gcd of $6$ and $15$.
\end{example}

\begin{example}
Let $a = r_1 = 48$ and $b = r_0 = 66$.
\begin{itemize}
    \item We divide $66$ by $48$ to get $1$ with a remainder of $r_2=18$.
    \item We divide $48$ by $18$ to get $2$ with a remainder of $r_3 = 12$.
    \item We divide $18$ by $12$ to get $1$ with a remainder of $r_4 = 6$.
    \item We divide $12$ by $6$ to get $2$ with a remainder of $r_5 = 0$.
\end{itemize}
Thus $r_4 = 6$ is the gcd of $48$ and $66$.
\end{example}

\begin{example}
Let $a = r_1 = 120$ and $b = r_0 = 214$.
\begin{itemize}
    \item We divide $214$ by $120$ to get $1$ with a remainder of $r_2=94$.
    \item We divide $120$ by $94$ to get $1$ with a remainder of $r_3 = 26$.
    \item We divide $94$ by $26$ to get $3$ with a remainder of $r_4 = 16$.
    \item We divide $26$ by $16$ to get $1$ with a remainder of $r_5 = 10$.
    \item We divide $16$ by $10$ to get $1$ with a remainder of $r_6 = 6$.
    \item We divide $10$ by $6$ to get $1$ with a remainder of $r_7 = 4$.
    \item We divide $6$ by $4$ to get $1$ with a remainder of $r_8 = 2$.
    \item We divide $4$ by $2$ to get $2$ with a remainder of $r_9 = 0$.
\end{itemize}
Thus $r_8 = 2$ is the gcd of $120$ and $214$.
\end{example}

\begin{example}
Let $a = r_1 = 14$ and $b = r_0 = 348$.
\begin{itemize}
    \item We divide $348$ by $14$ to get $24$ with a remainder of $r_2=12$.
    \item We divide $14$ by $12$ to get $1$ with a remainder of $r_3 = 2$.
    \item We divide $12$ by $2$ to get $6$ with a remainder of $r_4 = 0$.
\end{itemize}
Thus $r_3 = 2$ is the gcd of $14$ and $348$.
\end{example}

\begin{example}
Let $a = r_1 = 2059$ and $b = r_0 = 6744$.
\begin{itemize}
    \item We divide $6744$ by $2059$ to get $3$ with a remainder of $r_2=567$.
    \item We divide $2059$ by $567$ to get $3$ with a remainder of $r_3 = 358$.
    \item We divide $567$ by $358$ to get $1$ with a remainder of $r_4 = 209$.
    \item We divide $358$ by $209$ to get $1$ with a remainder of $r_5 = 149$.
    \item We divide $209$ by $149$ to get $1$ with a remainder of $r_6 = 60$.
    \item We divide $149$ by $60$ to get $2$ with a remainder of $r_7 = 29$.
    \item We divide $60$ by $29$ to get $2$ with a remainder of $r_8 = 2$.
    \item We divide $29$ by $2$ to get $14$ with a remainder of $r_9 = 1$.
    \item We divide $2$ by $1$ to get $2$ with a remainder of $r_{10} = 0$.
\end{itemize}
Thus $r_9 = 1$ is the gcd of $2059$ and $6744$.
In particular, $2059$ and $6744$ are coprime.
\end{example}

One important corollary of the Euclidean Algorithm is the following.
\begin{theorem}[Bezout’s  Identity]
\label{t:lincombgcd}
 Let $a$ and $b$ be integers.
 Then $\gcd(a,b)$ is an integer linear combination of $a$ and $b$.
 That is, there are integers $s$ and $t$ so that $\gcd(a,b)=sa+tb$.
\end{theorem}

Let's revisit some of the examples above to find the $s$ and $t$.
\begin{example}
Let $a = r_1 = 6$ and $b = r_0 = 15$.
\begin{itemize}
    \item We divide $15$ by $6$ to get $2$ with a remainder of $r_2=3$.
    \item We divide $6$ by $3$ to get $2$ with a remainder of $r_3 = 0$.
\end{itemize}
Thus $\gcd(6,15) = 3 = 15-2\cdot 6$.
\end{example}

\begin{example}
Let $a = r_1 = 48$ and $b = r_0 = 66$.
\begin{itemize}
    \item We divide $66$ by $48$ to get $1$ with a remainder of $r_2=18$.
    \item We divide $48$ by $18$ to get $2$ with a remainder of $r_3 = 12$.
    \item We divide $18$ by $12$ to get $1$ with a remainder of $r_4 = 6$.
    \item We divide $12$ by $6$ to get $2$ with a remainder of $r_5 = 0$.
\end{itemize}
Thus $\gcd(48,66) = 6 = 18-12 = 3\cdot 18-48 = 3\cdot 66 -4\cdot 48$.
\end{example}

\begin{example}
Let $a = r_1 = 2059$ and $b = r_0 = 6744$.
\begin{itemize}
    \item We divide $6744$ by $2059$ to get $3$ with a remainder of $r_2=567$.
    \item We divide $2059$ by $567$ to get $3$ with a remainder of $r_3 = 358$.
    \item We divide $567$ by $358$ to get $1$ with a remainder of $r_4 = 209$.
    \item We divide $358$ by $209$ to get $1$ with a remainder of $r_5 = 149$.
    \item We divide $209$ by $149$ to get $1$ with a remainder of $r_6 = 60$.
    \item We divide $149$ by $60$ to get $2$ with a remainder of $r_7 = 29$.
    \item We divide $60$ by $29$ to get $2$ with a remainder of $r_8 = 2$.
    \item We divide $29$ by $2$ to get $14$ with a remainder of $r_9 = 1$.
    \item We divide $2$ by $1$ to get $2$ with a remainder of $r_{10} = 0$.
\end{itemize}
Thus
\begin{align*}
    \gcd(6744, 2059) = 1 &= 29 - 14\cdot 2 \\
    &= 29\cdot 29 - 14 \cdot 60 \\
    &= 29\cdot 149-72\cdot 60 \\
    &= 101\cdot 149-72\cdot 209 \\
    &= 101\cdot 358-173\cdot 209 \\
    &= 274\cdot 358-173\cdot 567 \\
    &= 274\cdot 2059-995\cdot 567 \\
    &= 3259\cdot 2059-995\cdot 6744 \\
\end{align*}
\end{example}

Bezout's Identity is incredibly useful, even in examples that seem to have nothing to do with the gcd:

\begin{exercise}
Let $a$ and $b$ be coprime integers. Show that
\begin{description}
\item[(i)] If  $a$ divides $c$ and $b$ divides $c$ then $ab$ divides $c$.
\item[(ii)] If $a$ divides $bc$ then $a$ divides $c$.
\end{description}
Show that both parts of this result can fail if $a$ and $b$ are not coprime.
\end{exercise}

\begin{solution}
\begin{description}
\item[(i)] Since $a$ and $b$ are coprime, we can find integers $s$ and $t$ so that $sa + tb = 1$. If $a$ divides $c$ then we can write $c = ax$, and if $b$ divides $c$ then we can write $c = by$ for some $x,y \in \Z$. Then $c = csa + ctb = (by)sa + (ax)tb = (ab)sy + (ab)xt$, so $ab $ divides $ c$.
\item[(ii)] As in (i), $c = csa + ctb$. If $a $ divides $bc$ then write $az = bc$ for some integer $z$, and then we have $c = csa + t(az) = a(cs + tz)$, so $a$ divides $c$.
\end{description}
\end{solution}

\begin{exercise}
Show that if $a, b$ are postive integers, and $d=gcd(a,b)$ then there exist positive integers $s, t$ such that $d=sa-tb$. {\bf Note:} that this exercise differs from Bezout’s  Identity, in that we ask $s, t$ to be \underline{positive}.

\begin{solution}
{\bf Claim}: If $a,b$ are positive integers and $d=gcd(a,b)$, we can find positive integers $s, t \in\mathbb{Z}$ such that $d=sa-tb$.

\begin{proof}  By the Euclidean algorithm, we can find integers $s,t\in\mathbb{Z}$, such that
\begin{equation}d=sa+tb\label{gcdeqn}\end{equation}
Since $0<d\leq a,b$, at most one of $s$ and $t$ can be positive, and at most one can be non-positive (i.e. zero or negative).  If it happens that $t$ is negative, then we will have produced the desired expression for $d$ (by writing $d=sa - (-t)b$).

In case $t$ is non-negative, we can apply the following observation:
\begin{lemma} The integers $s$ and $t$ satisfy Equation \eqref{gcdeqn} if, and only if, the integers $s+kb$ and $t-ka$ do as well, for any integer $k$.\end{lemma}
\begin{proof} We have $(s+kb)a + (t-ka)b = sa+tb$, simply by expanding the LHS, so one finds the same condition for both pairs.\end{proof}

Choosing $k$ sufficiently large, we can replace $t$ by $t-ka$ so that it is negative.  It follows then that $s+kb$ is positive.
\end{proof}

\end{solution}

\end{exercise}
%\item Find such positive integers $s, t$ in the following examples (from Exercise 10.1):
%\begin{itemize}
%\item $a=17, b=29$.
%\item $a=552, b=713$.
%\item $a=345, b=299$.
%\end{itemize}
%\end{itemize}

%

%\begin{solution}
%\begin{description}
%\item[(i)] Since $a$ and $b$ are coprime, we can find integers $s$ and $t$ so that $sa + tb = 1$. If $a$ divides $c$ then we can write $c = ax$, and if $b$ divides $c$ then we can write $c = by$ for some $x,y \in \Z$. Then $c = csa + ctb = (by)sa + (ax)tb = (ab)sy + (ab)xt$, so $ab $ divides $ c$.
%\item[(ii)] As in (i), $c = csa + ctb$. If $a $ divides $bc$ then write $az = bc$ for some integer $z$, and then we have $c = csa + t(az) = a(cs + tz)$, so $a$ divides $c$.
%\end{description}
%\end{solution}

\section{Corollaries of Bezout's Identity and the Linear Combination Lemma}%
\label{bezoutcorollaries}

Below we prove some useful corollaries using  \hyperref[t:lincombgcd]{ Bezout's Identity} (Theorem \ref{t:lincombgcd}) and \easylemma.

 \begin{corollary}
 \label{c:c|abc|b}
 Let $a,b,c\in\Z$. Suppose $c\neq
0$, $c$ divides $ab$ and $\gcd(a,c)=1$. Then $c$ divides $b$.
\end{corollary}

\begin{proof}
There are integers $s$ and $t$ such that $1=\gcd(c,a)=sa+tc$. This gives $b=sab+tcb$.  Then $c$ divides $b$ by \easylemma .
 \end{proof}

\begin{corollary} Let $a,c\in\Z$ and let $d\in\mathbb{Z}$ be so that $d$ divides $a$ and $d$ divides $c$.  Then $d$ divides $\gcd(a,c)$.
\end{corollary}

\begin{proof}
By  \hyperref[t:lincombgcd]{ Bezout's Identity}, there are integers $s$ and $t$ such that
$\gcd(a,c)=sa+tc$.
 Then as $d$ divides $a$ and $d$ divides $b$, we have $d$ divides $\gcd(a,c)$ by \easylemma.
\end{proof}

\begin{corollary} Let $d$ be a common divisor of
 $a$ and $c$, which is divisible by all divisors of $a$ and $c$. Then $d = \pm gcd(a,c)$.
\end{corollary}

\begin{proof}
By the previous corollary, $d$ divides $\gcd(a,c)$, and since $d$ is divisible by all divisors, it is divisible by $\gcd(a,b)$, so now we apply Corollary \ref{l:abba}.
\end{proof}

Here are some versions of the above corollaries when some of the numbers involved are primes:

\begin{corollary}
\label{c:p|ab}
If $a,b\in\mathbb{Z}$, $p$ is prime, and $p$ divides $ab$, then either $p$ divides $a$ or $p$ divides $b$ (or both).
\end{corollary}

This just follows from Corollary \ref{c:c|abc|b} by letting $c=p$.

%\begin{claim}
%If there are integers $m,n$ so that $ma+nb=1$, then $\gcd(a,b)=1$.
%\end{claim}
%
%\begin{proof}
%Recall $ma+nb=1$ for some $m,n$.   By Easy Lemma, $\gcd(a,b)$ divides $ma+nb=1$, only possible if $\gcd(a,b)=1$.
%\end{proof}
%This is an application of the Euclidean algorithm to get gcd as a linear combination, and then \easylemma .
 \begin{corollary}
 \label{c:p|p...p}
If $n=p_1^{m_1}\cdots p_k^{m_k}$, where each of $p_1,\dots,p_k$ is \emph{prime}, and if $p$ is a prime number that divides $n$, then $p=p_i$ for some $i=1,\ldots, k$.
\end{corollary}

\begin{proof}
We prove this by induction.
If $k=1$, then $p$ divides ${p_1}^{m_1}$, so by the previous corollary, $p=p_1$.
Assume that the theorem is true for $k=r$; let us prove it for $k=r+1$.
Assume that $p$ divides $p_{1}^{m_1}\cdots p_{r+1}^{m_{r+1}}$.
Again by the previous corollary, either $p$ divides $p_{k+1}^{m_{r+1}}$ or $p$ divides $p_1^{m_1}\cdots p_r^{m_r}$.
Thus, either $p=p_{r+1}$, or, by the induction hypothesis, $p=p_i$ for some $i=1,2,...,r$. This completes the proof.
\end{proof}

 Let's use these results to solve a simple diophantine equation.

 \begin{example}
 \label{ex:2x=5y}
 Find all integer solutions to $2x=5y$. \\

 Suppose $(x,y)$ are integers solving $2x=5y$. Then Corollary \ref{c:p|ab} implies $2$ divides $y$ and $5$ divides $x$, so $y=2z$ and $x=5w$ for some integers $z$ and $w$. Inserting these into the original equation, we get a new equation
 \[
 10w = 2(5w)=5(2z) = 10z.
 \]
 Thus, $w=z$. Hence, any solution $(x,y)$ must be of the form $(x,y)=(5w,2w)$ for some integer $w$. We can also see that each pair of integers of the form $(5w,2w)$ is a solution. Thus, the solutions to $2x=5y$ are exactly all pairs of integers $\{(5w,2w) :  w\in\mathbb{Z}\}$.

 \end{example}

\section{The Fundamental Theorem of Arithmetic (FTA)}%
\label{fundamentaltheoremofarithmetic}

The fundamental theorem of arithmetic says that all integers have a \emph{unique} factorization as a product of powers of prime numbers.

\begin{theorem}[The Fundamental Theorem of Arithmetic (FTA)] \label{t:FTA} Let $n\geq 2$ be an integer.
\begin{itemize}
\item (Existence) Then $n$ is equal to a product $n=p_1^{r_{1}}\cdots p_k^{r_{k}}$ of powers of prime numbers, where $p_1< \ldots < p_k$ and $r_{i}>0$ for all $i$.
\item (Uniqueness) The factorization is unique: If we also have
$$ p_1^{r_{1}}\cdots p_k^{r_{k}} = n = q_1^{s_1}\cdots q_\ell^{s_{\ell}}$$
where  $q_{1}<\cdots < q_{\ell}$ are primes and $s_{i}> 0$, then $k=\ell$, $p_i=q_i$, and $r_{i}=s_{i}$ for all $i$.
\end{itemize}
\end{theorem}
%If some of the $p_i$'s are repeated, we can collect them into powers, and write instead,
%$$n=p_1^{a_1}\cdots p_l^{a_l},$$
%with $p_1<\cdots < p_l$ all prime, and $a_i$ positive integers.

%
%\begin{frame}
%\frametitle{Match the claim to its proof (note: they are out order)}
% \begin{claim}[1 - existence of prime decomposition] Every integer $n$ can be written as a product of primes $n=p_1\cdots p_n$.\end{claim}
%
% \begin{claim}[2 -uniqueness of prime decomposition] Every integer $n$ can be written as a product of primes $n=p_1\cdots p_n$ in a \emph{unique} way.
%\end{claim}
%
% \begin{claim}[3]
%The $\operatorname{gcd}(a,b)$ is divisible by any common factor of $a$ and $b$.
%\end{claim}
%
% \begin{claim}[4]
%If $n=p_1\cdots p_n$ is a product of \emph{prime} numbers, and if $p$ prime divides $n$, then $p=p_i$ for some $i=1,\ldots, n$.
%\end{claim}
%
% \begin{claim}[5]
%If $a,b\in\mathbb{Z}$ are coprime, $p$ is prime, and divides $ab$, then either $p$ divides $a$ or $p$ divides $b$ (or both).
%\end{claim}
%
%
%\end{frame}

We will split the proof into three lemmas:

\begin{lemma}[Existence of prime decomposition, Part I]
Every integer $n\geq 2$ can be written as a product of primes $n=p_{1}\cdots p_{k}$.
\end{lemma}

{\bf Remark:} If $n$ is prime, this statement still makes sense: we just interpret $n$ as being the product of just one number, $n$ itself.

\begin{proof}
We prove by strong induction on $n$. The case $n=2$ immediately holds (taking into account the previous remark). For the induction step, suppose the theorem holds for all integers $n<N$.  If $N$ is prime, there is nothing to prove; otherwise, if $N$ is not prime, then $N=a\cdot b$ for some positive integers $a$ and $b$, both greater than $1$ and less than $N$. Both $a$ and $b$ can be  decomposed by the strong induction hypothesis, thus so can $n=ab$.
\end{proof}

\begin{lemma}[Existence of prime decomposition, Part II]
Every integer $n\geq 2$ can be written as a product of powers of primes $n=p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}$ where $p_{1}<\cdots < p_{k}$ and $r_{i}\geq 0$.
\end{lemma}

\begin{proof}
By the previous lemma, $n=q_{1}\cdots q_\ell$ for some primes $q_{1}\cdots q_{\ell}$ that are not necessarily distinct. If $p_{1}<p_{2}<\cdots < p_{k}$ are the {\it distinct} primes that appear in the list $q_{1},...,q_{\ell}$, let $r_{i}$ denote the number of times that $p_{i}$ appears in the list. Then
\[
n=q_{1}\cdots q_{\ell} = p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}\]
which proves the lemma.
\end{proof}

\begin{lemma}[Uniqueness of prime decomposition] Every integer $n$ can be written as a {\it unique} product of powers of primes $n=p_1^{r_{1}}\cdots p_k^{r_{k}}$.
\end{lemma}

  \begin{proof}
  Suppose $p_1^{r_{1}}\cdots p_k^{r_{k}} = n = q_1^{s_{1}}\cdots q_\ell^{s_{\ell}}$ are two decompositions. By cancelling any common factors, we can assume that no $p_i$ equals any $q_j$. If there are  any $p_i's$ and $q_j's$ remaining,  Corollary \ref{c:p|p...p} implies each $p_{i}$ equals some $q_j$, which is a contradiction, thus there can be no terms remaining, so the two factorizations must have been equal.
  \end{proof}

Let's use the FTA to solve another simple-looking diophantine equation.

\begin{exercise}
Find all integer solutions to $x^{2}=y^{5}$. \\

\begin{solution}
We want to apply the FTA to $x$ and $y$, but this only works when $x,y\geq 2$. Note that if $(x,y)$ is a solution, then so is $(-x,y)$, so we just need to find all solutions with $x\geq 0$. The only solutions with $x=0$ is $(0,0)$ and the only solution with $x=1$ or $y=1$ is $(1,1)$. Thus, we have narrowed things down to just finding all solutions $(x,y)$ with $x,y\geq 2$.

By the FTA, there are primes $p_{1}<\cdots <p_{k}$ and $q_{1}<\cdots <q_{\ell}$ and $r_{i},s_{j}\in \mathbb{N}$ so that
\[
x=p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}\;\;\; \mbox{ and } \;\;\; y= q_1^{s_{1}}\cdots q_\ell^{s_{\ell}}.\]
Plugging this into $x^2=y^5$, we get
\[
p_{1}^{2r_{1}}\cdots p_{k}^{2r_{k}} = q_1^{5s_{1}}\cdots q_\ell^{5s_{\ell}}.
\]
The FTA says these two factorizations must equal, so $k=\ell$, $p_i=q_i$, and $2r_{i}=5s_{i}$ for $1\leq i\leq k$. By Example \ref{ex:2x=5y}, this means $r_{i}=5w_{i}$ and $s_{i}=2w_{i}$ for some integer $w_{i}$. Thus,
\[
x=p_{1}^{5w_{1}}\cdots p_{k}^{5w_{k}}= \left(p_{1}^{w_{1}}\cdots p_{k}^{w_{k}}\right)^{5} \;\;\; \mbox{ and } \;\;\;
y
=p_1^{2w_{1}}\cdots p_\ell^{2w_{\ell}} = \left(p_1^{w_{1}}\cdots p_\ell^{w_{\ell}}\right)^2.
\]
If we let $z=p_1^{w_{1}}\cdots p_\ell^{w_{\ell}}$, we see that $(x,y)=(z^{5},z^{2})$. Thus, we have shown that all integer solutions $x,y\geq 2$ are of the form $(x,y)=(z^{5},z^{2})$ for some $z\in\mathbb{N}$. Recalling our reductions, all solutions are either $(0,0)$, $(\pm 1,1)$, and $(\pm z^5,z^2)$ for $z\in\mathbb{N}$. More succinctly, all solutions must be of the form $( z^5,z^2)$ for some $z\in\mathbb{Z}$. One can also easily check that every pair in this set is also a solution to $x^2=y^5$, thus $S$ is {\it exactly} the set of solutions.
\end{solution}
\end{exercise}

\begin{protip}
{\bf Reductions:} Look for ways of simplifying your problem from the start by reducing the number of cases you have to investigate. The next problem is a good example of how to find reductions.
\end{protip}

\begin{example}
Find all integer solutions to $x^2-y^2=91$. \\

\noindent {\bf Reductions:} Let's make a few observations first to narrow down what to solve for.
\begin{itemize}
\item Notice that if $(x,y)$ is a solution, then so is $(\pm x,\pm y)$, and so we can assume that $x,y\geq 0$, since then the other solutions will be of the form $(\pm x,\pm y)$.
\item Moreover, we can't have $x=y$ since then the equation is not satisfied, so assume $x\neq y$. We can also assume $x>y$, since $x<y$ would imply $x^2<y^2$, so $x^2-y^2<0<51$.
\item Finally, we can't have either $x$ or $y$ equal to zero, since $91$ is not a perfect square.
\end{itemize}
Thus, after these reductions, we can assume $x>y>0$.\\

Note that $91$ has prime factorization $91=7\cdot 13 $, and so
\[
91=7\cdot 13 = x^2-y^2=(x-y)(x+y).\]
Since $x+y>x-y$, there are two cases to consider:
\begin{itemize}
\item If $x+y=91$ and $x-y=1$, solving these linear equations simultaneously  gives $x=46$ and $y=45$.
\item If $x+y=13$ and $x-y=7$, solving these two equations gives $x=10$ and $y=3$.
\end{itemize}
Thus, all the {\it positive} solutions $(x,y)$ are $(46,45)$ and $(10,3)$. Thus, recalling our reductions, {\it all} solutions are just $(\pm 46, \pm 45),$ and $(\pm 10,\pm 3)$ (where we range over all possible combinations of $\pm$ for a total of $8$ solutions).

\end{example}

\section{Finding divisors via FTA}%
\label{findingdivisors}
If we know the prime decompositions of two integers $m$ and $n$, it is easy to tell whether $m$ divides $n$:

\begin{theorem}
Let $n=p_1^{a_1}\cdots p_k^{a_k}$ be a prime decomposition (i.e. $p_i$s are prime, $p_1<\cdots <p_k$, and $a_i>0$). Then $m$ divides $n$ if, and only if:
\begin{equation}
\label{e:m=p1...pk}
m = p_1^{b_1}\cdots p_k^{b_k}, \quad \textrm{with each $0\leq b_i\leq a_i$}.
\end{equation}
\end{theorem}

\begin{proof}
\begin{itemize}
\item  ($\impliedby$): If \eqref{e:m=p1...pk} holds, then
\[
n=m \cdot  p_1^{b_1-a_{1}}\cdots p_k^{b_k-a_{k}} \;\;\; \Longrightarrow \;\;\; m \text{ divides }n.
\]
 \item ($\implies$): Suppose $m$ divides $n$, then $n=mc$ fr some positive integer $c$.  Then $m$ and $n$ have prime decompositions whose product is the prime decomposition for $n$ as shown below:
$$\underbrace{p_1^{a_1}\cdots p_k^{a_k}}_n = \underbrace{q_1^{c_1}\cdots q_l^{c_l}}_m\underbrace{r_1^{d_1}\cdots r_s^{d_s}}_c.$$
The FTA implies each $q_i$ and $r_i$ equals to some $p_j$. To ease notation, we'll assume $q_{i}=r_{i}=p_{i}$ for all $i$, but that $c_{i}=0$ if $p_{i}$ didn't appear as one of the primes $q_{i}$ originally, and similarly for the $d_{i}$. Then the product above is in fact
$$\underbrace{p_1^{a_1}\cdots p_k^{a_k}}_n = {q_1^{c_1}\cdots q_l^{c_l}}{r_1^{d_1}\cdots r_s^{d_s}}
=p_{1}^{c_{1}+d_{1}}\cdots p_{k}^{c_{k}+d_{k}}.
$$

The FTA now implies each power $c_i+d_i$ equals to $a_j$. In particular, this means $m=p_{1}^{c_{1}}\cdots p_{k}^{c_{k}}$ where $0\leq c_{k}\leq a_{k}$.
  \end{itemize}
\end{proof}

\begin{example}
What are all the divisors of $360$?\\

First, we find the prime factorization of $360$: we can see that $36=4\cdot 9=2^2\cdot 3^2$, and so $360 = 10\cdot 2^2\cdot 3^2 = 2^3\cdot 3^2\cdot 5$. To list all the divisors, we just have to look at the values $2^j\cdot 3^{k}\cdot 5^{\ell}$ where $0\leq j\leq 3$, $0\leq k\leq 2$, and $0\leq \ell\leq 1$. The possible powers of $2$ are $1, \; 2, \; 4, \; 8,$ and the possible powers of $3$ are $1,3,9$, so we multiply the powers of two by these to get

\[
\begin{array}{cccc}
1, &  2, &  4, &  8, \\
3, & 6, &  12, &  24, \\
 9, &  18, &  36, &  72.
 \end{array}
 \]

 The possible powers of $5$ are just $1$ and $5$, so we can just multiply these numbers by $1$ and $5$ to get
 \[
 \begin{array}{cccccccccccc}
1, &  2, &  4, &  8,
& 3, & 6, &  12, &  24,
&  9, &  18, &  36, &  72, \\
5, &  10, &  20, &  40,
& 15, & 30, & 60,  &  120,
&  45, &  90, &  180, &  360.
 \end{array}
 \]
\end{example}

\section{LCM and GCD via prime factorizations}%
\label{lcmandgcd}

\def\lcm{\rm lcm}
\begin{definition}
The least common multiple $\lcm(a,b)$ of positive integers $a$ and $b$ is the smallest positive integer divisible by both $a$ and $b$.
\end{definition}

For example, $\lcm(15,12)=60$.

 \begin{theorem}
 \label{t:lcm}
Let $a$ and $b$ have prime factorizations,
$$a=p_1^{r_1}\cdots p_m^{r_m}, \;\; b=p_1^{s_1}\cdots p_m^{s_m}.$$  Here $p_i$'s are distinct, but $r_i$ and $s_i$ are allowed to be zero.  Then:
\begin{itemize}
 \item $\gcd(a,b) = p_1^{min(r_1,s_1)}\cdots p_m^{min(r_m,s_m)}$.
 \item $\lcm(a,b) = p_1^{max(r_1,s_1)}\cdots p_m^{max(r_m,s_m)}$.
 \item $\lcm(a,b) = ab/\gcd(a,b)$.
\end{itemize}
\end{theorem}

We leave the proof of this as an exercise.

\begin{example}
If $a=120=2^3\cdot 3\cdot 5$ and $b=36=2^2\cdot 3^2$, then
$\gcd=2^2\cdot 3=12$ and $\lcm=2^3\cdot 3^2 \cdot 5=360=\frac{120\cdot 36}{12}$,
\end{example}

\begin{lemma}
\label{lem:rationalsascoprimefractions}
Let $r \in \mathbb{Q}$.
Then there exist unique coprime integers $a$ and $b$ such that $r = \frac{a}{b}$ and $b \geq 1$.
\end{lemma}

\begin{proof}
Since $r$ is rational, there are integers $a'$ and $b'$ such that $r = \frac{a'}{b'}$, with $b' \neq 0$.
Without loss of generality, we may assume that $b'>0$.
If we find $-r = \frac{a}{b}$ with $a$ and $b$ coprime, then $r = \frac{-a}{b}$ with $-a$ and $b$ coprime, so we may assume that $r\geq 0$.

Now let $p_1, \cdots, p_k$ be the prime numbers that divide either $a'$ or $b'$. We may therefore write
\[
a' = p_1^{m_1}\cdots p_k^{m_k} \text{\quad and \quad} b' = q_1^{n_1}\cdots q_k^{n_k},
\]
where each exponent $m_i$ and $n_i$ is nonnegative.
Now for each $i$, let
\[
s_i = \max(m_i-n_i, 0)\text{\quad and \quad}t_i = \max(n_i-m_i, 0).
\]
Thus for each $i$, at most one of $s_i$ and $t_i$ is nonzero, and
\[
\frac{p_i^{m_i}}{q_i^{n_i}} = \frac{p_i^{s_i}}{q_i^{t_i}}.
\]
Thus we set
\[
a = p_1^{s_1}\cdots p_r^{s_k} \text{\quad and \quad} b = q_1^{t_1}\cdots q_k^{t_k},
\]
and we find that
\[
r = \frac{a'}{b'} = \frac{a}{b}.
\]
Now
\[
\gcd(a,b) = p_1^{\min(s_1,t_1)}\cdots p_r^{\min(s_k,t_k)} = p_1^0\cdots p_r^0 = 1. \qedhere
\]

Now let us prove that $a$ and $b$ are unique with this property.
If $c$ and $d$ are coprime integers with $d \geq 1$, and if $r = \frac{c}{d}$, then $ad = bc$. Since $a$ and $b$ are coprime, it follows that $b$ divides $d$, and since $c$ and $d$ are coprime, it follows that $d$ divides $b$. Thus $d=b$, and so $a=c$ as well.
\end{proof}

%
%
%\begin{example} What is the smallest positive integer that can be written in the form $375a + 147b$ where $a$ and $b$ are integers?
%\end{example}
%
%This was a question on University Challenge \href{https://www.youtube.com/watch?v=YoVvcMAV2YU#t=703}{(S43E28 Queen's, Belfast vs Southampton)}. As you can see from the video, no one got it right. But now you'll be ready!
%
%Let's find the prime factorizations of $375$ and $147$. We can just keep dividing by numbers we think divide:
%\[
%375 = 3\cdot 125=3\cdot 5^{3}, \;\; 147 = 7\cdot 21 = 7^2\cdot 3.
%\]
%Now we can see that $\gcd(375,147)=3$. By Bezout’s  Identity, we can find $a,b\in\mathbb{Z}$ so that $375a+147b=3$. We cannot make this number any smaller with different choices of $a,b$ by \easylemma, thus $3$ is the smallest such number I can express as $375a+147b$.
%
%

\section{Powers}%
\label{powersofintegers}

\begin{theorem}
\label{t:perfectsquare}
 Let $n$ be a positive integer.  Then $\sqrt{n}\in\mathbb{Q}$ if and only if $n$ is a perfect square, that is, $n=m^2$ for some integer $m$.
 \end{theorem}

\begin{proof}
Suppose $\sqrt{n}\in\mathbb{Q}$.
Thus $\sqrt{n} = \frac{a}{b}$, and $\frac{a^2}{b^2} = n$.
By Lemma \ref{lem:rationalsascoprimefractions}, we may assume that $a$ and $b$ are coprime and positive.
Now since $n = \frac{n}{1}$, the uniqueness of Lemma \ref{lem:rationalsascoprimefractions} implies that $b^2=1$.
Since $b$ is positive, it follows that $b = 1$.
\end{proof}

Below, we say an integer $a$ is an {\it $n$th power} if $a=b^n$ for some integer $b$.

\begin{theorem}
\label{t:abn}
If $a,b\in\mathbb{N}$ are coprime, and $ab$ is an $n$th power, then so are $a$ and $b$.
\end{theorem}

\begin{proof}
 If $a=1$, then we have $ab=b$ is an $n$th power, so the theorem is trivial in this case, and similarly if $b=1$, so we can assume $a,b \geq  2$, so we can apply the FTA.

Let $a=p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}$ and $b=q_{1}^{s_{1}}\cdots q_{\ell}^{s_{\ell}}$ be the prime factorizations of $a$ and $b$. Since $a$ and $b$ are coprime, they share no common prime factors, so $p_{i}\neq q_{j}$ for all $j$.  By assumption $ab=c^n$ for some $c$. Let $w_{1}^{t_{1}}\cdots w_{j}^{t_{j}}$ be the prime facorization for $c$, then
\[
ab = p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}q_{1}^{s_{1}}\cdots q_{\ell}^{s_{\ell}}
=c^n=w_{1}^{nt_{1}}\cdots w_{j}^{nt_{j}}.
\]
By the FTA,  these prime factorizations are equal, which means for each $i$,  $p_{i}=w_{j}$ for some $j$, and $r_{i}=nt_{j}$. In particular, $n$ divides $r_{i}$ for all $i$, and so $a$ is an $n$th power. The same holds for $b$.
\end{proof}

Let's do an example of another diophantine equation. This one is from Liebeck (but is not proven correctly there).

\begin{example}
Find all integer solutions to  $4x^2=y^3+1$. \\

\noindent {\bf Reductions:} If $(x,y)$ is a solution,  then $(-x,y)$ is also a solution, so we can assume $x\geq 0$. If $x=0$, then $y^3=-1$, which is only possible if $y=-1$. So now we can assume $x>0$. But then $y^3+1=4x^2\geq 4$, so we must have $y>0$ as well.\\

\noindent We are now left to finding all integer solutions $x,y>0$. Observe that
\[
y^3=4x^2-1=(2x-1)(2x+1).
\]
By The Linear Combination Lemma, the gcd of $2x-1$ and $2x+1$ must divide $2x+1-(2x-1)=2$, so the gcd is either $1$ or $2$. However, since both of these numbers are odd, the gcd must actually be $1$. Hence, $2x\pm 1$ are coprime.

By Theorem \ref{t:abn}, $2x\pm 1$ are both cubes. Thus, there are $m,n\in\mathbb{Z}$ so that $m^3=2x+1$ and $n^3=2x-1$. Since $x$ is a positive integer, so are $m^3$ and $n^3$, and thus so are $m $ and $n$.  Then
\[
2=2x+1-(2x-1)=m^3-n^3=(m-n)(m^2+mn+n^2),\]
so $m-n$ divides $2$, hence it is either $1$ or $2$ (since $m>n$). If it is $1$, then $m=n+1$, and so by the above equation
\[
2=m^3-n^3=(n+1)^3-n^3=3n^2+3n+1\geq 3\cdot 1^2+3\cdot 1 + 1 = 7\]
which is a contradiction. The case that $m-n=1$ can be handled similarly. Thus, there are {\it no integer solutions} to $4x^2=y^3+1$ when $x>0$. Thus, the only integer solution is $(x,y)=(0,-1)$.

\end{example}

 \section{Application: Pythagorean Triples*}%
 \label{pythagoreantriples}

In this section we will classify all Pythagorean Triples, that is, all positive integer solutions to

\[
x^2+y^2=z^2.
\]

This is a bit more involved than other diophantine problems and harder to figure out on your own on a homework problem (so it is not required reading), but we'll give a proof here since it's a neat application of what we've learned and can test your knowledge of the material in this chapter. \\

We narrow down the solutions in a few steps:

\begin{itemize}
\item If $x=0$, then we must have $y^2=z^2$, and so $y=\pm z$. Thus, we know all solutions if $x=0$, so let's assume $x> 0$. Similarly, we can assume $y,z> 0$.
\item We can assume that $x,y$ and $z$ are coprime (that is, no two of them share a common factor other than $1$). To see this, suppose $d=\gcd(x,y)$. Then $a=x/d$ and $b=y/d$ are coprime, and then
\[
z^2= x^2+y^2 = d^2(a^2+b^2).
\]
By Theorem \ref{t:abn}, $a^2+b^2=c^2$ for some integer $c$, and so $z^2=d^2 c^2=(cd)^2$. Hence, $(x,y,z)=(da,db,dc)$ for some other Pythagorean Triple $(a,b,c)$ where $a$ and $b$ are coprime.  Thus, if we find all solutions $(x,y,z)$ where $x$ and $y$ are coprime, then all other solutions are multiples of these. A similar proof shows that all solutions will be multiples of solutions where $x$ and $z$ are coprime and where $y$ and $z$ are coprime.
\item Thus, assume $x,y,$ and $z$ are coprime and positive solutions to $x^2+y^2=z^2$. Then either $x$ or $y$ is odd, assume it is $x$.
\item We now claim $z+y$ and $z-y$ are coprime. Suppose not. Then there is a prime $q$ that divides them both. But then
\[
q\text{ divides }(z+y)-(z-y) = 2y,\;\; q\text{ divides } (z+y)+(z-y) = 2z.
\]
Since $y$ and $z$ are coprime, $q=2$. But then
\[
2\text{ divides }(z-y)(z+y)=z^{2}-y^{2}=x^{2}
\]
which implies $x$ is even, a contradiction.
\item Since $z\pm y$ are coprime and $x^{2}=(z+y)(z-y)$, we know that $z+y=s^{2}$ and $z-y=t^{2}$ for some integers $s$ and $t$ by Theorem \ref{t:abn}. Hence,
\[
z=\frac{z+y+z-y}{2} = \frac{s^{2}+t^{2}}{2}
\]
Similarly, $y=\frac{s^{2}-t^{2}}{2}$, and finally,
\[
x^{2} =(z-y)(z+y) = s^{2}t^{2}.
\]
Thus, all positive coprime solutions with $x$ odd are of the form
\[
(x,y,z) = \left( st, \frac{s^2-t^2}{2}, \frac{s^{2}+t^{2}}{2}\right).
\]
\item Finally, we now recall that all solutions are multiples of these soluitions, thus all Pythagorean Triples are of the form
\[
(x,y,z) = \left( as t, a\frac{s^2-t^2}{2}, a\frac{s^{2}+t^{2}}{2}\right)
\]
where $a,s,t$ are integers.
\end{itemize}

\section{Exercises}%
\label{numbertheoryexercises}

The relevant exercises in Liebeck's book are in Chapters 10 and 11.

\begin{exercise} Show that $n$ and $n^2+n+1$ are coprime.

\begin{solution}
Let $d=\gcd(n,n^2+n+1)$. By The Linear Combination Lemma,
\[
d\text{ divides} n^2+n+1 - n\cdot n = n+1
\]
and so we also have $d$ divides $ n+1 - n =1$, thus $n$ and $n^2+n+1$ are coprime.
\end{solution}
\end{exercise}

\begin{exercise} Find all $n\in\mathbb{N}$ so that $n-2$ divides $n^2-2$.

\begin{solution}
Note that $n-2$ divides $n^2-4$, so The Linear Combination Lemma implies
\[
n-2\text{ divides } (n^2-2-(n^2-4))=2.
\]
Thus $n-2$ must be $\pm 1$ or $\pm 2$, and this is only possible if $n=1,3$ or $4$.
\end{solution}

\end{exercise}

\begin{exercise} Show that $\gcd(n!+1,(n+1)!)$ is either $1$ or $n+1$ for all $n\in\mathbb{N}$.

\begin{solution}
Let $d=\gcd(n!+1,(n+1)!)$. By The Linear Combination Lemma,
\[
d \text{ divides } (n+1)(n!+1)-(n+1)! = n+1
\]
If $d>1$ and $k<n+1$ is such that $d|k$, then  by The Linear Combination Lemma
\[
d \text{ divides } n!+1- k\cdot \frac{n!}{k} =1
\]
a contradiction, thus $d$ cannot divide any number less than $n+1$ bigger than $1$, so $d$ is either $1$ or $n+1$.
\end{solution}

\end{exercise}

Below, if an exercise just states an equation, find all integer solutions to that equation.

\begin{exercise}
 $x^2=16y^2+8y+2$

\begin{solution}
First note that if $(x,y)$ is a solution, so is $(-x,y)$, so we can assume first that $x\geq 0$.Note that $x^2=16y^2+8y+2$ implies
\[
x^2-1 =16y^2+8y+1=(4y+1)^2
\]
Note that this implies $x\neq 0$, thus $x>0$. Note that $x^2-1=(x-1)(x+1)$, so their gcd is at most $2$, but $x^2-1=(4y+1)^2$ which is odd, thus $x\pm 1$ are coprime. Thus, they are perfect squares that differ by $2$, so $x+1=m^2$ and $x-1=n^2$ (since $x>0$, both $x\pm 1\geq 0$). We can assume  $m>n\geq 0$. But then
\[
2= x+1-(x-1) = m^2-n^2 = (m-n)(m+n),
\]
so we must have that $m-n=1$ and $m+n=2$ (since $m-n\leq m+n$), which has no integer solutions.
\end{solution}
\end{exercise}

\begin{exercise}
 $x^2+2y^2=8z+5$.

\begin{solution}

{\bf Claim:} There are no integer solutions to $x^2+2y^2=8z+5$.

\begin{proof}
Suppose $(x,y,z)$ was an integer solution. Notice that $8z+5$ is odd, so $x^2+2y^2$ is odd as well, thus $x^2$ is odd, and so $x$ is odd. Thus, $x=2n+1$ for some integer $n$. Plugging that back into the above, we get
\[
8z+5 =x^2+2y^2 = (2n+1)^2 +2y^2 = 4n^2 +4n+1 + 2y^2\]
and so
\[
8z+4 = 4n^2+4n+2y^2.\]
Dividing both sides by $2$, we get
\[
4z+2=2n^2+2n+y^3.\]
In particular, $y^3$ must be even, so $y$ is even, hence $y=2m$ for some $m$, so we get
\[
4z+2-2n^2-2n=y^3=8m^3
\]
and dividing through by $2$ gives
\[
2z+1-n^2-n = 4m^3.\]
But notice that $2z+1-n^2-n = 2z-n(n+1)+1$ is odd, because $n(n+1)$ is even and so is $2z$, but $4m^3$ is even, and we get a contradiction. Thus, there are no solutions.
\end{proof}
\end{solution}
\end{exercise}

\begin{exercise}
$x^2 = y^3$.

\begin{solution}

{\bf Claim:} We have the following:
The integer solutions of $x^2=y^3$ are of the form $x=n^3$, $y=n^2$ for some integer $n$.

\begin{proof}
 It is clear to see that letting $x=n^3, y=n^2$ for any integer $n$ solves the equation.  So we need to show that this is in fact necessary, not only sufficient.

Let $(x,y)$ be a solution to $x^2=y^3$. Note that if $(x,y)$ is a solution, then so is $(-x,y)$, and moreover, $y\geq 0$ since $y^3=x^2\geq 0$. Thus, we can assume for the moment that $x,y\geq 0$, since if  $(x,y)$ is a solution with $x<0$, then $(-x,y)$ will be a positive solution.

Since $x,y\geq 0$, we can look at their prime factorizations. Let us write $x=p_1^{k_1}\cdots p_n^{k_n}$ and $y=q_1^{l_1}\cdots q_m^{l_m}$ for the prime factorizations for $x$ and $y$ respectively, (so we assume $p_i<p_j$ for $i<j$, and $q_r<q_s$ for $r<s$).  Then we have:
$$p_1^{2k_1}\cdots p_n^{2k_n} = x^2=y^3 = q_1^{3l_1}\cdots q_m^{3l_m}.$$
Hence, applying the Fundamental Theorem of Arithmetic to $x^2$, we have $m=n$, $p_i=q_i$, for each $i$, and moreover,
$$2k_i=3l_i,$$
for each $i$.  Since 2 and 3 are coprime, Proposition 10.5 implies that $k_i$ is divisible by 3 and $l_i$ is divisible by 2.  Let $k_i'=\frac{k_i}{3}$.  Let $n:= p_1^{k_1'}\cdots p_n^{k_n'}.$  Then $x=n^3$ and $y=n^2$, as claimed. That is, if $(x,y)$ is a solution with $x,y\geq 0$, then $(x,y)=(n^3,n^2)$ for some $n\geq 0$. Thus, if $(x,y)$ is {\it any } solution, then $(x,y)=(\pm n^3,n^2)$ for some $n\geq 0$, or alternatively, $(x,y)=(n^3,n^2)$ for some $n\in \mathbb{Z}$.
\end{proof}

\end{solution}

\end{exercise}
%
%\begin{exercise}
%$x^2-x=y^3$.
%
%
%\begin{solution}
%
%
%{\bf Claim:} The integer solutions of $x^2-x=y^3$ are $x=0,y=0$ and $x=1,y=0$.
%
%\begin{proof}
%We may factorize the right hand side as $x(x-1)$.  As $1 = x - (x-1)$ we have $gcd(x,x-1)\leq 1$, hence equals one, and so they are coprime.  Their product is the cube $y^3$ of an integer, and hence by Proposition 11.4, both $x$ and $x-1$ must be cubes.  This happens if and only if $x=0$ or $x=1$; in either case, the equation then implies $y=0$.
%\end{proof}
%\end{solution}
%\end{exercise}

%
%\begin{exercise}
%$x^2=y^4-77$.
%
%
%
%\begin{solution}
%
%  {\bf Claim:} The integer solutions of $x^2=y^4-77$ are $x=\pm 2, y=\pm 3$ (all four possible combinations are solutions).
%
%
%\begin{proof}
% We may rewrite the equation as:
%$$77=y^4-x^2 = (y^2-x)(y^2+x).$$
%As 77 has a prime factorization $77=7\cdot 11$, we know that exactly one of the following eight situations is true:
%$$y^2-x = 7, \textrm{ and } y^2+x=11,$$
%$$y^2-x = -7, \textrm{ and } y^2+x=-11,$$
%$$y^2-x = 11, \textrm{ and } y^2+x=7,$$
%$$y^2-x = -11, \textrm{ and } y^2+x=-7.$$
%
%$$y^2-x = 1, \textrm{ and } y^2+x=77,$$
%$$y^2-x = -1, \textrm{ and } y^2+x=-77,$$
%$$y^2-x = 77, \textrm{ and } y^2+x=1,$$
%$$y^2-x = -77, \textrm{ and } y^2+x=-1.$$
%
%In the second and fourth cases, we can add the two necessary equations to find $y^2=-18$, which clearly has no real, let alone integer solutions.  In the first case, we solve for $x$ to find $x=2$, hence $y=\pm 3$, and in the third case we find $x=-2, y=\pm3$.  Combining these cases gives the claimed result.
%
%In any of the situations 5-8, we can see that $y^2 = 78$ or $y^2=76$, neither of which have integer solutions in $y$.
%\end{proof}
%
%\end{solution}
%\end{exercise}

\begin{exercise}
 $x^3-y^3=7$.

\begin{solution}
{\bf Claim:} The only solutions $(x,y)$ are $(2,1)$ and $(-1,-2)$.

\begin{proof}
Suppose $(x,y)$ is a solution. First note that this implies $x^3>y^3$, and so $x>y$. Furthermore, we immediately see that $x=0$ and $y=0$ don't lead to solutions, so we can assume that $x\neq 0 \neq y$ as well.

Factoring, we see that
\[
7 = x^3-y^3= (x-y)(x^2+xy+y^2)
\]
Thus, $x-y$ divides $ 7$, and since $x>y$, this means $x-y>0$, so the only possibilities then are $x-y=1$ or $x-y=7$. We split into two cases:

\begin{enumerate}[label=(\alph*)]
\item $x-y=1$. Then we have that $x=y+1$, and so
\[
7=x^3-y^3= (y+1)^3-y^3 = 3y^2+3y+1,\]
thus $6=3y^2+y$, so $2=y^2+y=y(y+1)$, so we see that $y$ divides $2$ and $y+1$ divides $2$. The only way this is possible is if either $y=1$ or $y=-2$. By plugging these values into the original equation, we see that the solutions are in this case $(x,y)=(2,1)$ and $(-1,-2)$.
\item Suppose $x-y=7$. Then
\[
7=(x-y)(x^2+xy+y^2)=7(x^2+xy+y^2)
\]
implies
\[
1=(x^2+xy+y^2).
\]
Note that if $|x|\geq |y|$, then $xy\geq -|x|\cdot |y|\geq -|x|^2=-x^2$, and so the above is at least
\[
1\geq x^2-x^2=y^2>0\]
since we are assuming $y\neq 0$. Hence, $y^2=1$, so $y=\pm 1$. Again, we can plug these into our original equation $7=x^3-y^3$ and we find that $y=-1$ leads to no solution and $y=1$ leads to $(x,y)=(2,1)$, which we already found.

Thus, the solutions are just $(x,y)=(2,1)$ and $(-1,-2)$.
\end{enumerate}

\end{proof}

\end{solution}
\end{exercise}

\begin{exercise}
$xy= x+y+2y^2-1$.

\begin{solution}

{\bf Claim:} The only solutions $(x,y)$ are $(1,0)$, $(0,-1)$, $(9,2)$, and $(10,3)$.

\begin{proof}

First, let's rearrange and factor:
\[
2y^2=xy-x-y+1 = (x-1)(y-1)
\]
and since $\gcd(y,y-1)=1$, we know $y-1$ divides $2$, thus $y-1=-1,1,-2,$ or $2$, that is, $y=0,2,-1, $ or $3$. We can now just try al these values out to see what $x$ should be:
\begin{itemize}
\item[$y=0$:] In this case, $0=(x-1)(0-1)=1-x$, i.e. $x=1$.
\item[$y=-1$:] In this case, $2=2y^2=(x-1)(y-1)=-2(x-1)$, so $x=0$.
\item[$y=2$:] We see that $8=2y^2 = (x-1)(y-1)=x-1$, and so $x=9$, and the solution in this case is $(x,y)=(9,2)$.
\item[$y=3$:] We see that $18=2y^2 = (x-1)(y-1)=2(x-1)$, and so $x=10$, and the solution in this case is $(x,y)=(10,3)$.
\end{itemize}
\end{proof}
\end{solution}

\end{exercise}

\begin{exercise}
 $x^3-x=12y+6$.

\begin{solution}

\begin{proof}
Suppose $(x,y)$ is a solution, then
\[
6(2y+1) = x^3-x = (x+1)x(x-1).
\]
Note that $\gcd(x+1,x)=\gcd(x,x-1)=1$, and $\gcd(x-1,x+1)$ divides $2$, so it is either $1$ or $2$. Note that since $2$ does not divide $2y+1$, the prime factorization of $(x+1)x(x-1)$ contains exactly one $2$. In particular, $x-1$ and $x+1$ can't both be even, so the only even number is $x$, hence $\gcd(x-1,x+1)=1$. Since $x$ only has one 2 in its prime factorization, $\frac{x}{2}$ is an integer and $x\pm 1$ and $\frac{x}{2}$ are all mutually coprime and
\[
3(2y+1)=(x+1)\frac{x}{2}(x-1).
\]

Thus, by Theorem \ref{t:abn},

\end{proof}

\end{solution}

\end{exercise}

\begin{exercise}
$xy+2x+3y=4$.

\begin{solution}
Write
\[
4=xy+2x+3y =(x+3)(y+2)-6
\]
So now we have
\[
(x+3)(y+2)=10=2\cdot 5=10\cdot 1\]
and so the only way this can happen is if either
\begin{itemize}
\item $y+2=2$, $x+3=5$ (which implies $y=0$ and $x=2$)
\item $y+2=-2$, $x+3=-5$ (so $y=-4$ and $x=-8$)
\item $y+2=5$, $x+3=2$ (so $y=3$ and $x=-1$)
\item $y+2=-5$, $x+3=-2$ (so $y=-7)$ and $x=-5$.
\item $y+2=10$ and $x+3=1$ (so $(x,y)=(-2,8))$
\item $y+2=-10$ and $x+3=-1$ (so $(x,y)=(-4,-12))$
\item $y+2=1$ and $x+3=10$ (so $(x,y)=(7,-1)$)
\item $y+2=-1$ and $x+3=-10$ (so $(x,y)=(-13,-3)$)
\end{itemize}
So the above pairs of $(x,y)$ are the only solutions.
\end{solution}

\end{exercise}

\begin{exercise}
 $6x^2=5x^3$.

\end{exercise}

\begin{exercise} {\bf Challenging:} Find all integer solutions to the equation $x^3+3y^3+9z^3= 0$.

\begin{solution}
{\bf Claim:} $x^3+3y^3+9z^3= 0$ does not have any solutions in positive integers.

\begin{proof}
Assume that $(x, y, z)$ is a solution in positive integers. Clearly, $x$ is divisible by $3$, so $x= 3x_1$ for some positive integer $x_1$. But then $27x_{1}^{3}+3y^3+9z^3= 0$,hence $9x_1^3+y^3+3z^3= 0$. Now $y= 3y_1$, and we find $3x_1^3+9y^{3}_{1}+z^3= 0$. Finally, $z= 3z_1$ for some positive integer $z_1$, and $x_{1}^{3}+ 3y_{1}^{3}+ 9z_{1}^{3}= 0$.Thus if $(x, y, z)$ is a, integer solution of the equation $x^3+3y^3+9z^3= 0$, then so is $(x/3,y/3,z/3)$. Repeating this argument we find that for every positive solution there is a smaller solution in positive integers: but this is nonsense, thus there is no solution in positive integers.
\end{proof}
\end{solution}

\end{exercise}

%
%\item $x^3=4y^2+4y-3$.
%
%\begin{solution}
%{\bf Claim:} There are no integer solutions of $x^3=4y^2+4y-3$.
%
%
%\begin{proof}
%
%
% We can factorize the RHS as $(2y+3)(2y-1)$.  As the two factors differ by four, their highest common factor is a divisor of four.  However, they are both odd, so their highest common factor is odd, hence their highest common factor is one, and they are coprime.  Now applying Proposition 11.4, we have that each must be a perfect cube.  However, there are no pairs of perfect cubes which differ by four:
%
%
%%
%%{\bf Claim:} For any integers $m,n$, $m^3-n^3\neq 4$.
%%
%%
%%\begin{proof}
%%Suppose for the sake of contradiction that $m^3-n^3=1$ for some integers $m$ and $n$. Note that $m>n$, since if $n\geq m$, then $n^3\geq m^3=n^3+1$, which is a contradiction. Then
%%\[
%%4=m^3-n^3= (m-n)(m^2+mn+n^2)
%%\]
%%hence $m-n$ divides $4$, so $m-n=\pm 1,\pm 2,$ or $\pm 4$. Since $m>n$, this means $m-n= 1,2,$ or $4$. Suppose first that $m-n=1$. Then
%%\[
%%4=m^3-n^3=(n+1)^3-n^2 = 3n^2+3n+1.
%%\]
%%This implies $3=3n^2+3n=3n(n+1)$, so $n(n+1)=1$, which is impossible.
%%
%%If $m-n=2$, then
%%\[
%%4=m^3-n^3=(n+2)^3-n^2 = 6n^2+12n+8.
%%\]
%%Then this implies $-4=6n^2+12n$, but this is impossible since $4$ is not divisible by 3.
%%
%%Finally, if $m-n=4$, then
%%\[
%%4=m^3-n^3=(n+4)^3-n^2 = 12n^2+48n+64.
%%\]
%%so $-60=12n^2+48n=12n(n+4)$, so $-5 = n(n+4)$. For this to be possible, either $n=\pm 1$ or $\pm 5$, but by checking all 4 values, we see that this equation cannot be satisfied,.
%%\end{proof}
%%\end{proof}
%%\end{solution}

%
%Homework
%\begin{exercise} Find all integer solutions to $x^2-2y^2=1$ given that $y$ is prime.
%
%
%\end{exercise}
%

\begin{exercise} Find all primes $p$ and integers $a,b\in\N$ so that $p^{a}-p^{b}=24$.

\begin{solution}
 If $b=0$, then the above equation implies $p^{a}=25$, so $p=5$ and $a=2$.

 If $b> 0$, then $p$ divides $24$, so $p=2$ or $p=3$ and as $a>b$,
 \[
 24 = p^{a}-p^{b} = (p^{a-b}-1)p^{b}
 \]
 so $p^{b}$ divides $24$.
 \begin{enumerate}[label=(\alph*)]
 \item If $p=2$, then $b=1$ or $2$, and checking both of these cases turns up no solutions for $a$.
 \item If $p=3$, then $b=1$, and we find that $p^{a} = 24+3^{1}=27=3^3$ so $a=3$.
 \end{enumerate}

 Thus, the only solutions $(a,b,p)$ are $(2,0,5)$ and $(3,1,3)$.

 \end{solution}

\end{exercise}

%Exam problem 2020
%\begin{exercise} Show that if $f_{n}$ denotes the $n$th Fibonacci number, then $f_{n}$ and $f_{n+1}$ are coprime.
%
%
%
%\begin{solution}
%We prove this by induction. Clearly, $f_{1}=f_{2}=1$ are coprime. For the induction step, let $n\geq 2$, we wish to show $d=\gcd(f_{n},f_{n+1})$ are coprime. By The Linear Combination Lemma,
%\[
%d$ divides $f_{n+1}-f_{n} = f_{n-1},
%\]
%so $d$ divides $f_{n-1}$ and $d$ divides $f_{n}$, so $d$ divides $\gcd(f_{n},f_{n-1})=1$ by the induction hypothesis. Thus, $d=1$, as desired.
%\end{solution}
%
%\end{exercise}

%Workshop problem
%\begin{exercise} Determine whether the following statement is true or false: if $a,b,k\in\mathbb{N}$ and $a^k$ divides $b^k$, then $a$ divides $b$.
%
%\begin{solution}
%Our plan is to use Theorem \ref{t:lcm}, so we just need to show that if $p$ is a prime in the decomposition of $a$ and $n$ is its power, then $p^n$ divides $b$ (so $p$ appears in the prime decomposition of $b$ and if $m$ is its power, then $n\leq m$).
%
%Note that if $p$ divides $a$, then $p$ divides $a^k$ so $p$ divides $b^k$, hence $p$ divides $b$. In particular, any prime appearing in the prime decomposition of $a$ appears in the prime decomposition of $b$. Let $n$ be the power of $p$ in the prime decomposition of $a$ and $m$ the power of $p$ in the decomposition for $b$. Then $p^{kn}$ divides $p^{km}$, so $p^n$ divides $p^m$. Thus, $n\leq m$. The claim now follows from Theorem \ref{t:lcm}.
%\end{solution}
%
%
%\end{exercise}

\begin{exercise} Is there a rational number $q$ so that $q^{5}-q^{3}+39=0$?

\begin{solution}
 The answer is no: Suppose such rational number $q$ exists. Then $q=\frac{m}{n}$ for some {\it coprime} integers $m$ and $n$. Then
\[
n^5-n^3m^3 + 39m^5 = 0,\]
which implies that $m$ divides $n^5$. Since $m$ and $n$ are coprime, this implies
that $m$ divides $n$ by Corollary \ref{c:c|abc|b}. Then $m = 1$, because $m$ and $n$ are coprime. Then
\[
n^5-n^3+39=0,
\]
which implies that $n$ divides $ 39$, so $n\in \{1,-1, 3,-3, 13,-13\}$. Since
$n^5-n^3 =-39$  is divisible by $n^3$, we see that either $n=1$ or $n=-1$. But
\[1^{5}-1^3+39=39\neq 37 = (-1)^{5}-(-1)^{3}+39\]
which is a contradiction. Suppose such rational number $q$ exists. Then $q=\frac{m}{n}$ for some {\it coprime} integers $m$ and $n$. Then
\[
n^5-n^3m^3 + 39m^5 = 0,\]
which implies that $m$ divides $n^5$. Since $m$ and $n$ are coprime, this implies
that $m$ divides $n$ by Corollary \ref{c:c|abc|b}. Then $m=1$, because $m$ and $n$ are coprime. Then
\[
n^5-n^3 +39=0,
\]
which implies that $n$ divides $ 39$, so $n\in \{1, -1, 3, -3, 13, -13\}$. Since
$n^5-n^3 =-39$  is divisible by $n^3$, we see that either $n=1$ or $n=-1$. But
\[1^{5}-1^3+39=39\neq 37 = (-1)^{5}-(-1)^{3}+39\]
which is a contradiction.
\end{solution}

\end{exercise}

\begin{exercise} Recall that a natural number $n$ is {\it perfect} if it is the sum of all its proper divisors (that is, all positive divisors other than $n$). For example, $1 + 2 + 3 = 6$, so $6$ is perfect. If $p$ is a prime and $n\in\mathbb{N}$, when is $p^{n}$ perfect?

\begin{solution}
Suppose $p^{n}$ is perfect. Then
\[
p^{n} = 1+p+\cdots + p^{n-1} = \frac{p^{n}-1}{p-1}<p^{n}-1<p^{n},
\]
a contradiction. Thus, $p^{n}$ is {\it never} perfect.
\end{solution}

\end{exercise}

\begin{exercise} Show that $x^{n}+y^{n}=z^{n}$ has integer solutions if and only if it has rational solutions.
\begin{solution}
Since integer solutions are also rational, we just need to show that rational solutions imply integer solutions. Suppose $(x,y,z)$ satisfy the above equation, so $x=\frac{a}{b}$, $y=\frac{c}{d}$ and $z=\frac{e}{f}$ for some integers $a,b,c,d,e,f$, so
\[
\frac{a^n}{b^n} + \frac{c^n}{d^n}=\frac{e^{n}}{f^{n}}.
\]
Multiplying both sides by all the denominators, we get
\[
a^{n}d^{n}f^{n} + c^{n}b^{n}f^{n} = e^{n} b^{n}d^{n}
\]
an so $(adf,cbf,ebd)$ is an integer solution to the original equation.
\end{solution}
\end{exercise}

\begin{exercise}
Recall that there are infinitely many pythagorean triples, i.e. triples of positive integers $(x,y,z)$ so that $x^2+y^2=z^2$. Using this, show that there are infinitely many integer solutions to $x^2+y^2=z^3$.

\begin{solution}
Given $(x,y,z)$ a pythagorean triple, so
\[
x^2+y^2=z^2\]
then
\[
x^2z^4+ y^2 z^4 = z^2 z^4
\]
hence
\[
(xz^2)^2+(yz^2)^2 = (z^2)^3.
\]
\end{solution}
\end{exercise}

\begin{exercise} Let
\[
\Z[\sqrt{-5}]=\{a+b\sqrt{-5}: a,b\in\Z\} = \{a+bi\sqrt{5}: a,b\in\Z\}\subseteq \C.
\]
We say that $p\in \Z[\sqrt{-5}]$ is {\it primo} if the only solutions to $p=ab$ wtih $a,b\in \Z[\sqrt{-5}]$ have either $a$ or $b$ equal to $\pm 1$.

 \begin{enumerate}[label=(\alph*)]
\item Prove that $|a+b\sqrt{-5}|=\sqrt{a^2+5b^2}$. Conclude that if $|z|=1$, then $z=\pm 1$.
\item Assuming (a), prove that every nonzero $z\in\Z[\sqrt{-5}]$ can be written as a product of primos.
\item Show that $2,3,1\pm\sqrt{-5}$ are all primo.

\end{enumerate}

\begin{solution}
For part (a), the fact that $|a+b\sqrt{-5}|=\sqrt{a^{2}+5b^{2}}$ juts follows from the definition of the modulus of a complex number. In particular, if $z=a+b\sqrt{-5}$ has modulus 1, then $1=|z|^{2}=a^{2}+5b^{2}$, and the only way this can happen is if $b=0$ and $a=1$ (since a and b are integers).

For part b, we can prove this by strong induction on $|z|^2$. If $|z|^2=1$, then $z$ must be $\pm 1$, which is a primo (since if we have $xy=z$, then $1=|z|=|xy|=|x||y|$, so $|x|=|y|=1$, and hence $x,y=\pm 1$, that is, they are both equal to plus or minus z).

Now suppose we know that claim b holds for all $z$ with $|z|^2\leq n$ for some integer $n$. Let $z$ now be an element with $|z|^2=n+1$. If it is not primo, then there is a way of writing z as $xy$ where neither x or y are equal to plus or  minus z, so in particular, neither of them are $\pm 1$ either. By part (a), since neither of them are 1, then $|x|,|y|>1$, and $|z|=|xy|=|x||y|$, so $1<|x|^2=|z|^2/|y|^2<|z|^2=n+1$. Thus, $x$ can be factored into a product of primos by the induction hypothesis, and y can be similarly. This proves the induction step.

For c, note that 2 is primo since otherwise 2 can be written as a product of two numbers with modulus less than 2, but then they must have modulus 1, and so they must be $\pm1$, which is impossible.

3 is primo since if it were the product of two numbers xy with modulus less than 3, then they would have to either be $\pm 1$ or $\pm 2$, which don't multiply up to 3.

$1+\sqrt{-5}$ is primo since, if it were written as a product $xy$ with neither of x or y equal to 1 or $1+\sqrt{-5}$, notice that $|x|\cdot |y|=|xy|=|1+\sqrt{-5}|=\sqrt{1+5}=\sqrt{6}$, so $|x|,|y|\leq \sqrt{6}$, and the only numbers for which this holds are $\pm 1$ and $\pm 2$.
\end{solution}

\end{exercise}

%
%\begin{exercise} Suppose $f$ is a degree $n>5$ polynomial so that for some distinct integers $a<b<c<d$ we have $f(a)=f(b)=f(c)=f(d)=0$. Show that there is no integer $k$ so that $f(k)=3$.
%
%\begin{solution}
%We can factor $f$ as
%\[
%f(x) = (x-a)(x-b)(x-c)(x-d)g(x)
%\]
%for some polynomial $g$. If $f(x)=3$ for some integer $x$, then
%\[
%3=(x-a)(x-b)(x-c)(x-d)g(x),\]
%so $3$ is a product of at least 4 distinct integers, but this is impossible: the FTA implies that exactly one of these integers must be $\pm 3$, in which case the other integers must be either $\pm 1$, but that means $3$ can only be written as a product of 3 distinct integers.
%\end{solution}
%
%\end{exercise}

%
\begin{exercise} Show that if $n> 4$ is an integer that is {\it not} prime, then $n$ divides $(n-1)!$.

\begin{solution}

Since $n$ is not prime, $n=ab$ for some integers $1<a,b<n$.
\begin{enumerate}[label=(\alph*]
\item If $a<b$, then
\[
(n-1)!=1\cdot 2 \cdots a\cdots b\cdots (n-1).
\]
so $n=ab$ divides $(n-1)!$.
\item if $a=b$, then $n=a^2$ and $a>2$, so $2a\leq a^2-1$, and
\[
(n-1)!=1\cdot 2 \cdots a\cdots 2a\cdots (a^2-1)
\]
so we see that $a^2$ divides $(n-1)!$.
\end{enumerate}
\end{solution}
%
\end{exercise}

\begin{exercise} Recall that the Fermat numbers $F_{n}=2^{2^{n}}+1$ satisfy the recurrence relation
\[
F_{n} = F_{n-1}\cdots F_{0}+2.
\]
Show that the Fermat numbers are mutually relatively prime, that is, $\gcd(F_{n},F_{m})=1$ whenever $m\neq n$.

\end{exercise}

\subsection{  Challenging Exercise: A special case of Fermat's last theorem*}

In this exercise we will show that $x^4 + y^4 = z^4$ (the Fermat quartic) has no {\it nontrivial solutions}, that is, no integer solutions apart from $(x,y,z)=0$. The method we will use is known as ``infinite descent'': we suppose there is a solution $(x,y,z)$ with $z>0$, then we assume $(x,y,z)$ has $z$ smallest among all such solutions, then we show that we can find another solution $(u,v,w)$ with $w<z$, contradicting that $z$ was the smallest integer appearing in a solution to $x^4+y^4=z^4$. This is quite difficult and wouldn't be expected of you on a homework or exam, it's just a bit of fun.
\begin{itemize}
\item Explain why it suffices to only find positive solutions?
\item Why does it suffice to show $x^4+y^4=z^2$ has no nontrivial integer solutions?
\item Suppose there is a non-trivial solution positive solution to  $x^4 + y^4 = z^2$. Then there is a solution $(x,y,z)$ with smallest $z>0$ among all solutions. Show that $x,y,z$ have no common factors.
\item Show that exactly one of $x$ or $y$ is odd. Now assume $x$ is odd and $y$ is even.
\item Note that $a=x^2$ and $b=y^2$ along with $c=z$ form a Pythagorean triple, $a$ is odd, and $x,y,z$ are coprime.  Hence, by our work on classifying Pythagorean triples, we must have

\[
(a,b,c) = \left( as^2 t^2, a\frac{s^2-t^2}{2}, a\frac{s^{2}+t^{2}}{2}\right)
\]
for some $s>t$. Show that $4$ divides $s-t$.
\item Using that $(s-t)(s+t)=2b=2y^2$, show that there are integers $u$ and $v$ with $s+t = 2u^2$ and $s-t=4v^2$, and $u$ and $2v$ are relatively prime. solving for $s$ and $t$, verify that $x^2+4v^4 = u^4$.
\item Let $A = x$, $B = 2v^2$ and $C = u^2$. Show that this is another Pythagorean triple of the form described in the parametrization theorem. So they can be expressed as above for $(a,b,c)$, but with parameters $S$ and $T$ instead of $s$ and $t$.
\item Arguing in a similar fashion to (d), show that $S$ and $T$ satisfy $S+T = 2X^2$ and $S-T=2Y^2$ for some positive integers$ X$ and $Y$.
\item Solve for $S$ and $T$ and verify that $u^2 = X^4 + Y^4$. Finally show that $uXY\neq 0$ and that $0 < u < z$, leading to a contradiction.
\end{itemize}

%Interesting open problems:
%
%
%\begin{question}[Bocard's problem]
%Are there infinitely many $n\in\mathbb{N}$ so that $n!+1$ is a square? Only 3 integers are known: $4!+1=5^2$, $5!+1=11^2$, and $7!=71^2$.
%\end{question}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 7: Modular Arithmetic}

\chapterimage{Figures/blank.png}

\chapter{Modular Arithmetic}%
\label{modular}

%
%\epigraph{\it And this proposition is generally true for all progressions and for all prime numbers; the proof of which I would send to you, if I were not afraid to be too long.}{Pierre de Fermat, 1640, in a letter to his friend Fr\'{e}nicle de Bessy commenting on his what later was later to be called, ironically, {\it Fermat's Little Theorem}.}

\indent Is $30^{99}+61^{100}$  divisible by 31? This seems like a daunting question. However, in a couple of pages, you'll learn a trick that will make this and other problems involving divisibility of very large numbers much easier. This method involves {\it modular arithmetic}.

The framework of modular arithmetic as we are familiar with it was first developed by Gauss in his book {\it  Disquisitiones Arithmeticae} in 1801, whereas many fundamental results that are now stated in terms of congruences were in fact proven much earlier; for example, Fermat's Little Theorem below was proven in 1640, though now it is standard to formulate it using congruences.

Despite its classical roots, modular arithmetic is important for both mathematics and applications. In mathematics, it is fundamental for studying {\it abstract algebra}, which you will learn more about in {\it Fundamentals of Pure Math} next year and in {\it Honours Algebra} the year after that. Moreover, it is also a cornerstone of modern cryptography (Chapter 15 in Liebeck's book has a good introduction to {\it public key cryptography}). \\

If you like the material from this week, you might enjoy taking the following classes:

\begin{itemize}
\item Fundamentals of Pure Math (2nd year)
\item Honours Algebra, Introduction to Number Theory (3rd year)
\item Commutative Algebra, Group Theory, Galois Theory, Algebraic Geometry (4th Year)
\end{itemize}

\section{Arithmetic modulo $m$}%
\label{arithmeticmodm}

\begin{definition}
\label{d:mod}
For $m\in \mathbb{N}$ and $a,b\in \mathbb{Z}$, we write $a = b\mod m$ or sometimes $a  = b\mod m$ (read {\it $a$ is congruent to $b$ modulo $m$}) if one of the following equivalent conditions holds:

\begin{itemize}
\item $m$ divides $a-b$;
\item $a$ and $b$ have same remainder when divided by $m$;
\item $b=qm+a$ for some $q\in \mathbb{Z}$.
\end{itemize}
\end{definition}

\begin{proposition}
For any integer $n$, there exists a unique element $\overline{n}$ such that $n = \overline{n} \mod m$.
\end{proposition}
\begin{proof}
Exercise.
\end{proof}

\begin{example}
$32=2\mod 10$. $82=1 \mod 3$.
\end{example}

\begin{theorem}
We have the following properties for integers $a,b,c\in\mathbb{Z}$ and $m\in\mathbb{N}$:
\begin{enumerate}[label=(\alph*)]
\item $a = a\mod m$.
\item If $a = b\mod m$, then $b = a\mod m$.
\item If $a = b\mod m$ and $b = c\mod m$, then $a = c\mod m$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item Since $m$ divides $a-a=0$, we have that $a = a$.
\item If $a = b\mod m$, then $m$ divides $(a-b)$, so $m$ divides $(b-a)$ as well, hence $b = a \mod m$.
\item Suppose $a = b \mod m$ and $b = c\mod m$. Then $m$ divides $a-b$, so $a-b=km$ for some integer $k$. Moreover, $m$ divides $b-c$, so $b-c=jm$ for some integer $j$. Thus,
\[
a-c = a-b+b-c = km+jm = (k+j)m,\]
hence $m$ divides $(a-c)$, thus $a = c \mod m$.\qedhere
\end{enumerate}
\end{proof}

\begin{theorem}%[Propositions 13.3 and 13.4]
\label{t:mod-artithmetic}
Suppose $a  = x \mod m$ and $b = y \mod m$. Then
\begin{enumerate}[label=(\alph*)]
\item $-a = -x \mod m$;
\item $a+b = x+y \mod m$;
\item $ab  = xy \mod m$;
\item for any natural number $k$, one has $a^{k} = x^{k} \mod m$.
\end{enumerate}
\end{theorem}

\begin{proof}
We have that $m$ divides $a-x$ and $m$ divides $b-y$
\begin{enumerate}[label=(\alph*)]
\item It is clear that $m$ divides $-(a-x)=(-a)-(-x)$, so that $-a = -x \mod m$.
\item The Linear Combination Lemma implies that $m$ divides
\[
(a-x)+(b-y)=(a+b)-(x+y),
\]
and so $a+b = x-y\mod m$.
\item $a = x \mod m$ implies $a=x+pm$ for some $p\in\mathbb{Z}$ and $b = y \mod m$ implies $b=y+qm$ for some $q\in\mathbb{Z}$. Thus,
\[
ab = (x+pm)(y+qm) = xy+pmy+xqm+ pqm^2 = xy + m(py+xq+pqm),\]
and so $ab = xy$ by Definition \ref{d:mod}(3).
\item This can be proven by induction. For the base case of $k=1$, this is immediate. Now suppose $a^{k} = x^{k}\mod m$ for some $k\geq 1$. Then by part (b) of this theorem with $b=a^{k}$ and $x=y^{k}$ and our induction hypothesis
\[
a^{k+1}=a\cdot a^{k} = a x^{k} \mod m\]
and then applying part (b) again but with $b=y=x^k$,
\[
a x^{k} = x\cdot x^k=x^{k+1}.\]
This proves the induction and hence the theorem. \qedhere
\end{enumerate}
\end{proof}

This result proves that many of the algebraic manipulations we enjoy performing on integers -- addition, subtraction, and multiplication -- can be done modulo $m$ systematically.
This simplifies many challenging-looking problems.
Let's revisit our earlier example: is $30^{99}+61^{100}$ divisible by 31?
Note that $30=31-1=-1\mod 31$ and $61=2\cdot 31-1=-1\mod 31$, and so
\[
30^{99}+61^{100} = (-1)^{99}+(-1)^{100}=-1+1=0 \mod 31.
\]
Indeed, $31$ divides $30^{99}+61^{100}$.\\

Theorem \ref{t:mod-artithmetic} says we can add and multiply modulo $m$. Can we also divide? That is, if $ad = bd\mod m$, do we also have $a = b\mod m$? This is \emph{not} always the case.
Perhaps the simplest example is the following: $2 = 0 \mod 2$, but we cannot divide this equation by $2$, because $1 \neq 0 \mod 2$.
More subtly, $12 = 24 \mod 6$, but here we cannot divide by $3$, because $4 \neq 8 \mod 6$.
The following theorem says when we can divide in this way.

\begin{proposition}
\label{p:xa=ya-x=a}
Let $d$ and $m$ be coprime. If $x,y\in \mathbb{Z}$ and $xd=yd\mod m$, then $x = y\mod m$.
In particular, if $p$ is prime and $p$ does not divide $a$, then $xa = ya\mod p$ implies that $x = y\mod p$.
\end{proposition}

\begin{proof}
Suppose $a$ and $m$ are coprime and $xa = ya\mod m$. Then $m$ divides $xa-ya=(x-y)a$. Since $m$ and $a$ are coprime, $m$ divides $x-y$, and so $x = y\mod m$ by definition. The second part of the proposition follows from the first.
\end{proof}

\begin{example}
Find an integer $x\in \{0,1,..,6\}$ so that $4^{6} = x\mod 7$. \\

Let's look at some powers of $4$ and see what they are modulo $ 7$.

\begin{align*}
4^2 & = 16 =14+2 = 2 \mod 7 \\
4^4 & = (4^2)^2  = 2^2 =4 \mod 7.
\end{align*}

Thus,
\[
4^6=4^2\cdot 4^4 = 2\cdot 4=8 = 1 \mod 7.
\]
This is certainly much quicker than computing $4^6$ by hand and then doing long division!
\end{example}

\section{Solving linear equations modulo $m$}%
\label{linearequationsmodm}

When can we solve $ax = b\mod m$ for $x$? Before we even start, it's good to have a test to see if there is actually a solution:

\begin{theorem}%[(Liebeck Proposition 13.6)]
The equation $ax = b\mod m$ has a solution if and only if $\gcd(a,m)$ divides $b$.
\end{theorem}

\begin{proof} An integer $x$ is a solution to $ax = b\mod m$ if and only if $ax=b-my$ for some integer $y$.
In other words, $ax = b\mod m$ has a solution if and only if $b$ can be written as $ax + my$ for some integers $x$ and $y$.

If $b = ax+my$, then the Linear Combination Lemma ensures that $\gcd(a,m)$ divides $b$.

Conversely, assume that $\gcd(a,m)$ divides $b$, so that $b = h\gcd(a,m)$ for some integer $h$.
Bezout's identity implies that there exist integers $x'$ and $y'$ such that $\gcd(a,m)=ax'+my'$.
Hence if $x = hx'$ and $y=hy'$, then
\[
b = h\gcd(a,m) = ax + my,
\]
as desired.
\end{proof}

\begin{example}
Find $x\in \{0,1,...,13\}$ so that $18x = 10\mod 14$. \\

We see that $\gcd(18,14)=2$ divides $10$, so there is at least one solution.
We can use the Euclidean Algorithm to express $2$ as an integer linear combination:
since $18 - 14 = 4$, and $14 - 3\cdot 4 = 2$ is the gcd, so we can write
\[
2 = 14 - 3\cdot 4 = 4\cdot 14 - 3 \cdot 18.
\]
Multiplying this by $5$, we obtain
\[
10 = 20\cdot 14 - 15 \cdot 18.
\]
So $-15$ is a solution, and so is $3 = -15 \mod 18$.
\end{example}

Note that when finding a solution to $a = bx\mod m$, any integer $x$ is congruent modulo $m$ to a number in $\{0,1,...,m-1\}$. Thus, whenever we ask for a solution, we prefer to write it as one of these integers.

\begin{exercise}
Find $x$ so that $7x = 8\mod 12$. \\
\begin{solution}
Since $\gcd(7,12)=1$, we can use the Euclidean Algorithm:
\[
12-7 = 5 \text{\quad and \quad} 7-5 = 2 \text{\quad and \quad} 5-2\cdot 2 = 1.
\]
We find
\[
1 = 5 - 2\cdot 2 = 3\cdot 5 - 2\cdot 7 = 3\cdot 12 - 5\cdot 7,
\]
so by multiplying by $8$, we get
\[
8 = 24 \cdot 12 - 40 \cdot 7.
\]

Thus $x = -40$ is a solution, and so is $8 = -40 \mod 12$.
\end{solution}
\end{exercise}

\section{$\mathbb{Z}/m$}%
\label{Zmodm}

In this section we introduce a set $\mathbb{Z}/m$ that will be important your future algebra classes.
Just like the integers, the rationals, and the reals, it is what is called a \emph{ring}: it is equipped with addition, subtraction, and multiplication operations.
Unlike $\mathbb{Z}$, $\mathbb{Q}$, or $\mathbb{R}$, the set $\mathbb{Z}/m$ is {\it finite}.

As a set,
\[
\mathbb{Z}/m=\{0,1,\dots,m-1\}.
\]
On this set we shall define addition and multiplication operations: for any $x,y \in \mathbb{Z}/m$,
\[
x + y = z \mbox{ where $z\in \{0,1,...,m-1\}$ is the unique element such that } x + y = z\mod m,
\]
and
\[
x \cdot y = z \mbox{ where $z\in \{0,1,...,m-1\}$ is the unique element such that } x \cdot y = z\mod m,
\]
There is an additive inverse:
\[
- x = m-x
\]
(Some books write $\mathbb{Z}_m$ for $\mathbb{Z}/m$, but most algebraists reserve this notation for a very different object!!)

\begin{example}
Let's write the addition and multiplication tables for $\mathbb{Z}/5=\{0,1,2,3,4\}$.

\begin{center}
\begin{tabular}{| c| c | c | c | c | c |}
\hline
{$+$} & {0} & {1} & {2} & {3} & {4}\\
\hline
{0} & {0} & {1} & {2} & {3} & {4} \\
\hline
{1} & {1} & {2} & {3} & {4} & {0} \\
\hline
{2} & {2} & {3} & {4} & {0} & {1} \\
\hline
{3} & {3} & {4} & {0} & {1} & {2} \\
\hline
{4} & {4} & {0} & {1} & {2} & {3} \\
\hline
\end{tabular}

\smallskip

\begin{tabular}{| c| c | c | c | c | c |}
\hline
{$\times$} & {0} & {1} & {2} & {3} & {4}\\
\hline
{0} & {0} & {0} & {0} & {0} & {0} \\
\hline
{1} & {0} & {1} & {2} & {3} & {4} \\
\hline
{2} & {0} & {2} & {4} & {1} & {3} \\
\hline
{3} & {0} & {3} & {1} & {4} & {2} \\
\hline
{4} & {0} & {4} & {3} & {2} & {1} \\
\hline
\end{tabular}
\end{center}
\end{example}

We can check that for $m\in\mathbb{N}$, $\mathbb{Z}/m$ has all the nice algebraic properties of being a field (recall this definition from Week 1) apart from perhaps property (M4), which said that each nonzero element must have a multiplicative inverse.
An element $x\in \mathbb{Z}/m$ is {\it invertible} if there exists $y\in \mathbb{Z}/m$ so that $x\cdot y=1$.

If $m$ is composite, then not every nonzero element $\mathbb{Z}/m$ is invertible.
Consider the multiplication table of $\mathbb{Z}/4$:
\begin{center}
\begin{tabular}{| c| c | c | c | c |}
\hline
{$\times$} & {0} & {1} & {2} & {3} \\
\hline
{0} & {0} & {0} & {0} & {0}  \\
\hline
{1} & {0} & {1} & {2} & {3}  \\
\hline
{2} & {0} & {2} & {0} & {2}  \\
\hline
{3} & {0} & {3} & {2} & {1}  \\
\hline
\end{tabular}
\end{center}
As you can see, $2x=1$ has no solution modulo $4$.

Observe that an element $a \in \mathbb{Z}/m$ is invertible if and only if the equation $ax = 1 \mod m$ admits a solution.
Such a solution exists if and only if $\gcd(a,m)$ divides $1$, hence if and only if $a$ and $m$ are coprime.
Thus the invertible elements of $\mathbb{Z}/m$ are precisely those elements that are coprime to $m$.
In particular, if $p$ is prime, then every nonzero element of $\mathbb{Z}/p$ is coprime to $p$, so we deduce the following.

\begin{proposition}
Let $m\geq 2$ be a natural number.
Then $\mathbb{Z}/m$ is a field if and only if $m$ is prime.
\end{proposition}

\begin{exercise}
How many invertible elements are there in $\mathbb{Z}/{81}$?
\end{exercise}

\begin{solution}
An element $y\in\mathbb{Z}/{81}$ is invertible if and only if $x$ and $81$ are coprime.
The is possible if and only if $3$ does not divide $y$.
The number of numbers in $\{0,1,...,80\}$ divisible by $3$ is $\frac{81}{3}=27$, so the number of invertible elements is $81-27 = 54$.
\end{solution}

Just like there is a notion of inverses for $\mathbb{Z}/m$, we also have a notion of a root. We say $y\in\mathbb{Z}/m$ is the $n$-th root of $x\in\mathbb{Z}$ if $x^n=y$.

\begin{example}
In $\mathbb{Z}/{5}$, does $2$ have a cubed root?
Let us test the values:

\begin{itemize}
\item $0^{3}=0$.
\item $1^{3}=1$.
\item $2^{3}=3$ since $2^{3} = 8 = 3\mod 5$.
\item $3^{3}=2$ since $3^{3} = 27 = 2\mod 5$.
\item $4^{3}=4$ since $4^{3} = (-1)^{3}=-1 = 4 \mod 5$.
\end{itemize}
Thus, the unique cubed root of $2$ is $3$. \\
\end{example}

However, not all numbers in every $\mathbb{Z}/m$ have roots! Confirm for yourself that $2\in\mathbb{Z}/{4}$ has no cubed root.

\section{Fermat's Little Theorem}%
\label{Fermatlittle}

The great thing about congruence is that arithmetic  becomes easier since you can replace big numbers with smaller numbers that are congruent. The following is a little but powerful theorem due to Fermat that allows you to eliminate large powers if you are working modulo some prime.

\begin{theorem}[Fermat's Little Theorem (FLT)]
Let $a$ be an integer, and let $p$ be a prime number.
If $p$ does not divide $a$, then
\[
a^{p-1} = 1 \mod p.
\]
\end{theorem}

\begin{proof}
Without loss of generality, we may assume that $a \in \{1,\dots,p-1\}$.
We consider the numbers $a,2a,3a,\dots,(p-1)a$. We first observe that each of these numbers is distinct modulo $p$;
that is, for each $c,d\in \mathbb{Z}/p$, one has $ca = da\mod p$ if and only if $c=d$.
This follows from Proposition \ref{p:xa=ya-x=a}.

If $\overline{a},\overline{2a}, \dots, \overline{(p-1)a}$ are the remainders of $a,2a,\dots,(p-1)a$ after dividing by $p$, then we deduce that the sets $\{\overline{a},\overline{2a}, \dots, \overline{(p-1)a}\}$ and $\{1,2,\dots,p-1\}$ are equal
Consequently,
\[
a\cdot 2a \cdots (p-1)a = \overline{a}\cdot\overline{2a}\cdot \cdots\cdot \overline{(p-1)a} = 1\cdot 2 \cdots \cdot (p-1) \mod p.\]
Thus
\[
(p-1)!a^{p-1} = (p-1)!\mod p.
\]
Now since $p$ does not divide $(p-1)!$, Proposition \ref{p:xa=ya-x=a} implies that $a^{p-1} = 1 \mod p$, as desired.
\end{proof}

\begin{example}
Find $5^{100}\mod 7$. \\

Since $7$ does not divide $5$, the FLT says $5^{6} = 1\mod 7$. Thus, we can shave off any power of $5$ from $5^{100}$ that is a multiple of $6$. Thus,
\[
5^{100} = 5^{96}5^{4}=(5^{6})^{16}5^{4} = 1^{16}5^{4}= 5^{4}.
\]
Then the rest is as before:
\begin{align*}
5^{2} & =25 = 4\mod 7\\
5^{4} & =(5^{2})^{2} = 4^{2}=16 = 2\mod 7.
\end{align*}
\end{example}

Fermat's Little Theorem gives us a way of solving a large class of polynomial equations mod $m$:

\begin{theorem}
\label{t:x^n=bmodp}
Let $n\in\mathbb{N}$ and $p$ be a prime number. Assume $n$ and $p-1$ are coprime and $p$ does not divide $b$. Then the equation
\begin{equation}
\label{e:x^n=bmodp}
x^{n} = b\mod p.
\end{equation}
has exactly one solution $x\in \{1,\dots,p-1\}$.
\end{theorem}

\begin{proof}
Since $n$ and $p-1$ are coprime, there are integers $s,t$ so that $sn+t(p-1)=1$.

Let us prove the uniqueness.
If $x_1,x_2 \in \{1,\dots,p-1\}$ are two such solutions, then $x_1^n=x_2^n \mod p$, and so for $i \in \{1,2\}$,
\[
x_i = x_i^{sn+t(p-1)} = x_i^{sn}x_i^{t(p-1)} = b^s(x_i^t)^{p-1} \mod p.
\]
Since $p$ does not divide $x_i^t$, Fermat's Little Theorem that each $x_i = b^s \mod p$, whence $x_1=x_2$.

Now let us prove the existence.
Let $x$ be the remainder after dividing $b^s$ by $p$.
Now in $\mathbb{Z}/p$,
\[
x^{n}  = (b^{s})^{n}=b^{ns}  = b^{1-t(p-1)}=b\cdot (b^{p-1})^{-t}.\]
(Note that this makes sense, since $p$ does not divide $b$, so $b$ is invertible.)
Since $p$ does not divide $b$, Fermat's Little Theorem implies that $x^n = b \mod p$.
\end{proof}

Notice that even though the choice of $s, t$ is not unique, the remainder after dividing $b^s$ by $p$ \emph{is} unique.
In other words, if $s_1,t_1,s_2,t_2$ are integers such that for $i \in \{1,2\}$, we have
\[
s_in+t_i(p-1)=1,
\]
then $b^{s_1} = b^{s_2} \mod p$.
To see this directly, note that in $\mathbb{Z}/p$, we have by FLT:
\[
b^{s_1} = (b^{s_1})^{s_2n+t_2(p-1)} = b^{s_1s_2}(b^{s_1t_2})^{p-1}=b^{s_1s_2},
\]
and by the same reasoning:
\[
b^{s_2} = (b^{s_2})^{s_1n+t_1(p-1)} = b^{s_1s_2}(b^{s_2t_1})^{p-1}=b^{s_1s_2}.
\]

We now have a blueprint to how to solve equations of the form \eqref{e:x^n=bmodp} when $n$ and $p-1$ are coprime and $p$ does not divide $b$.

\begin{example}
 Find $x\in\{0,1,...16\}$ so that $ x^{7} = 4\mod 17$.  \\

Since $\gcd(7,17-1)=\gcd(7,16)=1$, we can use the Euclidean Algorithm to show that $7\cdot 7=1+3\cdot 16$.  Thus, if $x$ solves the above equation, then $4 = x^{7}$,   and so
\[
4^{7} = (x^{7})^{7}
= x^{7\cdot 7}
= x^{1+3\cdot 16}
 = x\cdot (x^{16})^{3}
   = x\cdot (1)^{3}
   = x.
\]
Thus, $x = 4^{7}$, so we need to find a representative in $\{0,1,...,16\}$. Note that $4^{2}=16 = -1\mod 17$,  so
\[
4^{7}=(4^{2})^{3} \cdot 4
 = (-1)^{3} \cdot 4
=-4 = 13\mod 17.
\]
Thus, $x=13$ is the unique solution in $\{0,1,...,16\}$.
\end{example}

Not all polynomial equations can be solved using the above method:

\begin{example}
Find a solution to $x^{22} = 3 \mod 11$.  \\

Note that $\gcd(22,11-1)=\gcd(22,10)=2$, so we can't use the method in Theorem \ref{t:x^n=bmodp}. However, let's suppose $x$ is a solution to the above equation and try to narrow down what it must be. Note that $11$ does not divide $x$   (otherwise $x^{22} = 0$).  Thus, FLT implies $x^{10} = 1 \mod 11$. Hence,
\[
x^{22}=x^{2}(x^{10})^{2}   = x^{2}.\]
Now we just need to solve $x^{2} = 3 \mod 11$.  Again, $\gcd(2,11-1)=\gcd(2,10)=2$, so we still can't use the method of Theorem \ref{t:x^n=bmodp}.  But now the power $2$ is small enough we can just try values $x\in \{0,1,...,10\}$ and see what works:

\begin{itemize}
\item $1^{2}=1$
\item $2^{2}=4$
\item $3^{2}=9$
\item $4^{2}=16 = 5 \mod 11$.
\item $5^{2}=25 = 3\mod 11$.
\end{itemize}
Thus, $x=5$ is a solution to $x^{22} = 3 \mod 11$:
\[
5^{22}=5^{2}(5^{10})^{2} = 3\cdot (1)^{2}=3.
\]

Note that if we can't use Theorem \ref{t:x^n=bmodp}, then the solution may not be unique. In this example, note that $x=6$ is also a solution
\end{example}

\begin{example}
 Find $x\in\{0,1,...,10\}$ that solves $x^{3} = 9 \mod 11$.  \\

Note $\gcd(3,11-1)=\gcd(3,10)=1$, so this tells us that the method of Theorem \ref{t:x^n=bmodp} should work in finding the unique solution. Note also that $7\cdot 3 = 21=1+2\cdot 10$.  Thus, if $x$ is the solution, then $9 = x^{3}\mod 11$ implies
\[
9^{7} = (x^{3})^{7}
=x^{3\cdot 7}
 =x^{1+2\cdot 10}
  =x\cdot (x^{10})^{2}
   = x\cdot(1)^{2}
  =x \mod 11.\]
  Thus, $x = 9^{7}$. Now we must find $x\in \{0,1,...,10\}$ so that $x = 9^{7}$.
  \begin{itemize}
  \item $9^{2}=81   = 4 \mod 11$
  \item $9^{4}=(9^{2})^{2}   = 4^{2}  =16 = 5\mod 11$.
  \item $9^{7}=9^{4}\cdot 9^{2}\cdot 9  = 5\cdot 4\cdot 9 =180 =176+4 = 4\mod 11$.
  \end{itemize}

Thus, $x=4$ is the solution.  We can also check $4^{3}=64=55+9 = 9\mod 11$.

\end{example}

\subsection{Diophantine equations}

As congruences relate to divisibility, we can also use them to solve diophantine equations.

\begin{example}
What are the integer solutions to $9x^{2}+9x+2=y^{4}$?

 We will present two methods for comparison, first using the usual divisbility methods from last week, and then a second method using modular arithmetic.\\

\noindent {\bf Method 1:}  If $x,y$ are integer solutions, then
\[
y^{4}=9x^{2}+9x+2 =(3x+2)(3x+1).
\]
 Since $y^{4}\geq 0$, we know $3x+1,3x+2> 0$ or $<0$.  First assume $>0$.   They are coprime since $\gcd(3x+1,3x+2)$ divides $ (3x+2)-(3x+1)=1$.   Thus, $3x+1=a^{4}$ and $3x+2=b^{4}$ for some integers $a$ and $b$.   (We can assume they are non-negative). But then $b^{4}-a^{4} =3x+2-(3x+1) =1$.   This is only possible if $(a,b)=(0,1)$  (exercise :). We can't have $a=0$ since then $3x+1=0$,  which is impossible if $x$ is an integer. The same happens if $3x+1,3x+2<0$.  Thus, there are no solutions.

\vspace{10pt}

As you can see, the above solution is quite long (and we didn't do some of the details). Now let's instead use modular arithmetic for a shorter proof:

 \vspace{10pt}

\noindent {\bf Method 2:} If $x,y$ are integer solutions to $9x^{2}+9x+2=y^{4}$, then
\[
y^{4}=9x^{2}+9x+2
 = 2\mod 3.
\]
Let $z\in \{0,1,2\}$ be such that $z = y\mod 3$, then   $z^{4} = y^{4} = 2\mod 3$.
 So let's test some $z$'s!
\begin{itemize}
\item $0^{4}=0$
\item $1^{4}=1$
\item $2^{4}=16 = 1 \mod 3$.
\end{itemize}
So there are no $z\in \{0,1,2\}$ so that $z^{4} = 2 \mod 3$.  Thus, there are no integer solutions to $9x^{2}+9x+2=y^{4}$.

\end{example}

\section{Exercises}%
\label{modularexercises}

The relevant exercises in Liebeck's book are in Chapters 13 and 14.

\begin{exercise} Determine whether the following equations have integer solutions:
\begin{itemize}
\item $x^2+y^2=9z+3$.

\begin{solution}
If $(x,y,z)$ was a solution, then $x^2+y^2 = 3\mod 9$, but the squares modulo $9$ are $0,1,4,0,7,0,0,1$, and adding any pair of these gives $0,1,2,4,5,7,8$ modulo $9$, and none of these are $3$, thus there cannot be any solutions to the original equation.
\end{solution}

\item $3x^2-y^2=-2$.

\begin{solution}
If $(x,y)$ was a solution, then $-y^2 = -2\mod 3$,and so $y^2 = 2\mod 3$, but if we look at the squares of integers mod 3, we see that they are just $0$ and $1$, so there is no integer $y$ for which $y^2 = 2 \mod 3$, thus there are no solutions to the original equation.
\end{solution}

\item $3x^2+2=y^2+6z^3$.

\begin{solution}
If $(x,y,z)$ is a  solution, we see then that $y^2 = 2\mod 3$, and we have already seen that this has no solutions.

\end{solution}

\end{itemize}

\end{exercise}

\begin{exercise} Show that If $n\in\mathbb{N}$, then $n^5$ and $n$ have the same last digit (that is, the digit in the $1$'s place).

\begin{solution}
FLT says that $n^{4} = 1 \mod 5$, hence $n^{5} = n \mod 5$, and so $n^{5}-n = 0\mod 5$. Also, $n^5-n=n^4(n-1)$, and since either $n$ or $n-1$ is even, we have $n^5-n = 0\mod 2$. Thus, we have $2$ divides $n^5-n$ and $5$ divides $ n^5-n$, hence $10$ divides $n^5-n$, so the last digit of $n^5$ and $n$ must be the same.
\end{solution}

\end{exercise}

\begin{exercise} Find the last digit of $7^{7^{7^{7}}}$.

\begin{solution}
$7^2=49 = -1\mod 10$, $7^3 = -7 = 3\mod 10$, $7^4 = 1 \mod 10$. Thus, as we take powers of $7$, the 1's digit cycles through 7,9,3, and 1. So we should look at what $7^{7^7}\mod 4$ is.

Note that $7 = 3\mod 4$, and $7^2 = 1\mod 4$, thus odd powers of $7$ are equivalent to $3\mod 4$. Since $7^7$ is odd, $7^{7^7} = 3\mod 4$, and so $7^{7^{7^{7}}} = 3\mod 10$.
\end{solution}

\end{exercise}

\begin{exercise} Suppose $n\in\mathbb{N}$ satisfies $n = 3\mod 4$. Show that $n$ has a prime factor $p$ with $p = 3\mod 4$.

\begin{solution}

Note that $n$ is not even, so $n$ has prime factorizaiton $p_{1}^{r_{1}}\cdots p_{k}^{r_{k}}$ for some primes $2<p_{1}<\cdots < p_{k}$ and integers $r_{i}>0$. Suppose $p_{i} = 1 \mod 4$ for all $i$. Then $p_{i}^{r_{i}} = 1 \mod 4$ as well, and so
\[
n=p_{1}^{r_{1}}\cdots p_{k}^{r_{k}} = 1 \mod 4,
\]
which is a contradiction. Thus, there is $i$ so that $p_{i} = 3\mod 4$.

\end{solution}

\end{exercise}

\begin{exercise}
Show that there are no integer solutions to $m^3-n^3=3$ or $m^3-n^3=4$. (Hint: if there were solutions, there would also be a solution $\mod 7$).

\begin{solution}
Let us look at what cubes are $\mod 7$:
\begin{align*}
1^3 & =1 = 1 \mod 7 \\
2^3 & =8  = 1 \mod 7\\
3^3 & =27  = -1 \mod 7 \\
4^3 & =64  = 1 \mod 7\\
5^3 & =125 = -1 \mod 7\\
6^3 & = 216  = -1 \mod 7
\end{align*}
Thus, the only values $m^3-n^3$ can be $\mod 7$ are $0,\pm 1,$ and $\pm 2$, or alternatively, $0,1,2,5,6$. In particular, $m^3-n^3=3$ and $m^3-n^3=4$ have no solutions.

\end{solution}

\end{exercise}

\begin{exercise} Show that for all primes $p>3$, $p = \pm 1\mod 6$. Use this to show $24$ divides $p^2-1$ for all primes $p>3$.

\begin{solution}
Note that $p$ is odd so $p\not = 2,4\mod 6$. Also, $p>3$, hence $p\neq 3\mod 6$. Thus, the only remaining possibilities are $\pm 1 \mod  6$. Hence, $p=6k\pm 1$ for some integer $k$, so that
\[
p^2-1 = (6k\pm 1)^2-1=36k^2\pm 12k=12(3k^2\pm k).
\]
Note that $3k^2\pm k$ is always even (just consider the different cases when $k$ is odd or even), so in fact $p^2-1$ is divisible by 24.

\end{solution}

\end{exercise}

%
%\begin{exercise} ({\bf Challenge!}) For which primes $p$ does $(p-1)!+1=p^{k}$ for some integer $k$?
%
%\begin{solution}
%Let's rearrange this equality: this would imply
%\[
%(p-1)! = p^{k}-1=(p-1)\sum_{j=0}^{k-1} p^{j}
%\]
%and so
%\[
%(p-2)! = \sum_{j=0}^{k-1} p^{j}.
%\]
%Notice that $p = 1\mod (p-1)$, hence
%\[
%(p-2)! = \sum_{j=0}^{k-1} 1=k\mod (p-1).
%\]
%Also notice that for $p>5$, $p-1$ divides $(p-2)!$, and so $(p-2)! = 0\mod (p-1)$, hence the above equation implies $k = 0 \mod (p-1)$, thus $k=\ell(p-1)$ for some integer $\ell$. But then $p^{k}\geq p^{p-1}>(p-1)!$, a contradiction. Hence, $p$ cannot be greater than $5$, so $p=2,3$ or $5$.
%
%\begin{itemize}
%\item If $p=2$, then $(p-1)!+1=2=2^{1}$, so $(p,k)=(2,1)$ is a solution.
%\item If $p=3$, then $(p-1)!+1=2!+1=3=3^{1}$, so $(3,1)$ is a solution.
%\item If $p=5$, then $(p-1)!+1=4!+1=25=5^2$, so $(5,2)$ is a solution.
%\end{itemize}
%
%Thus, these are the only solutions.
%
%
%\end{solution}
%
%
%\end{exercise}

\begin{exercise} Show that if $p_{1}<\cdots < p_{31}$ are prime and $30$ divides $p_{1}^{4}+\cdots + p_{31}^{4}$, then $p_{1}=2$, $p_{2}=3$ and $p_{3}=5$. {\it Hint: Assume the contrary and use FLT.}

\begin{solution}

\begin{itemize}
\item  If $p_1\neq 2$ then $p_1>2$ and so $p_1,p_2,\cdots p_{31}$ are all odd, hence $p_1^4+p_2^4+\cdots+p_{31}^4$ is odd, so in particular $30$ does not divide $ p_{1}^{4}+\cdots + p_{n}^{4}$, and we get a contradiction. Thus, $p_{1}=2$.

\item If $p_2\neq 3$ then $p_{i} = \pm 1\mod 3$ since $p_{1}=2 = -1$, and for $i\geq 2$, $p_{i}\neq 3$. Thus, $p_{i}^{2} = 1\mod 3$ by FLT, so $p_i^{4} = 1(\mod 3)$ for $i=1,2,\cdots,31$. Thus,
\[
p_1^4+p_2^4+\cdots+p_{31}^4 = 31 = 1 \mod 3.
\]
Thus, $p_1^4+p_2^4+\cdots+p_{31}^4$ is not a multiple of $30$. So $p_3=3$.
\item A similar argument shows that $p_3$ should be $5$: $p_3\neq 5 $ implies $ p_i^4 = 1 (\mod 5)$ for $i=1,2,\cdots,31$ by FLT and $p_1^4+p_2^4+\cdots+p_{31}^4 = 1 (\mod 5)$.
\end{itemize}
\end{solution}

\end{exercise}

\begin{exercise} (Regional Mathematical Olympiad 1998, India) Show that if $5<p_{1}<\cdots < p_{n}$ and $6$ divides $p_{1}^{2}+\cdots + p_{n}^{2}$, then $6$ divides $n$.

\begin{solution}
From an earlier exercise, we know that $p_{i} = \pm 1 \mod 6$, hence $p_{i}^{2} = 1 \mod 6$, thus
\[
0=p_{1}^{2}+\cdots + p_{n}^{2}  = n\mod 6,
\]
where in the first equation we used our problem assumption, thus $n = 0\mod 6$, hence $6$ divides $n$.
\end{solution}

\end{exercise}

%\begin{exercise} How  many  prime  numbers $p$ are  there  such that $29^p+ 1$ is a multiple of $p$?
%
%\begin{solution}
%If $p$ divides $29^{p}+1$, then $0 = 29^{p}+1 \mod p$, and by FLT, $29^{p}+1 = 29+1=30\mod p$, so $p$ divides $30$, hence $p=2,3,$ or $5$.
%\end{solution}
%
%\end{exercise}

\begin{exercise} Given that $p$ and $8p^2+1$ are prime, find $p$.

\begin{solution}
By trying out some primes, we can see that it seems like $3$ is the only option, so let's try to prove $p=3$. Suppose $p\neq 3$. Note that any prime $p\neq 3$ satisfies $p = \pm 1 \mod 3$, so $p^2  = 1 \mod 3$. Thus, since $8 = -1\mod 3$,
\[
8p^2+1 = (-1)1+1=0\mod 3,\]
thus $8p^2+1$ is a prime divisible by 3, which is only possible if $8p^2+1=3$, which is impossible. Thus $p=3$ is the only solution.
\end{solution}

\end{exercise}

%%
%%\begin{exercise} Let $a,x,y\in \mathbb{N}$. Show that if $a>1$ and $a^x+1$ divides $a^y+1$, then $x$ divides $y$.
%%
%%{\it Hint: Recall the geometric series formula.}
%%
%%\begin{solution}
%%Recall from the geometric series formula that
%%\[
%%\frac{a^y+1}{a+1}   = \sum_{k=0}^{y-1}(-a)^{k}
%%\]
%%Since $a=-1 \mod (a+1)$,
%%\[
%%\frac{a^y+1}{a+1} = \sum_{k=0}^{y-1}(-a)^{k} = \sum_{k=0}^{y-1}1^k=y\mod(a+1)
%%\]
%%The same holds with $x$ in place of $y$. If $a^x+1$ divides $a^y+1$, then $a^y+1 = n (a^x+1)$ for some integer $n$, hence
%%\[
%%y  = \frac{a^{y}+1}{a+1}= n \frac{a^{x}+1}{a+1} = nx \mod (a+1)
%%\]
%%Thus, $y= nx + m(a+1)$ for some integer $m$. We could do the same argument $\mod (ka+1)$ instead of $\mod (a+1)$ for any $k\in\mathbb{N}$ (since $a = -1 \mod (ka+1)$, and so for each such $k$ there is an integer $q$ so that $y=nx+q(ka+1)$. Since $x,y,n>0$, we have that $y>q(ka+1)>qka$, so if we pick $k>y$, then the only way this inequality can hold is if $q=0$, and so $y=nx$.
%%\end{solution}
%%
%%
%%\end{exercise}
%
%
%\begin{exercise} Let $a,b\in \mathbb{N}$. Show there are infinitely many $n\in \mathbb{N}$ so that $n^b+1 $ does not divide $a^n+1$.
%
%\begin{solution}
%Suppose the contrary that $n^b+1$ does not divide $a^b+1$ for only finitely many $n$, so we have $n^b+1$ divides $ a^n+1$ for all $n$ large enough (specifically, $n>N$ where $N$ is the last integer where they don't divide).
%
%Let $n=a^k$, then for $k$ large enough, $a^k>N$ and so
%\[
%(a^k)^b +1 =a^{kb}+1$ divides $ a^n+1=a^{a^{k}}+1.
%\]
%Thus, by the previous problem, $kb$ divides $a^k$. Hence, if $k$ is any integer coprime to $a$.
%
%
%\end{solution}
%

%\end{exercise}

\begin{exercise}
Prove that $42$ divides $n^7-n$ for all $n\in\mathbb{N}$.
\begin{solution}
Since $42 = 2\cdot 3 \cdot 7$, we just need to verify each of these factors divides $n^7-n$ for each $n\in\mathbb{N}$.
\begin{itemize}
\item Since $n^7-n=n^6(n-1)$, one of $n$ and $n-1$ must be even, so one of $n^6$ and $n$ must be even, so $2$ divides $n^{7}-n$.
\item $n^7-n = n^4\cdot (n-1)n\cdot (n-1)$, and one of $n-1,n,n+1$ must be divisible by 3.
\item Finally, by FLT,
\[
n^7-n = n-n=0 \mod 7.
\]
\end{itemize}
\end{solution}
\end{exercise}

\begin{exercise}
If $7$ does not divide $a$, then $7$ divides $a^3-1$ or $7$ divides $a^3+1$.
\begin{solution}

Note that since $7$ does not divide $a$, FLT implies
\[
(a^{3}-1)(a^3+1) = a^6-1 = 1-1=0 \mod 7
\]
and so $7$ divides $(a^{3}-1)(a^3+1) $, and since $7$ is prime, it must divide one of the factors.
\end{solution}
\end{exercise}

%
%\begin{exercise}
%If $p$ is prime, what is $(p-1)!\mod p$? (Hint: Recall that $ax = b\mod m$ has a solution if and only if $\gcd(a,m)$ divides $b$.)
%\begin{solution}
%By using the hint, we know that for each $q\in \{1,2,...,p-1\}$, there is $r\in \{1,2,...,p-1\}$ so that $qr = 1\mod p$. For $q=1$, this is just $r=1$. If $q\neq 1$ and $r=q$, then $q^2 = 1\mod p$ and so $p$ divides $q^2-1=(q-1)(q+1)$, thus $q$ is either $p-1$ or $p+1 = 1$. Hence, for $q\in \{2,3,...,p-2\}$, the inverse $r$ of $q$ is another number. Thus, all these numbers can be paired up and they cancel each other out in the product $(p-1)!$, so that $(p-1)!=p-1 = 01\mod p$.
%\end{solution}
%\end{exercise}
%

\part{Week 8: Relations and Functions}

\chapterimage{Figures/blank.png}

%\chapterimage{}

\chapter{More set theory and Equivalence relations}%
\label{moresets}

\section{Cartesian products}%
\label{cartesianproducts}

When we have two mathematical objects $x$ and $y$, an ordered pair $(x,y)$ is a way of listing the objects in order, so that $(x,y) \neq (y,x)$.
The main point is that we want $(x,y) = (u,v)$ to hold if and only if both $x =u$ and $y = v$.
This is in contrast with the sets $\{x,y\}$ and $\{u,v\}$, where the order does not matter: we have $\{x,y\} = \{y,x\}$.

Here is the way we make this set-theoretically sensible:
\begin{definition}
Let $S$ and $T$ be sets, and let $x \in S$ and $y \in T$.
Then we define the \emph{ordered pair}
\[
(x,y) = \{\{x\},\{x,y\}\}.
\]

The set of all ordered pairs is the \emph{cartesian product}:
\[
X \times Y = \{(x,y) : (x \in X)\wedge (y \in Y)\}.
\]
\end{definition}

You have already seen some Cartesian products in ILA:
\[
\R^{2} = \{(x,y) : x,y\in\mathbb{R}\} = \mathbb{R}\times \mathbb{R}.
\]

A rectangle in $\mathbb{R}^{2}$ can be defined using two intervals, $[a,b]$ in the $x$-axis and $[c,d]$ in the $y$ axis:
\[
[a,b]\times [c,d]=\{(x,y) : (x\in [a,b]) \wedge (y\in [c,d])\}.
\]
We can take products of very different sets. For example, $\{1,2,...,8\}\times \{Cat,Dog\}$ is the set of all pairs of numbers between 1 and 8 with either a cat or dog, so elements of the form $(2,Dog)$, $(8,Cat)$, etc.

We can also define \emph{ordered $n$-tuples}:
\[
(x_1, \dots, x_n) = \{\{x_1\}, \{x_1,x_2\}, \dots, \{x_1, \dots, x_n\}\},
\]
as well as $n$-fold cartesian products:
\[
A_1 \times \cdots \times A_n =\{(x_1, \dots, x_n) : (\forall i \in \{1,\dots,n\})(x_i \in A_i)\}.
\]

\section{Maps}%
\label{maps}

The real point of defining things as we have is so we can talk about \emph{maps} as soon as possible.
Maps -- \textsc{aka} \emph{set maps}, \emph{mappings}, \emph{functions} -- are an important notion in set theory.
The idea is that a map $f\colon S \to T$ is a way of taking each element $s\in S$ and associating one and only one element $f(s)\in T$ thereto.

The phrase \emph{there is a unique $x$ such that $\phi(x)$} -- sometimes written $(\exists ! x)(\phi(x))$ -- is a helpful shorthand for the longer sentence
\[
	(\exists x)(\phi(x)) \wedge ((\forall y)(\forall z)((\phi(y)\wedge \phi(z))\implies(y=z))).
\]

\begin{definition}
Let $S$ and $T$, be two sets.
A \emph{map} or \emph{function} $f$ from $S$ to $T$, denoted $f\colon S\to T$, is a subset $\Gamma(f) \subseteq S \times T$ such that for every $x \in S$, there exists a unique $f(x) \in T$ such that $(x,f(x)) \in \Gamma(f)$.
The set $S$ is called the {\it domain} or \emph{source} of $f$, and $T$ is the {\it codomain} or \emph{target} of $f$. The subset $\Gamma(f)$ is called the \emph{graph} of $f$.
\end{definition}

In practice, the way we describe maps is pretty relaxed.
We typically identify the source $S$ and the target $T$, and
then we provide a \emph{rule} for sending elements of $S$ to the associated elements of $T$.
The idea is that for every $ s \in S$, we have to specify a unique $f(s) \in T$ attached to $s$ in the sense that $(s, f(s)) \in \Gamma(f)$.

\begin{example}
We can define a map $f \colon \mathbb{Z}\rightarrow \mathbb{N}_0$ by the rule $f(m)=m^2$.
What this means is that the graph of $f$ is the subset $\Gamma(f) = \{(m,n) \in \mathbb{Z}\times\mathbb{N}_0 : n = m^2\}$.
Note that for each $m \in \mathbb{Z}$, there is of course a unique element $n \in \mathbb{N}_0$ such that $n=m^2$.

On the other hand, it is not sensible to define a map $ g \colon \mathbb{Z} \to \mathbb{Z}$ by a rule like $g(m)^2 = m$.
This would have graph $\Gamma(g) = \{(m,n) \in \mathbb{Z}\times\mathbb{Z} : m = n^2\}$
The trouble here is twofold:
First, there are some integers $m$ such that there is no integer $m$ for which $m = n^2$. (No negative integer has a square root, for instance.)
Second, there are other integers $m$ such that there exists \emph{more than one} integer $n$ such that $m = n^2$. (The integer $4$ has two square roots: $2$ and $-2$.)
\end{example}

\begin{example}
Let $n$ be a natural number.
Recall that $\Z/n$ is the set $\{0,1,\dots,n-1\}$.
Then we may define a map $\mu_n \colon \Z \to \Z/n$ by the rule that for any integer $m$,
\[
\mu_n(m) = m\mbox{ mod }n.\]
In other words, $\mu_n(m)$ is the remainder of $m$ after dividing by $n$.
\end{example}

Sometimes, it's handy to use the following notation:
we may define a map $ f \colon S \to T $ by the \emph{assignment}
\[
	s \mapsto \text{[some formula involving $s$].}
\]

\begin{example}
We may define a map $ e \colon \mathbb{R} \to \mathbb{R}$ by the rule
\[
x \mapsto \exp(-x^2).
\]
\end{example}

\begin{example}
Let $T$ be a set, and let $S \subseteq T$ be a subset.
Then there is a map $i \colon S \to T$ given by the rule $i(x)=x$.
This is called the \emph{inclusion map}.
In the particular case in which $S=T$, the map $i$ is called the \emph{identity} map $\mathrm{id} \colon S \to S$.
\end{example}

Maps can be \emph{composed}.
If $f\colon S \to T$ is a map, and $g\colon T \to U$ is another, then you can \emph{do $f$ first, and then do $g$}.
That is, one can form a new map $g\circ f\colon S \to U$ such that for any $s\in S$, one has
\[
	(g\circ f)(s)= g(f(s)) .
\]
More formally, the graph $\Gamma(g\circ f)$ is given by
\[
	\Gamma(g\circ f)=\{(s,u)\in S\times U : (\exists t\in T)((s,t)\in\Gamma(f)\wedge(t,u)\in\Gamma(g))\}.
\]
It is easy enough to see that $g\circ\mathrm{id}=g$ and $\mathrm{id}\circ f=f$, and moreover composition is associative, so that $(h\circ g)\circ f=h\circ(g\circ f)$.

\section{Injective and surjective}%
\label{injectivesurjective}

\begin{definition}
Let $f \colon X\rightarrow Y$ be a function. We say that
\begin{itemize}
\item the map $f$ is \emph{surjective} (or a \emph{surjection} or \emph{onto}) if and only if
\[
(\forall y\in Y)(\exists x\in X)(f(x)=y);
\]

\item the map $f$ is \emph{injective} (or an \emph{injection} or \emph{one-to-one}) if and only if
\[
(\forall x,y\in X)((f(x)=f(y)) \implies (x=y));
\]
and

\item the map $f$ is bijective (or a \emph{bijection} or a \emph{one-to-one correspondence}) if and only if it is injective and surjective.
\end{itemize}
\end{definition}

Let $f \colon X \to Y$ be a map.
The map $f$ is an \emph{injection} if and only if, for every element $y \in Y$, there exists at most one element $x \in X$ such that $y =f(x)$.
The map $f$ is a \emph{surjection} if and only if, for every element $y \in Y$, there exists at \emph{least} one element $x \in X$ such that $y = f(x)$.
Finally, the map $f$ is a \emph{bijection} if and only if, for every element $y\in Y$, there exists \emph{exactly one} element $x \in X $ such that $y = f(x)$.

\begin{center}
\includegraphics[width=300pt]{Figures/inj-surj.png}
\end{center}

\begin{protip}
When proving that a function $f:X\rightarrow Y$ is injective, start by assuming $x,y\in X$ are such that $f(x)=f(y)$ and show that this implies $x=y$. To show a function is not injective, you just need to find one pair $x\neq y\in X$ with $f(x)=f(y)$.
\end{protip}

\begin{example}
The function $f\colon\mathbb{N}_0\rightarrow \mathbb{N}_0$ defined by $f(n)=n^2$ is injective. To see this, let $x,y\in\mathbb{N}_0$ be such that $f(x)=f(y)$. Then $x^2=y^2$, and so
\[
0=x^2-y^2=(x-y)(x+y)
\]
and since $x+y>0$, this means that $x-y=0$, i.e. $x=y$. Thus $f$ is injective. It is not surjective since there is no $n$ so that $f(n)=2$. \\

If we change the domain from $\mathbb{N}_0$ to $\mathbb{Z}$, then $f:\colon\mathbb{Z}\rightarrow \mathbb{N}_0$ is no longer injective, because $f(-1)=f(1)$.
\end{example}

\begin{protip}
When proving that a function $f\colon X\rightarrow Y$ is surjective, start by assuming $y$ is some element of $Y$ and show that there must be an $x\in X$ with $f(x)=y$. To show it is not surjective, you just need to exhibit one $y\in Y$ for which $f(x)\neq y$ for all $x\in X$.
\end{protip}

\begin{example}
The function $g\colon\mathbb{R}\rightarrow [-5,+\infty)=\{x\in\mathbb{R}: x\geq -5\} $ defined by $g(x)=x^2+2x-4$ is surjective. Proof: let $y\geq -5$, we need to show there is $x\in\mathbb{R}$ so that $g(x)=y$, that is, so that $x^2+2x-4=y$. We can use the quadratic formula to get that $x$ can be either
\[
x=\frac{-2\pm \sqrt{4+4(4+y)}}{2}
\]
and since $y\geq -5$, the square root is defined and this gives a solution. Thus, since we can find such an $x\in \mathbb{R}$ for every $y\in [-5,\infty)$, this implies $g$ is injective.

If we change the image to $[-6,+\infty)=\{x\in\mathbb{R} : x\geq -6\}$, then this is not surjective: if $-6=g(x)=x^2+2x-4$ for some $x\in\mathbb{R}$, then the quadratic formula says that $x=-1\pm i$, but these roots are not in the real line. Thus, there is no $x$ so that $g(x)=-6$, and so $g\colon \mathbb{R}\rightarrow [-6,+\infty)$ is not surjective.
\end{example}

The proof of the following result is a good one to study, since it showcases how to prove injectivity and surjectivity for functions.

\begin{theorem}
Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions.
\begin{enumerate}
\item If $f$ and $g$ are injective, so is $g\circ f$.
\item If $f$ and $g$ are surjective, so is $g\circ f$.
\item If $f$ and $g$ are bijective, so is $g\circ f$.
\end{enumerate}
\end{theorem}

\begin{proof}
Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions.
\begin{enumerate}
\item Suppose $f$ and $g$ are injective. To show that $g\circ f$ is injective, we need to show that if $g\circ f(x)=g\circ f(y)$, then $x=y$. So suppose $x,y\in X$ are so that $g\circ f(x)=g\circ f(y)$. Then $g(f(x))=g(f(y))$. Since $g$ is injective, this means $f(x)=f(y)$; since $f$ is injective, this implies $x=y$, as desired. This completes the proof that $g\circ f$ is injective.
\item Now suppose $f$ and $g$ are surjective. To show that $g\circ f$ is surjective, we need to show that for all $z\in Z$, there is $x\in X$ so that $z=g\circ f(x)=g(f(x))$. Since $g$ is surjective, there is $y\in Y$ so that $z=g(y)$; since $f$ is surjective, there is $x\in X$ so that $f(x)=y$, and so $g(f(x))=g(y)=z$, as desired. This finishes the proof that $g\circ f$ is surjective.
\item If $g$ and $f$ are bijective, then they are both injective and surjective, so by the previous two cases, $g\circ f$ is also both injective and surjective, hence $g\circ f$ is bijective.
\end{enumerate}
\end{proof}

When mathematicians are presented with a set, for many purposes, they won't be too very worried about what the elements are, but what structures they have.
The idea is that a bijection of sets is meant to be a \emph{mere labelling} of the elements of a set $S$ by the elements of a set $T$.
That labelling is meant to be a perfect match of information:
you should never use the same label twice, and
all the labels should be used.
So a \emph{bijection} of sets is a map $f\colon S \to T$ such that for any $t\in T$, there exists a unique $s\in S$ such that $f(s)=t$.

\begin{example}
	Let $S$ and $T$ be two sets.
	There is a bijection $ \sigma \colon S \times T \to T \times S $, which is given by $\sigma(s,t) = (t,s)$.
\end{example}

If $ f \colon S \to T $ is a bijection, then there exists a map $ g \colon T \to S $ such that $g \circ f = \mathrm{id} $, and $f \circ g = \mathrm{id}$.
To prove this, let us construct $g$:
the function $f $ gives us a subset $\Gamma(f) \subseteq S \times T$ such that for any $s \in S$, there exists a unique $ t \in T$ such that $(s, t) \in \Gamma(f)$.
So now let's define $g = (T, S, \Gamma(g))$, where $\Gamma(g) = \{ (t,s) \in T \times S : (s, t) \in \Gamma(f) \}$.
Of course $\Gamma(g)$ makes perfect sense as a subset, but we aren't done:
we have to show it is a map from $S$ to $T$.
For this we can use the fact that, since $f$ is a bijection, for every $t \in T$, there exists a unique $s \in S $ such that $(s,t) \in \Gamma(f)$.
In other words, for every $t \in T$, there exists a unique $s \in S $ such that $(t,s) \in \Gamma(g)$.
Thus if $t \in T$, then $g(t) \in S$ is the unique element such that $f(g(t) = t$.
Thus $f \circ g = \mathrm{id}$.
To see that $g \circ f = \mathrm{id}$, let $ s \in S$;
then $g(f(s)) \in S$ is the unique element such that $f(g(f(s))) = f(s)$.
But since this element of $S$ is \emph{unique} with this property, it follows that $g(f(s)) = s $.

The converse is also correct: if $f \colon S \to T$ is a map such that there exists a function $g \colon T \to S $ such that $g \circ f =\mathrm{id} $ and $f \circ g = \mathrm{id}$, then $f$ is a bijection.
Indeed, let $t \in T$ be an element;
we aim to prove that there exists a unique element $ s \in S$ such that $t = f(s)$.
The function $g$ provides us with exactly such an element: $g(t) \in T$ is an element, and $t = f(g(t))$.
Now suppose that $s'\in S$ is an element such that $t = f(s')$;
we see that $g(t) = g(f(s')) = s'$, so we have the uniqueness we sought!

In this case, we say that $g$ is the \emph{inverse} of $f$, and we sometimes write $f^{-1}$ for $g$.

\section{Images and preimages}%
\label{imagesandpreimages}

\begin{definition}
If $f\colon X\rightarrow Y$ and $A\subseteq X$,  the  \emph{image of $A$} under $f$ is  the set
\[
  f(A)=\{f(x):x\in A\} \subseteq Y.
\]
If $B\subseteq Y$, the  \emph{preimage of $B$} under $f$ is
\[
f^{-1}(B)=\{x\in X: f(x)\in B\} \subseteq X.
\]
\end{definition}

Some authors call the preimage the \emph{inverse image}.

This notation is a little ambiguous since we have already let $f^{-1}$ denote the inverse of a bijective function $f$, whereas now we are using it to denote the preimage of a set under a function $f$ that might not be bijective. However, notice that their meaning can be deciphered from context: if $f\colon X\rightarrow Y$ and we ever write $f^{-1}(y)$ for an {\it element} $y\in Y$, you know we are referring to $f^{-1}$ as a function, and if we write $f^{-1}(A)$ for a {\it set $A$}, you know we are talking about the preimage as defined above. Notice also that if $f$ is bijective so that the function $f^{-1}$ exists, then there are two ways of reading what $f^{-1}(A)$ means: it is the image of $A$ under $f^{-1}$, and it is the preimage of $A$ under $f$, but in this case, these two are the same set, so there is no ambiguity.

\begin{example}
If  $f\colon\mathbb{N}\rightarrow \mathbb{N}$ is $f(n)=n+1$, and $A=\{2n:n\in\mathbb{N}$ are the even integers, then the image of $A$ under $f$ is
\[
f(A)=\{f(n):n\in A\} = \{f(2n): n\in\mathbb{N}\} = \{2n+1:n\in\mathbb{N}\},\]
that is, $f(A)$ are the odd integers.
\end{example}

\begin{theorem}
Let $f:X\rightarrow Y$ be a function and $A,B\subseteq Y$. Then
\[
f^{-1}(A\cap B)=f^{-1}(A)\cap f^{-1}(B)
\]
and
\[
f^{-1}(A\cup B)=f^{-1}(A)\cup f^{-1}(B)
\]
\end{theorem}

\begin{proof}
To show the first equality, note that $x\in f^{-1}(A\cap B)$ if and only if $f(x)\in A\cap B$, that is, $f(x)\in A$ and $f(x)\in B$, which is true if and only if $x\in f^{-1}(A)$ and $x\in f^{-1}(B)$, that is, if and only if $x\in f^{-1}(A)\cap f^{-1}(B)$. Thus, $f^{-1}(A\cap B)=f^{-1}(A)\cap f^{-1}(B)$. The second equation has a similar proof (basically just change the ``and's'' to ``or's'').
\end{proof}

\section{Comparing sizes of sets with functions}%
\label{sizesofsets}

%\indent One of the main reasons that we wish to do so is that they give us a way of comparing sizes of sets to each other. Take for example the meaning of {\it finite}: A set $S$ is finite if we can count off the elements as $S=\{s_{1},s_{2},...,s_{n}\}$. Implicitly, the act of counting defines a function $f(j)=s_{j}$ from $\{1,2,...,n\}$ to $S$. That is, $S$ if finite because we are able to construct a function from the first $n$ integers to $S$ that pairs an element from one set with one from the other.

In the earlier diagram, note that whenever the function is injective there has to be at least as many elements in $B$ as there are in $A$, otherwise the arrows from $A$ couldn't point to distinct points in $B$. Similarly, when the function is surjective, there needs to be at least as many points in $A$ as there are in $B$, since the number of arrows (and hence the number of places they point to in $B$) is at most the size of $A$. We make this more precise in the following theorem. For a finite set $A$, we will let $|A|$ equal the size of $A$, that is, the number of elements in $A$.

\begin{theorem}
\label{t:f-size}
Let $X$ and $Y$ be finite sets and let $f:X\rightarrow Y$ be a function.

\begin{enumerate}
\item If $f$ is injective, then $|X|\leq |Y|$.
\item If $f$ is surjective, then $|X|\geq |Y|$.
\item If $f$ is bijective, then $|X|=|Y|$.
\end{enumerate}
\end{theorem}

\begin{proof}
Since $X$ is finite, we may number the elements $\{x_{1},...,x_{n}\}$ where $n=|X|$.
\begin{enumerate}
\item If $f$ is injective, then the element $f(x_{1}),...,f(x_{n})$ are distinct, so there are exactly $n$ elements in the set $S=\{f(x_{1}),...,f(x_{n}))\}$, and since this is a subset of $Y$, $Y$ must have at least as many elements as $S$, so $|Y|\geq n=|X|$.
\item If $f$ is surjective, then if we number the elements $Y=\{y_{1},...,y_{m}\}$ where $m=|Y|$, then for each $i=1,...,m$, there is $s_{i}\in X$ so that $f(s_{i})=y_{i}$. Each of the $s_{i}$ are distinct, since otherwise if $s_{i}=s_{j}$, for some $i\neq j$, then $y_{i}=f(s_{i})=f(s_{j})=y_{j}$, which is a contradiction since $y_{i}$ and $y_{j}$ are distinct. Thus, there are $m$ distinct elements $s_{1},...,s_{m}$ in $X$, and so $|X|\geq m=|Y|$.
\item This follows by combining (1) and (2).
\end{enumerate}

\end{proof}

\section{Equivalence Relations}%
\label{equivalencerels}

In this section we will talk about relations. A relation, loosely speaking, is a way two things are connected to each other, and there are many different ways that things can be related. The relation ``='' is an example, two things are related via ``='' if they are actually equal. Another example is the relationship that one number is at most another, and we write $a\leq b$ when we want to say $a$ and $b$ are related in this way. A less mathematical example is the relation ``daughter of,'' so for example ``$a$ is the daughter of $b$,'' let's denote this relationship by $a\sim b$.

Now we give a precise definition of a relation.

\begin{definition}
A {\it relation} $R$ on a set $S$ is just a subset of $S\times S$. Given $a,b\in S$, we write $aRb$ if $(a,b)\in R$.
\end{definition}

This may seem like a strange way of defining relations, but this definition encapsulates what we do when we define a relation: a relation is a specific way of pairing up elements of a set. In the earlier example, the daughter-of relation is the set of all pairs of people $(a,b)$ where $a$ is the daughter of $b$. Another familiar example is $\leq$: we declare $a\leq b$ if the pair $(a,b)$ is among the set of pairs where $a$ is at most $b$.

Below we mention some important properties relations can have:

 \begin{definition}
 Let $S$ be a set. Let $\sim$ be a relation on $S$. We say $\sim$ is
\begin{itemize}
\item {\it reflexive} if $a\sim a$ for all $a\in S$,
 \item {\it symmetric} if for all $a$ and $b$ in $S$,  we have $
a\sim b  \Longleftrightarrow  b\sim a$, and
 \item {\it transitive} if for all $a$, $b$, $c$ in $S$,  we have
\[
a\sim b\sim c\Longrightarrow  a\sim c.\]
\end{itemize}
If $\sim$ satisfies all three of these properties, we say $\sim$ is an {\it equivalence relation}.
\end{definition}

The relations we discussed earlier satisfy different combinations of these properties.

\begin{example}
\item Equality is an equivalence relation. We also showed last week that $\equiv \mod m$ is an equivalence relation.
\item $\leq$ is reflexive and transitive but not symmetric ($1\leq 2$ but $2\not\leq 1$).
\item The daughter-of relation $\sim$ is not symmetric: if $a$ is the daughter of $b$, then $b$ is certainly not the daughter of $a$, and it is certainly not reflexive since no one is the daughter of themselves. Also,and the daughter-of relation is not transitive (if $a$ is the daughter of $b$ and $b$ is the daughter of $c$, that does not mean $a$ is the daughter of $c$).
\end{example}

The moral here is that a relation does not necessarily satisfy all or any of the three properties above. So whenever we define a relation, we need to check these properties carefully.

\begin{protip}
{\bf Disproving properties:} Remember that to disprove a statement like $\forall x\;\; P$, you want to prove the negation, which is $\exists x\mbox{ s.t. } \bar{P}$. Hence, to disprove a property like symmetry, it suffices to just find {\it one} pair $a$ and $b$ for which $a\sim b$ but $b\not\sim a$. In the previous example, clearly $\leq  $ does not satisfy symmetry for more than just the numbers $1$ and $2$, but to verify that symmetry fails, we only need to show it fails for one pair.
\end{protip}

{\bf Why care?} Relations and equivalence relations come about when there is a collection of objects and you would like to treat many of them as really the same object. Take for example the rational numbers $\mathbb{Q}$, these are the sets of fractions $\frac{p}{q}$ where $p\in\mathbb{Z}$ and $q\in \mathbb{N}$. But notice that the notation $\frac{p}{q}$ is really just an ordered pair, we could have also just written $(p,q)$. But we know that several fractions should give the same number, that is $\frac{2}{1}=\frac{4}{2}$. Really what we're doing here is defining an equivalence relation ``='' by saying $\frac{p}{q}=\frac{r}{s}$ if there is an integer $a$ so that either $(p,q)=(ar,as)$ or $(r,s)=(ap,aq)$. We will also see in the next chapter that functions are also relations. You'll see more important examples in future classes.

\def\cl{{\rm cl}}

\begin{definition}
If $\sim$ is an equivalence relation on a set $S$ and $x\in S$, the {\it equivalence class} of $x$ is
\[
\cl(x) = \{y\in S : x\sim y\}.
\]
The set of {\it equivalence classes} is the set $X/{\sim} = \{\cl(x) : x\in S\}$.
This set comes with a map $\cl \colon X \to X/{\sim}$.
\end{definition}

As long as $X \neq \varnothing$ the map $\cl$ is a surjection.

\begin{example}
Consider the relation on people that $a\sim b$ if $a$ and $b$ were born in the same country. One can check this is an equivalence relation. What if we want to describe the equivalence classes? We could just say ``they are the sets $\cl(x)$ where $x$ ranges over all people'' but this is not helpful in general; this says no more about the relation or what the classes look like than making the same statement about any other relation. A more natural way of describing the equivalence classes is that they are just the sets of people born in a given country, that is, they are the sets $A_x$ of people born in country $x$ as $x$ ranges over all countries.
\end{example}

Note that in the previous example, the relation partitioned people into disjoint groups based on their country of birth. This happens with all equivalence relations.

\begin{example}
Consider the equivalence relation on $\mathbb{Z}$ defined by $\equiv \mod m$ where $m\in\mathbb{N}$. What are the equivalence classes? If $x\in \mathbb{Z}$, then the equivalence class associated to $x$ is
\begin{align*}
\cl(x)
&  =\{ y\in\mathbb{Z} : x\equiv y \mod m\}\\
& = \{y\in\mathbb{Z} : y=x+mj\mbox{ for some integer }j\} \\
& = \{x+mj : j\in\mathbb{Z}\}.
\end{align*}
Notice that for each $x$ there is $i\in \{0,1,...,m-1\}$ so that $x\equiv i\mod m$, and so $\cl(x) = \cl(i)$. Thus, all equivalence classes are one of
\[
\cl(0),\cl(1),...,\cl(m-1)
\]
that is, they are the sets of arithmetic progressions of the form $...,i-2m,i-m,i,i+m,i+2m,...$ for some $i\in \{0,1...,m-1\}$.
Observe that $i \mapsto \cl(i)$ defines a bijection from $\Z/m = \{0,1,\dots,m\}$ to the set $\{\cl(0),\dots,\cl(m-1)\}$.
In this way, we think of $\Z/m$ as the ``set of integers, modulo $m$.''
\end{example}

\section{Exercises}%
\label{moresettheoryexercises}

The relevant exercises in Liebeck's book are in Chapter 19.

\begin{exercise}
 Consider the following functions. Decide whether each composition is 1-1, onto, both or neither. Do any of the compositions have an inverse function?

\begin{enumerate}[label=(\alph*)]
\item $f_1 \colon \R \to \Z$ defined by $f_1(x) = \lfloor x \rfloor$
\item $f_2 \colon \Z \to \Z$ defined by $f_2(x) = x+1$
\item $f_3 \colon \Z \times \Z \to \Z$ defined by $f_3(x,y) = xy$
\item $f_4 \colon \R \to \Z \times \Z$ defined by $f_4(x) = (\lfloor x \rfloor, \lceil x \rceil)$
\item $f_5 \colon \{ 1,\dots,10\} \to \{0,1\}$ defined by $f_5(x) = \begin{cases} 0 & \text{ if }x \text{ is even} \\1 & \text{ if }x \text{ is odd} \end{cases}$
\item $f_6 \colon \{0,1\} \to \{ 1,\dots,10\}$ defined by $f_6(0) = 10$, $f_6(1) = 7$
\end{enumerate}

\begin{solution}
We can form the composition of $f \colon A \to B$ and $g \colon C \to D$ if and only if $B=C$. In some of the above examples we have that $B \subset C$. In this case, although we can morally do the composition, in practice we would need to do the composition $g \circ f' \circ f \colon A \to D$ where $f' \colon B \to C$ is defined by $f'(x)=x$.

Therefore the valid compositions are
\begin{enumerate}[label=(\alph*)]
 \item $f_2 \circ f_1 \colon \R \to \Z$ defined by $f_2 \circ f_1(x) = \lfloor x \rfloor + 1$. This is not 1-1 since 1 and 1.5 have the same image. It is onto, because $n-1$ maps to $n$ for every $n \in \Z$.
\item $f_2 \circ f_2 \colon \Z \to\Z$ defined by $f_2 \circ f_2(x) = x+2$. It is both 1-1 and onto, so is a bijection. Its inverse is the function $f'\colon \Z \to \Z$ defined by $f'(x) = x-2$.
 \item $f_2 \circ f_3 \colon \Z \times \Z \to \Z$ defined by $f_2 \circ f_3(x,y) = xy+1$. Not 1-1, since $(1,1)$ and $(-1,-1)$ have the same image. It is onto, since $(x-1,1)$ maps to $x$ for all $x \in \Z$.
 \item $f_3 \circ f_4 \colon \R \to \Z$ defined by $f_3 \circ f_4 (x) = \lfloor x \rfloor \lceil x \rceil$. Not 1-1, since 0.1 and 0.2 have the same image. Not onto, because we can only get products of consecutive integers. So, for example, there is no $x$ which maps to $14 = 2 \times 7$.
 \item $f_6 \circ f_5 \colon \{1,\dots, 10\} \to \{1,\dots, 10\}$ defined by $f_6 \circ f_5(x) = \begin{cases} 10 & \text{ if }x \text{ is even} \\7 & \text{ if }x \text{ is odd} \end{cases}$. This is not 1-1 since 2 and 4 have the same image. It is not onto because nothing maps to 1.
 \item $f_5 \circ f_6 \colon \{0,1\} \to \{0,1\}$ defined by $f_5 \circ f_6(0) = 0$, $f_5 \circ f_6(1) = 1$. This is both 1-1 and onto and is its own inverse function. (So just because $f \circ g$ is bijective, doesn't mean $g \circ f$ is bijective!)
\end{enumerate}

\end{solution}

\end{exercise}

%\begin{exercise}
%Show that if $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ are  functions, $g\circ f$ is injective, and $f$ is surjective, then $g$ and $f$ are bijections.
%
%
%\end{exercise}

\begin{exercise} Let $f:X\rightarrow Y$ be a function.

\begin{enumerate}[label=(\alph*)]
\item Show that $f$ is injective if and only if for all sets $Z$ and functions $g,h:Z\rightarrow Y$, $f\circ g=f\circ h$ implies $g=h$.
\item Show that $f$ is surjective if and only if for all sets $Z$ and functions $g,h:Y\rightarrow Z$, $g\circ f = h\circ f$ if and only if $g=h$.
\end{enumerate}

\end{exercise}

\begin{exercise}
Let $f:X\rightarrow Y$.
\begin{enumerate}[label=(\alph*)]
\item Show that $f(f^{-1}(B))\subseteq B$ for all $B\subseteq Y$, and give an example of an $f:X\rightarrow Y$ and a set $B\subseteq Y$ where $f(f^{-1}(B))\neq B$.
\begin{solution}
Note that $f^{-1}(B)=\{x\in X|f(x)\in B\}$, so if $x\in f^{-1}(B)$, $f(x)\in B$, hence $f(f^{-1}(B))\subseteq B$.

If we let $f=\cos:\mathbb{R}\rightarrow \mathbb{R}$ and $B=[2,3]$, then $f^{-1}(B)=\emptyset$, so $f(f^{-1}(B))=\emptyset\neq B$.
\end{solution}
\item Show that $f(f^{-1}(B))=B$ for all $B\subseteq Y$ if and only if $f$ is surjective.
\begin{solution}
Suppose $f$ is surjective. Then for all $y\in B$ there is $x\in X$ so that $f(x)=y$. This means $x\in f^{-1}(B)$ since $f(x)=y\in B$.  This means $y=f(x)\in f(f^{-1}(B))$ for all $y\in B$, thus $B\subseteq f(f^{-1}(B))$, and by (a) we then know $B = f(f^{-1}(B))$.
\end{solution}
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $f:X\rightarrow Y$.
\begin{enumerate}[label=(\alph*)]
\item Show that $f^{-1}(f(A))\supseteq A$ for all $A\subseteq X$, and show that these two sets can be unequal.
\begin{solution}
Note that $f^{-1}(f(A))$ is the set of $z\in X$ for which $f(z)\in f(A)$, which includes $A$. If we let $f=\cos:\mathbb{R}\rightarrow \mathbb{R}$ again but let $A=[0,2\pi]$, then $f(A)=[-1,1]$, and $f^{-1}(f(A))=\mathbb{R}\neq A$.
\end{solution}
\item Show that $f^{-1}(f(A))=A$ for all $A\subseteq X$ if and only if $f$ is injective.
\end{enumerate}
\end{exercise}

\begin{exercise} Let $X$ and $Y$ be finite sets, $m\in \mathbb{N}$, and suppose $f:X\rightarrow Y$ is a function such that for all $y\in Y$, $|f^{-1}(\{y\})|=m$ . Then
\[
|X|=m\cdot |Y|.\]

\begin{solution}

If we number the elements of $Y$ as $y_{1},...,y_{n}$ where $n=|Y|$. Then the sets $f^{-1}(\{y_{1}\}),f^{-1}(\{y_{2}\}),...,f^{-1}(\{y_{n}\})$ partition $X$. Thus,
\[
|X|=\sum_{i=1}^{n}| f^{-1}(\{y_{i}\})| = \sum_{i=1}^{n}m=nm=m|Y|
\]
\end{solution}

\end{exercise}

\begin{exercise}
Let $f:X\rightarrow Y$ be a function and $A,B\subseteq X$. Show that $f(A\cup B)=f(A)\cup f(B)$. Give an example of when we can have $f(A\cap B)\neq f(A)\cap f(B)$.
\end{exercise}

\begin{exercise} Let $f:X\rightarrow Y$ be a function. For $a,b\in X$, declare $a\sim b$ if $f(a)=f(b)$.
Show that $\sim$ defines an equivalence relation on $X$.

\begin{solution}
{\bf Claim:}Let $f:X\rightarrow Y$ be a function. For $a,b\in X$, delcare $a\sim b$ if $f(a)=f(b)$. Then $\sim$ defines an equivalence relation on $X$.

\begin{proof}
We need to verify reflexivity, symmetry, and transitivity:
\begin{itemize}
\item (Reflexivity) Let $x\in X$. Since $f(x)=f(x)$, we have $x\sim x$, so reflexivity holds.
\item (Symmetry) Let $x,y\in X$ and suppose $x\sim y$. Then $f(x)=f(y)$, so $f(y)=f(x)$, thus $y\sim x$ and symmetry holds.
\item (Transitivity)  Let $x,y,z\in X$ and suppose $x\sim y$ and $y\sim z$. Then $f(x)=f(y)=f(z)$, so $f(x)=f(z)$, hence $x\sim z$. This proves transitivity.
\end{itemize}
\end{proof}
\end{solution}

\end{exercise}

\begin{exercise}  Let $f:X\rightarrow Y$ and $\sim$ be as in the previous problem.  For $y\in f(X)$, define  $F(y)=f^{-1}(\{y\})$ (where recall $f^{-1}(A)=\{x\in X: f(x)\in A\}$).  Show that $F$ is a bijection between $f(X)$ and the set of equivalence classes in $X$ under the relation $\sim$. {\it Hint:
As a first step, you'll need to show the values of $F$ are indeed equivalence classes. }

\begin{solution}

For short, we will write $f^{-1}(y)$ to mean $f^{-1}(\{y\})$.
{\bf Claim:} Let $S$ denote the set of equivalence classes under $\sim$. Let $y\in f(X)$ and define  $F(y)=f^{-1}(y)$. Then $F:f(X)\rightarrow S$ is a bijection.

\begin{proof}
For $y\in f(X)$, $f^{-1}(y)$ is an equivalence class: since $y\in f(X)$, $y=f(x)$ for some $x\in X$, and if $[x]$ denotes the equivalence class for $x$,
\[
[x]=\{z\in X:x\sim z\} = \{z\in X:f(z)=f(x)\}
= \{z\in X:f(z)=y\}
 =f^{-1}(\{y\}).
 \]
hence $F$ is a function from $f(X)$ to $S$. We need to show that it is injective and surjective:

\begin{itemize}
\item (Injectivity) Let $y,z\in f(X)$ and suppose $F(y)=F(z)$, we will show that $y=z$. Since $F(y)=F(z)$, this means $f^{-1}(y)=f^{-1}(z)$. Let $x\in f^{-1}(y)$, so $f(x)=y$. But then $x\in f^{-1}(y)=f^{-1}(z)$, so $f(x)=z$, thus $y=f(x)=z$. This proves injectivity.
\item (Surjectivity) Let $[x]\in S$ be an equivalence class. Let $y=f(x)$. Then $[x]=f^{-1}(y)$ and $y\in f(X)$. Thus, $F(y)=f^{-1}(y)=[x]$. This implies surjectivity.
\end{itemize}
\end{proof}

\end{solution}
\end{exercise}

\begin{exercise} Show that $f:\mathbb{R}\rightarrow \mathbb{R}$ defined by $f(x)=x^{2n+1}$ is bijective for $n\in\mathbb{N}$.

\begin{solution}
Recall that
\[
1+z+\cdots + z^{n} = \frac{1-z^{n+1}}{1-z}
\]
and so if $x\neq y$, and $x\neq 0$, applying this formula with $z=(y/x)^2$,
\[
x^{2n+1} - y^{2n+1} = x^{2n+1} \left(1-\left(\frac{y}{x}\right)^{2n+1}\right)
= x^{2n+1} \left(1-\left(\frac{y}{x}\right)^2\right)\left(1+\left(\frac{y}{x}\right)^2+\cdots + \left(\frac{y}{x})^{2n}\right)\right).
\]
and this can't be zero if $x\neq 0$ and $x\neq y$. If $x=0$ and $y\neq x=0$, then we clearly have $f(x)\neq f(y)$. Thus, $f$ is injective.
\end{solution}

\end{exercise}

%\begin{exercise} Show that the following functions are bijective:
%
%\begin{enumerate}[label=(\alph*)]
%\item $f:\mathbb{H}\rightarrow \mathbb{D}$ where $\mathbb{H} = \{x+iy\in \C: y>0\}$, $\mathbb{D}=\{z\in\C:|z|<1\}$, and $f(z)=\frac{z+i}{z-i}$.
%
%\item $f:\mathbb{H}\rightarrow \C\backslash (0,\infty)$ (that is, $\mathbb{C}\backslash (0,\infty)$ is the set of all complex numbers minus the positive real numbers) and $f(z)=z^2$.
%
%\end{enumerate}

%\end{exercise}

\begin{exercise} {\bf Quite Challenging!} Find bijections between the following sets:

\begin{enumerate}[label=(\alph*)]
\item $\R$ and $(0,\infty)$

\begin{solution}
Let $f:(0,\infty) \rightarrow \R $ be $f(x)=x-\frac{1}{x}$. Then $f$ is increasing, so it is injective. Furthermore, if $y\in \R$, we can solve $f(x)=y$ by solving the equation
\[
y= x-\frac{1}{x},
\]
and rearranging, this is equivalent to
\[
0=x^2-yx-1,\]
which has solution
\[
x=\frac{y\pm\sqrt{y^2+4}}{2},\]
and one of these solutions is in $(0,\infty)$, thus $f$ is surjective, and hence bijective.
\end{solution}

\item $(0,\infty)$ and $(1,\infty)$

\begin{solution}
We can take $f:(0,\infty)\rightarrow (1,\infty)$ to be $f(x)=x+1$.
\end{solution}

\item ({\bf Even more challenging!}) $(0,\infty)$ and $[0,\infty)$

\item $(0,\infty)$ and $(0,1)$.
\end{enumerate}

\end{exercise}

\begin{exercise}
 The following argument supposedly shows that the reflexivity condition is unnecessary for a nonempty relation $\sim$, that is, it can be derived from symmetry and transitivity:

(a) Suppose $a\sim b$.

(b) By symmetry, $b\sim a$.

(c) Since $a\sim b$ and $b\sim a$, by transitivity, $a\sim a$. Therefore, $\sim $  is reflexive.

What's wrong with this argument?

\begin{solution}
The problem with this argument is (a), as it asserts that, for every $a$, there is a $b$ so that $a\sim b$. But not every relation requires every element to be related to something.

A counter example could be the relation on $\{1,2\}$ where only $1\sim 1$ (so $2\not\sim 2$ and $2\not\sim 1$). Then it is symmetric and transitive, but not reflexive.
\end{solution}
\end{exercise}

\begin{exercise}
Determine which of the following relations $\sim$ on the given set $S$ are reflexive, symmetric, or transitive.
\begin{enumerate}
\item $S$ is the set of lines in $\mathbb{R}^{2}$ and $a\sim b$ if they intersect.
\item $S=\mathbb{C}\backslash \{0\}$ and $x\sim y$ if $x$ and $y$ are contained in the same line passing through the origin.
\item $S$ is the set of subsets of $\mathbb{R}$ and $A\sim B$ if $A\subseteq B$.

\end{enumerate}

\end{exercise}

%
%\begin{exercise}
%Let $X,Y,Z$ be sets, and let $f:X\to Y$ and $g:Y\to Z$ be functions.
%
%\begin{enumerate}[label=(\alph*)]
%\item Given that $g\circ f$ is onto (=surjective), can you deduce $f$ is onto?
%
%\item Given that $g\circ f$ is onto (=surjective), can you deduce $g$ is onto?
%
%\item Given that $g\circ f$ is 1-1 (=injective), can you deduce that $f$ is 1-1?
%
%\item Given that $g\circ f$ is 1-1 (=injective), can you deduce that $g$ is 1-1?
%\end{enumerate}
%{\bf Note:} As always, you must prove your assertion either way in the above two problems.  If yes, give a general proof, if no, give a counter-example. As a hint, you should be able to get a complete intuition for what's going on (and set of counter-examples) by considering sets of size at most $2$.
%\end{exercise}
%
%
%
%\begin{solution}
%We claim that
%
%(a) There exists sets $X, Y, Z$, and functions $f:X\to Y$ and $g:Y\to Z$ with $g\circ f$ surjective but $f$ not surjective.
%
%(b) For any composable functions $f:X\to Y$ and $g:Y\to Z$, if $g\circ f$ is surjective, then $g$ is onto.
%
%(c) For any composable functions $f$ and $g$, if $g\circ f$ is injective (=injective), then $f$ is injective.
%
%(d) There are sets $X, Y, Z$, and functions $f$ and $g$ with $g\circ f$ injective (=injective), but $g$ not injective.
%
%
%\begin{proof}
%(a) Let $X=\{1\}, Y=\{1,2\}, Z=\{1\}$.  Let $f:X\to Y$ send $1$ to $1$, and let $g:Y\to Z$ send both elements to $1$.  Then clearly $g\circ f$ is surjective but $f$ is not, as it does not hit $2$ in $Y$.
%
%(b) To show that $g$ is surjective, we need to show that for all $z\in Z$ there is $y\in Y$ so that $g(y)=z$. Since $g\circ f$ is surjective, there is $x\in X$ so that $g\circ f(x)=g(f(x))=z$. Since $f(x)\in Y$, we have that $y=f(x)$ is such that $g(y)=z$, and this finishes the proof.
%%We prove the contrapositive.  Suppose there is an element $z$ in $Z$ which is not in the image of $g$.  Then there is no $y\in Y$, which maps to it under $g$, and hence no $x$ which maps to $z$ under $g\circ f$.
%
%
%
%(c) To show that $f$ is injective, we need to show that for all $x,y\in X$, if $f(x)=f(y)$, then $x=y$. So let $x,y\in X$ and suppose $f(x)=f(y)$. Then $g\circ f(x)=g(f(x))=g(f(y))=g\circ f(y)$, and since $g\circ f$ is injective, we have $x=y$. This finishes the proof.
%
%%We again prove the contrapositive.  Supose $f$ is not $injective$.  Then there exists $x_1\neq x_2\in X$ with $f(x_1)=f(x_2)$, hence
%%$g(f(x_1))=g(f(x_2))$.
%(d) The same example as in (a) works.
%
%
%\end{proof}
%
%
%\end{solution}

\chapter{Applications: What's bigger than $\infty$?*}%
\label{infinitecardinals}

This section is not required reading, but it is a preview into another aspect of mathematics that you will learn about later: cardinality

Consider the following question: {\it How do we compare sizes of infinite sets?} The sets $\mathbb{N}$, $\mathbb{Q}$, and $\mathbb{R}$ are all infinite, but is any one {\it more infinite} than the other?

We first need a notion of size. Recall that in Theorem \ref{t:f-size} we showed how two finite sets that have a bijection between them have the same size. We use this idea to define when two infinite sets have the same size:

\begin{definition}
We say two sets $A$ and $B$ have the same {\it cardinality} if there is a bijective function $f:A\rightarrow B$, and we write $|A|=|B|$ or $A\sim B$. If there is an injective man $f:A\rightarrow B$, we write $|A|\leq |B|$, and if $|A|\leq |B|$ and there is no injective map from $B$ to $A$, we write $|A|<|B|$.

When $A\sim \mathbb{N}$, we say $A$ is {\it countable}.
\end{definition}

\begin{exercise}
Show that the relation $\sim$ is an equivalence relation on the collection of sets.
\end{exercise}

The word {\it countable} comes from the fact that, if $A\sim \mathbb{N}$, then there is a bijection $f:\mathbb{N}\rightarrow A$, and so
\[
A=\{f(1),f(2),...\},\]
that is, we can count off the elements of $A$ in a list.

\begin{example}
We will show $\mathbb{Z}\sim \mathbb{N}$ by constructing an explicit bijection from $\mathbb{Z}$ to $\mathbb{N}$. Let
\[
f(n) = \left\{ \begin{array}{cl}
2n & n\geq 0 \\
-2n+1 & n<0\end{array}\right.
\]
\end{example}

More surprising is the following:
\begin{theorem}
$\Q\sim \N$.
\end{theorem}

\begin{proof}
The proof of this uses the so-called ``zig-zag'' trick: Write all the rational numbers in an infinite array, and then start listing all the rationals by going up and down each diagonal as follows:

 Let us add draw arrows in this infinite matrix as
follows

%$$\xymatrixrowsep{0.2cm}
%\xymatrixcolsep{0.3cm}\xymatrix{
%&\frac{1}{1}\ar@{->}[r] &\frac{2}{1}\ar@{->}[dl]&\frac{3}{1}\ar@{->}[r] &\frac{4}{1}\ar@{->}[dl]&\frac{5}{1}\ar@{->}[r] & \frac{6}{1}\ar@{->}[dl]& \frac{7}{1}\ar@{->}[r] & \frac{8}{1}\ar@{->}[dl]& \frac{9}{1}\ar@{->}[r] & \frac{10}{1}\ar@{->}[dl]& \frac{11}{1}\ar@{->}[r] &\cdots\ar@{->}[dl]&\\
%&\frac{1}{2}\ar@{->}[d] &\frac{2}{2}\ar@{->}[ur]&\frac{3}{2}\ar@{->}[dl]&\frac{4}{2}\ar@{->}[ur]&\frac{5}{2}\ar@{->}[dl]& \frac{6}{2}\ar@{->}[ur]& \frac{7}{2}\ar@{->}[dl]& \frac{8}{2}\ar@{->}[ur]& \frac{9}{2}\ar@{->}[dl]& \frac{10}{2}\ar@{->}[ur]& \frac{11}{2}\ar@{->}[dl]&\cdots &\\
%&\frac{1}{3}\ar@{->}[ur]&\frac{2}{3}\ar@{->}[dl]&\frac{3}{3}\ar@{->}[ur]&\frac{4}{3}\ar@{->}[dl]&\frac{5}{3}\ar@{->}[ur]& \frac{6}{3}\ar@{->}[dl]& \frac{7}{3}\ar@{->}[ur]& \frac{8}{3}\ar@{->}[dl]& \frac{9}{3}\ar@{->}[ur]& \frac{10}{3}\ar@{->}[dl]& \frac{11}{3}\ar@{->}[ur]&\cdots\ar@{->}[dl] &\\
%&\frac{1}{4}\ar@{->}[d] &\frac{2}{4}\ar@{->}[ur]&\frac{3}{4}\ar@{->}[dl]&\frac{4}{4}\ar@{->}[ur]&\frac{5}{4}\ar@{->}[dl]& \frac{6}{4}\ar@{->}[ur]& \frac{7}{4}\ar@{->}[dl]& \frac{8}{4}\ar@{->}[ur]& \frac{9}{4}\ar@{->}[dl]& \frac{10}{4}\ar@{->}[ur]& \frac{11}{4}\ar@{->}[dl]&\cdots &\\
%&\frac{1}{5}\ar@{->}[ur]&\frac{2}{5}\ar@{->}[dl]&\frac{3}{5}\ar@{->}[ur]&\frac{4}{5}\ar@{->}[dl]&\frac{5}{5}\ar@{->}[ur]& \frac{6}{5}\ar@{->}[dl]& \frac{7}{5}\ar@{->}[ur]& \frac{8}{5}\ar@{->}[dl]& \frac{9}{5}\ar@{->}[ur]& \frac{10}{5}\ar@{->}[dl]& \frac{11}{5}\ar@{->}[ur]&\cdots\ar@{->}[dl]&\\
%&\frac{1}{6}\ar@{->}[d] &\frac{2}{6}\ar@{->}[ur]&\frac{3}{6}\ar@{->}[dl]&\frac{4}{6}\ar@{->}[ur]&\frac{5}{6}\ar@{->}[dl]& \frac{6}{6}\ar@{->}[ur]& \frac{7}{6}\ar@{->}[dl]& \frac{8}{6}\ar@{->}[ur]& \frac{9}{6}\ar@{->}[dl]& \frac{10}{6}\ar@{->}[ur]& \frac{11}{6}\ar@{->}[dl]&\cdots &\\
%&\frac{1}{7}\ar@{->}[ur]&\frac{2}{7}\ar@{->}[dl]&\frac{3}{7}\ar@{->}[ur]&\frac{4}{7}\ar@{->}[dl]&\frac{5}{7}\ar@{->}[ur]& \frac{6}{7}\ar@{->}[dl]& \frac{7}{7}\ar@{->}[ur]& \frac{8}{7}\ar@{->}[dl]& \frac{9}{7}\ar@{->}[ur]& \frac{10}{7}\ar@{->}[dl]& \frac{11}{7}\ar@{->}[ur]&\cdots &\\
%& \vdots                & \vdots\ar@{->}[ur]    &    \vdots             & \vdots \ar@{->}[ur]   &   \vdots              &   \vdots\ar@{->}[ur]   &   \vdots               &   \vdots\ar@{->}[ur]   &   \vdots               &   \vdots\ar@{->}[ur]    &   \vdots    & \ddots &}%
%$$

So the list would be:

\[
\frac{1}{1}, \;\; \frac{2}{1}, \;\; \frac{1}{2}, \;\; \frac{1}{3}, \;\; \frac{2}{2}, \;\; \frac{3}{1}, \;\; \frac{4}{1}, \;\; \frac{3}{2}, \;\; \frac{2}{3}, \;\; \frac{1}{4}, \;\; \frac{1}{5}, \;\; \frac{2}{4},...\cdots \]

Note that this list repeats numbers (e.g. the number 1 appears as both $\frac{1}{1}$ and $\frac{2}{2}$). But we can just remove numbers that repeat, so the modified list looks like this:

\[
\frac{1}{1}, \;\; \frac{2}{1}, \;\; \frac{1}{2}, \;\; \frac{1}{3},  \;\; \frac{3}{1}, \;\; \frac{4}{1}, \;\; \frac{3}{2}, \;\; \frac{2}{3}, \;\; \frac{1}{4}, \;\; \frac{1}{5},...\cdots \]

and now this gives a bijection from $\mathbb{N}$ to $\mathbb{Q}$, where we send $n\in\mathbb{N}$ to the $n$th term in this list.

\end{proof}

Not everything is countable though:

\begin{theorem}
Let $S$ be an infinite set and let $P(S)$ denote the {\it power set} consisting of the subsets of $S$, i.e. $P(S)= \{A\subseteq S\}$. Then $|S|<|P(S)|$.
\end{theorem}

\begin{exercise}
Prove the above theorem. {\it Hint: Suppose there was $f:S\rightarrow P(S)$ bijective and think about the set $B=\{x\in S: x\not\in f(x)\}$.}
\end{exercise}

\begin{solution}
\begin{proof}
Suppose there was $f:S\rightarrow P(S)$ bijective, let $B=\{x\in S: x\not\in f(x)\}$. Since $f$ is bijective, it is surjective, and so there is $x$ so that $f(x)=B$.
\begin{enumerate}
\item If $x\in B$, then by definition of $B$, $x\not\in f(x)=B$, a contradiction.
\item If $x\not\in B$, then by definition of $B$, $x\in f(x)=B$, again a contradiction.
\end{enumerate}
Since these are the only two possibilities, we have a full contradiction. Thus, there cannot be a function $f:S\rightarrow P(S)$ bijective, thus $|S|\neq |P(S)|$. Since there is a clear injection $g:S\rightarrow P(S)$ defined by $g(x)=\{x\}$, we have that $|S|\leq |P(S)|$, and so $|S|<|P(S)|$.
\end{proof}
\end{solution}

A few natural questions arise from our discussions on cardinality. Firstly, we have seen that $|\N|<|\R|$, but is there anything in between? This is the {\it continuum hypothesis} posed by George Cantor:\\

\begin{description}
\item[Continuum Hypothesis (CH)] Is there no set $S$ with $|\mathbb{N}|<|S|<|\mathbb{R}|$?\\
\end{description}

Modern set theory (and mathematics in general) is based on a finite set of axioms called {\bf Zermelo--Fraenkel} set theory ({\bf ZFC}). In 1940, Kurt G\"odel showed that the CH cannot be disproved assuming ZFC, so it would see then that CH has to be true...but then Paul Cohen showed in 1963 that CH cannot be {\bf proved} assuming ZFC. In other words, CH is {\bf independent} of the axioms of mathematics: one can assume it's true or false and it won't contradict anything proven using the ZFC axioms!

%
%Cantor invented the notion of cardinality, countability vs. uncountability, the notion of and laid the groundwork for modern set theory. Until his time, people thought the only sets were finite sets and "infinite," whereas Cantor showed that some sets are more infinite than others, for example, that $|\N|<|\R|$.

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Week 9: Counting}

\chapterimage{Figures/blank.png}

\chapter{Counting}%
\label{counting}

\epigraph{\it    Though combinatorics has been successfully applied to many branches of mathematics, these cannot be compared either in importance or in depth to the applications of analysis in number theory or algebra to topology, but I hope that time and the ingenuity of the younger generation will change this.
}{Paul Erd\H{o}s, 1970.}

This week we will introduce {\it combinatorics}, which is the study of counting. As you can see from the above quotation, Paul Erd\H{o}s\footnote{Paul Erd\H{o}s was one of the greatest combinatorists in history, and the most prolific mathematician, publishing at least 1,525 mathematical papers, more than any other mathematician (though Euler wrote more total pages). His life is incredibly interesting as well, as he spent a large part of it technically homeless, with no permanent address, just travelling and staying with other mathematicians. For a short summary, you can read here ({\url{http://www-history.mcs.st-and.ac.uk/Biographies/Erdos.html}}), otherwise his biography {\it  The Man Who Loved Only Numbers} is a great read.}  felt that combinatorics played second fiddle to the fields of analysis and algebra. However, with the advent of computer science, combinatorics became essential for building efficient algorithms, and thus has a much larger importance for society than Erd\H{o}s could have anticipated back in 1970.\\

If you enjoy the material this week, you may enjoy the course {\it Combinatorics and Graph Theory}.

\section{The Multiplication Principle}%
\label{multiplicationprinciple}

Suppose I toss a coin twice: the total number outcomes is 4, since there are 2 outcomes for what the first coin can be (heads or tails), and then 2 outcomes for what the second coin can be. Hence the overall outcomes are (T,T), (T,H), (H,T), and (H,H), of which there are 4.

We can generalise this to events that involve more than just two possible outcomes:  if I roll a 6-sided die and then an 8-sided die, the total number of outcomes is $6\cdot 8=48$. I can figure this out because, for each of the outcomes for the first die (either $1, 2, \dots , \mbox{ or } 6$), there are 8 possible outcomes for the second die, and so the total of all possible outcomes is $48$. We can generalise this idea using the {\it multiplication principle}.

\begin{theorem}[Multiplication Principle]
Let $P$ be a process consisting of $n$ stages such that at each stage $i$, there are $a_{i}$ choices we can make, and no two distinct choices can result in the same outcome. Then after $n$ stages, the total number of outcomes is $a_{1}\cdot a_{2}\cdots a_{n}$.
\end{theorem}

\begin{proof}
We prove this by induction on $n$. For $n\in \mathbb{N}$, let $P(n)$ be the claim that, if $P$ is a process consisting of $n$ stages such that at each stage $i$, there are $a_{i}$ choices we can make, then there are $a_{1}\cdots a_{n}$ outcomes.
\begin{itemize}
\item[{\bf Base case:}] If $n=1$, then $P$ is a process of one stage where there are just $a_{1}$ choices, and so there are only $a_{1}$ many outcomes. This proves the base case.
\item[{\bf Inductive step:}] Suppose $P(n)$ is true for some $n\in \mathbb{N}$. Suppose now $P$ is a process with $n+1$ stages and the $i$th stage has $a_{i}$ choices. Note that if we stop the process before the last stage, we will have performed a process with $n$ stages and for each $i=1, \dots ,n$, there are $a_{i}$ choices. Then the induction hypothesis implies that there are $a_{1}\cdots a_{n}$ outcomes. However, for each outcome of the first $n$ stages, there are now $a_{n+1}$ choices we can make. So the total number of outcomes after the $n+1$ stage is the total number of outcomes after $n$ steps times the total number of outcomes that can happen from each of those outcomes, that is $(a_{1}\cdots a_{n})\cdot a_{n+1}$. This proves $P(n+1)$ and finishes the proof.
\end{itemize}
\end{proof}

This seems quite abstract, but let's look at a simple example.

\begin{proposition}
\label{c:1}
Let $S$ have $n$ elements. Then the number of subsets of $S$ is $2^{n}$.
\end{proposition}

\begin{proof}
Let $S$ be a set of $n$ elements and list the elements as $S=\{s_{1}, \dots ,s_{n}\}$.

Let us define a process for picking a subset $A$ of $S$: at each stage $i=1,2,\dots ,n$, we decide whether to add $s_{i}$ to our set or exclude it. At the end of the process, we will have picked a subset of $A$, and every subset of $A$ is the outcome of such a process. By the Multiplication Principle, since there are $2$ choices we can make at each stage, there are $2\cdot 2\cdots 2=2^{n}$ many outcomes for what $A$ we could have chosen. Thus, there are $2^{n}$ sets.
\end{proof}

If $S$ is a finite set consisting of exactly $n$ distinct members, we call $n$ the {\em cardinality}
or {\em size} of $S$ and we denote this by $|S| = n$ (or in some texts by $\#(S) =n$).

\begin{definition}
Let $S = \{a_1, \dots , a_n\}$ be a set of $n$ distinct objects. An {\it ordering} (or {\it arrangement}) of $S$ is a sequence $(a_{1}, \dots ,a_{n})$ in which each element of $S$ appears exactly once.
\end{definition}

For example $(1,3,2)$ and $(3,2,1)$ are orderings of $\{1,2,3\}$. We can use the multiplication principle to count the number of orderings of $S$:

\begin{example}
How many ways are to order the set $\{1,2,3\}$?

What we do is develop a process for constructing all of the orderings. Suppose we would like to choose an ordering $(a_{1},a_{2},a_{3})$ where the $a_{i}$ are distinct integers in $\{1, 2, 3\}$. We divide the process into 3 stages, where in the $i$th stage we pick the number $a_{i}$ for our ordering.
\begin{description}
\item[\bf Stage 1:] There are $3$ choices for $a_{1}$: either 1, 2, or 3.
\item[\bf Stage 2:] Having chosen $a_{1}$, there are now only 2 numbers remaining that we could pick to be $a_{2}$. So there are 2 choices for $a_2$.
\item[\bf Stage 3:] Having chosen $a_{1}$ and $a_{2}$ there is now only one choice for $a_{3}$.
\end{description}
\medskip
Thus, by the multiplication principle, there are $3\cdot 2\cdot 1 = 6$ outcomes, that is, $6$ orderings.
\end{example}

In fact, more generally, we have the following:

\begin{theorem}[Ordering Theorem]
Given a set $S$ of $n$ distinct objects, there are $n!$ ways of ordering them.
\end{theorem}

This is a special case of the following proposition, (part (b) with $k=m$).

\begin{proposition}
\label{p:n^k}
Let $S$ be a set with $n$ elements and let $0\leq k\leq n$.
\begin{enumerate}[label=(\alph*)]
\item The number of ways of picking an ordered selection of $k$ elements from $S$ (i.e. an ordered list of elements $(a_{1}, \dots ,a_{k})$, possibly with repetitions, with $a_i\in S$) is $n^{k}$.
\item The number of ways of picking an ordered selection of $k$ {\it distinct} elements from $S$ is $n(n-1)\cdots (n-k+1) = \frac{n!}{(n-k)!}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item We count these possibilities using a $k$-step process: there are $n$ options for $a_1$, $n$ options for $a_2$, and so on. The Multiplication Principle tells us that there are $n\cdot n\cdots n = n^{k}$ outcomes.
\item Again, we use the multiplication principle as in (a), but while there were $n$ options for $a_1$, now there are $n-1$ options for $a_2$ because, for the $a_i$ to be distinct, we cannot repeat, so $a_{2}\neq a_{1}$ and so there are only $n-1$ options for $a_2$. Similarly, for $a_{3}$ there are $n-2$ options for what it can be (since it can't equal $a_{1}$ or $a_{2}$). Continuing in this way, after having picked $a_{1},\dots,a_{i-1}$, there will be $n-i+1$ remaining options for $a_i$. Thus, the total number of sequences $a_{1}, \dots ,a_{k}$ of $k$ {\it distinct} elements of $S$ is $n(n-1)\cdots (n-k+1) $.
\end{enumerate}
\end{proof}

\section{Correspondence and overcounting}%
\label{correspendenceandovercounting}

\indent Sometimes we may try to count the members of some set, but we accidentally count the same elements more than once. We can avoid this by trying to be more careful. However, if we are aware of how many times we overcount, we can also use this to our advantage!

\begin{example}
How many diagonals are there in a regular polygon with $n \geq 3$ sides?\\

This was an exercise in the chapter on induction, but now we will give a different proof. A first attempt is as follows using the Multiplication Principle. Given two vertices $v$ and $w$, we will let $\overline{vw}$ denote the line segment between them. Pick a corner $v$, there are $n$ options. Then there are $n-3$ other corners $w$ that you can pick to form a diagonal $\overline{vw}$ (since we have to exclude the vertex $v$ and the two vertices next to it, since connecting to those from $v$ won't form a diagonal contained in the polygon). Thus, there are $n(n-3)$ ways of picking a corner $v$ and then another corner $w$ so that $\overline{vw}$ forms a diagonal.

However, this is {\it not} the final answer. Notice that in this algorithm, each diagonal actually gets counted twice, since for two nonadjacent corners $v$ and $w$, $\overline{vw}=\overline{wv}$. What we have really counted is the number of ordered pairs $(v,w)$ of corners that are not adjacent, and every diagonal corresponds to exactly two such pairs. Thus, if we divide $n(n-3)$ by two, we get the true number of diagonals, that is, $n(n-3)/2$. (And note that either $n$ or $n-3$ is even!)
\end{example}

The general idea behind overcounting is as follows: suppose you want to count a set of things $A$. Suppose there is another set $B$ that (a) is easier to count and (b) {\it to each $a\in A$ there correspond $m$ elements of $B$}, that is, {\it $B$ and $A$ are in $m$-to-one correspondence}. This means we can pair each element of $A$ with $m$ elements of $B$ so that every element of $B$ belongs to exactly one pairing. If we call the set of these pairs $P$, then $m|A|=|P|$ (since every $a$ is in exactly $m$ pairongs of the form $(a,b)\in P$), and $|P|=|B|$ since every $b\in B$ belongs in exactly one pairing. Thus, we get that $m|A|=|B|$.

In the previous example, we could match every diagonal $d$ with two ordered pairs of nonadjacent corners; the total number of such ordered pairs is $n\cdot (n-3)$, and this is twice the number of diagonals. Thus, the number of diagonals is this number divided by $2$. In the figure below, we show an example of how, to each element in a set $A$, there correspond 3 elements in a set $B$ (and so $|B|=3|A|$).

\begin{center}
\includegraphics[width=300pt]{Figures/corresponds.pdf}
\begin{picture}(0,0)(300,0)
%\putgrid
\put(75,0){$A$}
\put(300,80){$B$}
\end{picture}
\end{center}

\begin{example}
How many ways are there to rearrange the letters in the words ``orange'' and ``banana''? \\

By the Ordering Theorem, there are $6!$ ways of rearranging the letters in ``orange'' since there are six {\it distinct} letters, but for ``banana'' we have to be careful.

Suppose first that we treat each letter as an individual letter, that is, we look at the number of rearrangements of the word ``$ba_{1}n_{1}a_{2}n_{2}a_{3}$''. There are now 6 distinct characters, so there are a total of $6!$ ways of rearranging this word. This isn't the same thing as counting the number of ways of rearranging ``banana'', since if we change the $a_{1},a_{2},a_{3}$ in a rearrangement back into a's and the $n_{1}$ and $n_{2}$ back into n's, there are multiple ways of getting the same rearrangement of ``banana.'' For example, ``nanaba'' can be obtained from ``$n_{1}a_{1}n_{2}a_{3}ba_{2}$'' as well as ``$n_{2}a_{2}n_{1}a_{3}ba_{1}$.'' However, to each arrangement of ``banana,'' there are $3!$ ways of treating the a's as distinct letters and $2!$ ways of treating the n's as distinct letters so that they return the same rearrangement, and hence there are $3!\cdot 2!$ rearrangements of ``$ba_{1}n_{1}a_{2}n_{2}a_{3}$'' that correspond to the same rearrangement of ``banana'' when we turn the $n_i$'s into $n$'s and the $a_i$'s into 'a's. Thus,  counting the number of rearrangements of ``$ba_{1}n_{1}a_{2}n_{2}a_{3}$''  (which is $6!$) overcounts the number of rearrangements of ``banana'' by a factor of $3!\cdot 2!$. Hence, we just need to divide $6!$ by $3!\cdot 2!$ to get the number of rearrangements of ``banana,'' which is
\[
\frac{6!}{3!\cdot 2!} = \frac{720}{12} = 60.
\]
(Note that the original arrangement of the letters in ``orange'' counts as one of the
$6!$ rearrangements.)
\end{example}

\section{Counting subsets}%
\label{countingsubsets}

We will now use the method of overcounting to count the number of subsets of a set $S$ with $|S|=n$ which have a given size $k$. Because this number appears over and over again, we give it a special notation:\\

\begin{definition}
Let $S$ be set with $n$ distinct members, and let $k$ be an integer such that $0\leq k\leq n$. We let
\[
{n \choose k}
\]
denote the number of subsets of $S$ which have exactly $k$ members (and the above is read ``$n$ choose $k$'' because it gives the number of ways we can choose $k$ members from an $n$-element set). (If $k>n$, we set ${n \choose k}=0$).
\end{definition}

\begin{theorem}\label{t:choose}
For integers $0\leq k\leq n$, we have
\begin{equation}
\label{e:choose}
{n \choose k}=\frac{n!}{k!(n-k)!}
.\end{equation}
\end{theorem}

Liebeck gives a different proof in the book of this theorem, so please look at both. Recall that by convention we declare $0! = 1$.

Notice also that
\begin{equation}
{n \choose k}=\frac{n!}{k!(n-k)!} = \frac{n(n-1) \dots(n-k +1)}{k(k-1) \dots 2. 1}.
\end{equation}
For example ${3 \choose 2} = \frac{3!}{2!1!} = 3$,
${6 \choose 3} = \frac{6!}{3!3!} = \frac{6 \cdot 5 \cdot 4}{3 \cdot 2 \cdot 1} =20$ and ${n \choose 0} =1$.

\begin{proof}
If $k=0$, there is only one subset of size $0$, which is the empty set, and in this case we see that $\frac{n!}{0!(n-0)!}=\frac{n!}{n!}=1$. Now let $1\leq k\leq n$ and $S$ be a set of $n$ elements. Consider the process where we pick an ordered list $(a_{1}, \dots ,a_{k})$ of $k$ distinct elements from $S$, one at a time, to form a set $A=\{a_{1}, \dots ,a_{k}\}$ of $k$ elements. By Proposition \ref{p:n^k}, there are $\frac{n!}{(n-k)!}$ ways of picking the list $(a_{1}, \dots ,a_{k})$. However, this is not the same thing as counting the number of subsets of size $k$, since if we picked $a_1, \dots ,a_k$ in a different order, they would still give rise to the same set $A=\{a_{1}, \dots ,a_{k}\}$. What we have really counted is the number of ways of picking an {\it ordered} list of $k$ elements, not a {\it set} of $k$ elements. The number of orderings of $\{a_{1}, \dots ,a_{k}\}$  is $k!$ by the Ordering Theorem. Thus, we have overcounted the number of subsets of size $k$ by a factor of $k!$, hence if we divide$\frac{n!}{(n-k)!}$ by $k!$, we get that the number of subsets of size $k$ is the right side of \eqref{e:choose}.
\end{proof}
Note that it's perhaps not completely obvious that the right-hand side of \eqref{e:choose} actually defines a whole number; however once we can identify it as the number of ways of doing something, it {\em has to be} a nonnegative integer!
%
%
%Let $S=\{1,2,...,n\}$.  By the Rearrangement Theorem, the number of rearrangements is equal to $n!$. Now we count the number of rearrangements a second way via the following process that chooses a rearrangement:
%\begin{itemize}
%\item First, pick a subset $A$ of $S$ of size $k$. There are ${n\choose k}$ options.
%\item Next, pick an arrangement $s_{1}....s_{k}$ of the elements of $A$, there are $k!$ options.
%\item Also pick an arrangement $t_{1}...t_{n-k}$ of $S\setminus A$. There are $(n-k)!$ options.
%\item Let $s_{k+j}=t_{j}$, so now we have picked an arrangement $s_{1}...s_{n}$ of $S$.
%\end{itemize}
%By the Multiplication Principle, the total number of outcomes is ${n\choose k}\cdot k! \cdot (n-k)!$. Thus,
%\[
%{n\choose k} = {n\choose k}\cdot k! \cdot (n-k)! \frac{!}{k!(n-k)!} = \frac{n!}{k!(n-k)!}.
%\]
%This finishes the proof.
%\end{proof}

%One cool aspect about the above theorem is that, at first glance, it isn't clear that $\frac{n!}{r!(n-r)!}$ should be an integer for every choice of integers $0\leq r\leq n$.

\medskip
This argument leads us to a new kind of proof, called a {\it combinatorial proof}, which is a way of proving a formula by showing that both sides of an equation count the same number of things, rather than trying to prove establish the equation directly via algebra. Let's start with an easy example:

\begin{example}
For $0 \leq k \leq n$ we have
\[
{n\choose k} = {n \choose n-k}.
\]
This can be easily shown using the formula \eqref{e:choose}, but we will prove it instead using one-to-one correspondence (that is, $m$-to-one correspondence with $m=1$):

Note that if $S$ is a set of size $n$ and $A$ is a subset of size $k$, then $S\setminus A$ is a set of size $n-k$. Conversely, if $B$ is a set of size $n-k$, then $S \setminus B$ is a set of size $k$.
Therefore to every set of size $k$ we can associate a unique set of size $n-k$ and vice-versa. Thus the sets of size $k$ and sets of size $n-k$ are in one-to-one correspondence, so the number of these sets is the same, that is, ${n\choose k} = {n \choose n-k}$.
(If you need a bit more convincing, check that if
$B = S \setminus A$, then $S \setminus B = A$.) Slightly more formally we describe the correspondence $A \leftrightarrow S \setminus A$ as a {\em bijection} between the sets of size $k$ and the sets of size $n-k$.

%Alternatively, we can prove this with bijections: Let $S_{k}$ be the subsets of $S$ of size $k$ and define a function $f:S_{k}\rightarrow S_{n-k}$ by $f(A)= S\setminus A$. Then $f$ is a bijection with inverse $f^{-1}:S_{n-k}\rightarrow S_{k}$ also defined by $f^{-1}(A)=S\setminus A$. We can check this function is an inverse because $f(f^{-1}(A))=A$ for every $A\in S_{n-k}$ and $f^{-1}(f(A))= A$  for every $A\in S_{k}$. Thus, by Theorem 14.1,
%\[
%{n\choose k} =|S_{k}|=|S_{n-k}|= {n \choose n-k}.
%\]
\end{example}

Let's prove another identity where it may be less clear how to prove it algebraically.

\begin{corollary}
\label{c:2^n=nk}
For $n\geq 0$, we have
\[
2^{n} = \sum_{k=0}^{n} {n\choose k}
.\]
\end{corollary}

\begin{proof}
Let $S$ be a set of size $n$, let $A$ be the set of subsets of $S$, and for $k=0, \dots ,n$, let $A_k$ denote the subsets of $S$ of size $k$, so $A_{k}$ has size ${n\choose k}$ by Theorem \ref{t:choose}. These sets are disjoint and their union is all of $A$, so they form a {\em partition} of $A$. Thus, the size of $A$ is the sum of the sizes of the $A_{k}$. Hence, with $|A|$ denoting the size of $A$,
\[|A|=|A_{1}|+\cdots + |A_{n}|= \sum_{k=0}^{n} |A_{k}|=\sum_{k=0}^{n}{n\choose k}.
\]
We also know by Proposition~\ref{c:1} that $|A|=2^{n}$, and this proves the corollary.
\end{proof}

We have surreptitiously introduced the notion of a partition above. More formally, a {\em partition} of a nonempty finite set $A$ is a collection of subsets $A_1, \dots , A_m$ of $A$ such that $\bigcup_{i=1}^m A_i = A$ and such that for $i \neq j$, $A_i \cap A_j = \emptyset$. In other words, every $x \in A$ belongs to {\em exactly one} $A_i$.

%
%\begin{exercise}
%Prove that for $n\geq i\geq 0$,
%\[
%\sum_{k=0}^{n} {k \choose i} = {n+1\choose i+1}
%\]
%first by induction, then do this by showing that the two sides count the same thing two different ways.
%\end{exercise}
%

%\begin{protip} {\bf Divide and Conquer!} One way to count the number of elements in a set is to split or partition the set up into parts that are easier to count and then add up their sizes. For example, to get started on the above exercise, remember that ${n+1 \choose r}$ counts the number of subsets of size $r$ in a set of size $n+1$. Since we are writing this as a sum of ${n \choose r}$ and ${n \choose r-1}$, this suggests that we are splitting the set of subsets into two sets, counting them separately, and then adding their sizes together. So what you need to figure out is how we are splitting this up to get sets of sizes ${n \choose r}$ and ${n \choose r-1}$ respectively.\\
%
%We demonstrate this method in the following corollary of the above work.
%\end{protip}

\bigskip
Suppose we are given $n$ identical red objects laid out along a line, and we are told to {\em intersperse} $m$ identical blue objects amongst them. (We are allowed to begin and end our new chain of objects with blue objects if we wish.) How many ways of doing this are there?

\begin{proposition}\label{interspersion} {\bf -- Linear interspersion principle.}
The number of ways of interspersing $m$ identical blue objects amongst $n$ identical red objects laid along a line is $n+m \choose m$.
\end{proposition}
\begin{proof}
Each linear arrangement combining the $m$ blue objects and the $n$ red objects corresponds to choosing a subset of size $m$ (the blue objects) from a set of size $m+n$ (all the objects). There are $n+m \choose m$ such choices, and hence $n+m \choose m$ ways of interspersing $m$ identical blue objects amongst $n$ identical red objects laid along a line.
\end{proof}

\begin{example}
\label{ex:1+'s}
How many ways are there to express the number $9$ as a sum of three nonnegative integers? \\

This uses a clever trick which reduces matters to the linear interspersion principle. (It might look intimidating to think it up on your own, but now you'll know it!).

Consider a sum $x+y+z=9$ with $0\leq x,y,z\leq 9$ and replace $x,y,z$ with the number of $1$'s that add up to $x$, $y$ and $z$ respectively separated by $+$'s. For example, $4+3+2=9$ and $5+0+4$ correspond to
\[
1\;\; 1\;\; 1\;\; 1\;\; + \;\;1\;\; 1\;\; 1\;\; +\;\;1\;\; 1\;\; \;\;\;\;\;\mbox{ and }\;\;\;\;\;
1\;\; 1\;\; 1\;\; 1\;\;  1 \;\; + \;\; + \;\; 1\;\; 1\;\; 1\;\; 1\;\; .
\]
{\bf Thus, the number of solutions to $x+y+z=9$ is exactly the number of ways to intersperse 2 +'s amongst 9 1's laid out in a row.} This is the case $n=9$, $m=2$ of the linear interspersion principle, and is the number of such solutions is thus ${9+2 \choose 2} = {11\choose 2}$.
\end{example}

\begin{example}\label{identicalballsinboxes}
How many ways are there to place $9$ identical red balls in three boxes? \\

This is the previous example in disguise! Put the
$9$ balls in a line, and introduce two ``dividers''
to delineate the ``gaps'' between the three boxes.
So, for example

$$ B\;B\;B\;B\; |\; B\; |\; B\; B\;B\;B$$

\noindent
denotes placing $4$ balls in box $1$, $1$ ball in box $2$ and $4$ balls in box 3. The number of ways of inserting two dividers in a line of $9$ balls is therefore ${11\choose 2}$.

\end{example}
%
%
%Let's prove again that the number of subsets of a set of size $n$ is $2^{n}$ using pairing.
%
%\begin{proof}[Second proof of Proposition \ref{c:1}]
%Let $s_{n}$ be the number of subsets of a set of size $n$. We prove by induction on $n$ that $s_{n}=2^{n}$ for all integers $n\geq 0$. For the base case, we observe that if $n=0$, then $S$ has no elements, and so it is the emptyset, and the only subset of the empty set is the empty set. Hence, $s_{0}=1=2^{0}$.
%
%For the induction step, suppose we know that $s_{n}=2^{n}$. Let $S$ be a set of size $n+1$. Fix an element $a\in S$ and let $S'=S\setminus \{a\}$ (that is, $S$ is the set consisting of everything in $S$ apart from $a$). Then $|S'|=|S|-1=n+1-1=n$. Thus, we can apply our induction hypothesis to $S'$ to get that $S'$ has $2^{n}$ subsets. Let $A$ be the set of subsets of $S$ that contain $a$ and $B$ the subsets of $S$ that don't contain $a$. Then the subsets of $S$ are either in $A$ or $B$ but not both. The size of $A$ is $2^{n}$ by the induction hypothesis since every set in $A$ is a subset of $S'$ of size $n$ and vice versa. Now, notice that if $E\in B$, then $E$ contains $a$, and so $E\setminus \{a\}$ is a subset of $S'$. That is, we can pair subsets of $S'$ with every set in $S$ that contains $a$. Hence, $|B|=2^{n}$ as well. Thus, the total number of sets in $S$ is $|A|+|B|=2^{n}+2^{n}=2^{n+1}$. This completes the induction step and also the proof.
%
%\end{proof}

Now try your hand at combinatorial proofs in the following exercises. The second one will be a workshop problem.

\begin{exercise}
Given $S=\{1,2, \dots ,n\}$ and $0\leq j<n$, show that number of subsets of $S$ of the form $\{a,a+1, \dots ,a+j\}\subseteq S$ is ${n\choose 2} + n$.
\begin{solution}
Note that a set of the form $\{a,a+1, \dots ,a+j\}\subseteq S$ is uniquely determined by the first and last element of the set, which could be any two numbers, or the same number if $j=0$. Thus, the total number of sets of this form is the same as counting the number of subsets of size $2$ plus the number of sets of size $1$, and adding them together, hence the total number is
\[
{n\choose 2} + n.
\]
\end{solution}
\end{exercise}

\begin{exercise}
Let $1 \leq r \leq n$. Prove that
\begin{equation}
\label{e:n+1/r}
 \begin{pmatrix} n+1 \\ r \end{pmatrix}= \begin{pmatrix}n \\ r \end{pmatrix} + \begin{pmatrix} n \\ r-1\end{pmatrix}.
 \end{equation}
(You may be familiar with this result if you know about {\em Pascal's triangle}.) First prove this by induction using \eqref{e:choose}. But then can you give another proof by counting a single quantity in two different ways?

Use this result to show that for $2 \leq r \leq n$,
\[ \begin{pmatrix} n+2 \\ r \end{pmatrix}= \begin{pmatrix}n \\ r \end{pmatrix} + 2\begin{pmatrix} n \\ r-1\end{pmatrix} + \begin{pmatrix}n \\ r-2 \end{pmatrix}.\]
How can you further generalise these results?
\end{exercise}

\section{Counting partitions}%
\label{countingpartitions}

The binomial coefficient ${n\choose k}$ counts the number of ways to pick a subset of size $k$ from a set of size $n$, or in other words, the number of ways of splitting a set of size $n$ into a set of size $k$ and a set of size $n-k$. What if we wanted to count the number of ways to partition a set into more than two sets?

For example, let $A = \{1,2,3,4,5,6\}$. Then
$\{\{1,2,3\},\{4,5\},\{6\}\}$
is a partition of $A$ into sets of sizes $3$, $2$ and $1$ respectively. Another one is $\{\{1,4,6\},\{3,5\},\{2\}\}$. How many such partitions are there? One way to look at this is as follows. First we work out the number of ways to select $3$ members of $A$ for our first set: there are ${6 \choose 3}$ such ways. Each such choice leaves us with a set of $3$ remaining members of $A$, from which we choose $2$ for our second set, and there are $3 \choose 2$ ways to do this.
Finally there is exactly one choice for the last set. So altogether there are
$$ {6 \choose 3} \times {3 \choose 2} \times 1= \frac{6!}{3! 3!} \times \frac{3!}{2! 1!} \times \frac{1!}{1!} = \frac{6!}{3! 2! 1!}$$
ways to form such a partition, by the Multiplication Principle.

Another example: again let $A = \{1,2,3,4,5,6\}$ and suppose this time we want to partition $A$ into non-interchangeable sets of sizes
$2, 2$ and $2$. More concretely, suppose we are given a red, a blue and a green bin, and we want to calculate the number of ways of throwing $2$ elements of $A$ into the red bin, $2$ elements of $A$ into the blue bin  and $2$ elements of $A$ into the green bin. (In the previous example there was no need to assign colours to the bins because they were always going to hold {\em different} numbers of members of $A$ -- the red bin was always going to hold $3$ members of $A$, the blue bin $2$ members and the green bin $1$ member of $A$.) There are $6 \choose 2$ ways to throw $2$ members of $A$ into the red bin,  $4\choose2$ ways of throwing $2$ of the remaining $4$ members of $A$ into the blue bin, and $2 \choose 2$ ways of throwing the remaining $2$ members of $A$ into the green bin. So altogether there are
$$ {6 \choose 2} \times {4 \choose 2} \times {2 \choose 2}= \frac{6!}{4! 2!} \times \frac{4!}{2! 2!} \times \frac{2!}{2!} = \frac{6!}{2! 2! 2!}$$
ways to form such a partition, by the Multiplication Principle.

What we are discussing is more formally described using the terminology of an {\em ordered partition}.

\begin{definition}\label{part}
Let $S$ be a nonempty finite set.
An {\it ordered partition} of $S$ is a sequence of sets $(A_{1}, \dots ,A_{k})$ that partitions $S$.
\end{definition}

In the case of our second example, the two ordered partitions $(\{1,2\}, \{3,4\}, \{5,6\})$ and
 $(\{3,4\}, \{1,2\}, \{5,6\})$ are distinct because
 in the first case $1$ and $2$ are in the red bin while in the second case they are in the blue bin; we want to count these separately. In general we think of having $k$ colours, and the set $A_1$ as being assigned colour $1$, $A_2$ as being assigned colour $2$, and so on.

%Wait...what is the difference between these two definitions? The $A_{1}, \dots ,A_{k}$ appear in both?!

%The difference is that a partition is a {\it collection} or {\it set} of subsets that partition $S$, whereas an ordered partition is a {\it list} or {\it sequence} of sets, which implicitly means there is an ordering on the sets under consideration.\footnote{We've already seen something similar earlier: recall that the set $\{ a_1, \dots , a_k\}$ is different from the ordered sequence $(a_1, \dots , a_k)$, and there are $k!$ possible ordered sequences that we can build out of the set $\{ a_1, \dots a_k\}$.} For example
%\[
%\{\{1,2\},\{3,4\},\{5\}\}
%\]
%is a partition of $\{1,2,3,4,5\}$, but it makes no assumptions about the order of the sets. In particular, as sets, we have
%\[
%\{\{1,2\},\{3,4\},\{5\}\}=\{\{5\},\{3,4\},\{1,2\}\}
%.\]
%However, if we let $A_{1}=\{1,2\}$, $A_{2}=\{3,4\}$, and $A_{3}=\{5\}$, then $(A_1, A_2, A_3)$ is an ordered partition, and this is distinct from the ordered partition $(B_1, B_2, B_3)$ given by  $B_{1}=\{5\},B_{2}=\{3,4\},B_{3}=\{1,2\}$.

\begin{definition}
Given $n \in \mathbb{N}$, $k \in \mathbb{N}$ with $k \geq 2$, and nonnegative integers $r_{1}, \dots ,r_{k}$ such that $r_{1}+\cdots + r_{k}=n$, we denote the number of ordered partitions $(A_{1}, \dots ,A_{k})$ of a set $S$ such that $|A_{i}|=r_{i}$ by
\[
{n \choose r_{1}, \dots , r_{k}}.
\]
\end{definition}

Our examples show us that
\[
{6 \choose 3, 2, 1} = \frac{6!}{3!2!1!}
\]
and
\[
{6 \choose 2,2,2} = \frac{6!}{2!2!2!}.
\]
The following result should therefore not be too surprising.
\begin{theorem}
\label{t:mutlinomial}
For $n \in \mathbb{N}$, $k \geq 2$ and nonnegative integers $r_{1}, \dots ,r_{k}$ such that $r_{1}+\cdots + r_{k}=n$, we have
\[
{n \choose r_{1}, \dots , r_{k}} = \frac{n!}{r_{1}!r_{2}!\cdots r_{k}!}.
\]
\end{theorem}

Again, we give a different proof from Liebeck.

\begin{proof}
Let $S$ be a set of size $n$. We prove the above formula by induction on $k$. Our inductive hypothesis is that for a certain $k$, the statement of Theorem~\ref{t:mutlinomial} holds for {\em all} $n$ and {\em all} $r_1, \dots , r_k$ such that $r_1 + \dots + r_k = n$ .
\begin{itemize}
\item[{\bf Base case:}] If $k=2$, this follows since the number of ordered partitions $(A_{1},A_{2})$, where $|A_{1}|=r_{1}$ and $|A_{2}|=r_{2}=n-r_{1}$
is just the number of ways of choosing an $r_1$-element subset of $S$ which equals
$${n \choose {r_1}} = {n \choose {r_1, n-r_1}}.$$
This proves the base case.
\item[{\bf Inductive Step:}] Suppose the inductive hyothesis holds for some $k\geq 2$ and all $n$. Picking an ordered partition $(A_{1}, \dots,A_{k+1})$ with $|A_{i}|=r_i$ and $r_{1}+\cdots + r_{k+1}=n$ is the same as picking an $r_{k+1}$-member subset $A_{k+1}$ of $S$, and, for each such, then picking an ordered partition $A_1, \dots , A_k$ of the remaining $(n - r_{k+1})$-member set with $A_i = r_i$ for $1 \leq i \leq k$. By the Multiplication Principle the number of ways of doing this is
$$ { n \choose r_{k+1}} \times  {n- r_{k+1} \choose r_1, \dots , r_{k}}$$
which, by the inductive hypothesis, is
$$ { n \choose r_{k+1}} \times \frac{(n- r_{k+1})!}{ r_1! \dots r_{k}!}= \frac{n!}{(n-r_{k+1})! r_{k+1}!}\frac{(n- r_{k+1})!}{ r_1! \dots r_{k}!}=
\frac{n!}{r_{1}!r_{2}!\cdots r_{k+1}!},$$
completing the inductive step.
\end{itemize}
\end{proof}
%
%\begin{proof}
%Let's count the number of ways to rearrange a set $A$ of $n$ elements in the following way: first, pick an ordered partition $(A_{1},A_{2},...,A_{k})$ of sets of size $r_{1},r_{2},...,r_{k}$ so that $r_{1}+r_{2}+\cdots + r_{k}=n$, there are ${n\choose r_{1},...,r_{k}}$ ways of picking this set. Then there are $r_i!$ ways of rearranging each set $A_i$. If $a^{i}_{1},...,a^{i}_{r_{i}}$ is one rearrangement of $A_i$, we can combine all the arrangements into one rearrangement of $A$:
%\[
%a^{1}_{1},...,a^{1}_{r_{1}},a^{2}_{1},...,a^{2}_{r_{2}},.....,a^{k}_{r_{k}}.
%\]
%Thus, the total number of rearrangements is
%\[
%{n \choose r_{1},...r_{k}} r_{1}!r_{2}!\cdots r_{n}!.
%\]
%We also know the total number of rearrangements of $A$ is $n!$, and so the above is equal to $n!$. Solving for ${n \choose r_{1},...r_{k}}$ gives the theorem.
%\end{proof}

Take a good look at the above proof and check that the inductive hypothesis was indeed valid at the inductive step.

\begin{example}
Let's revisit the ``banana'' problem. Each arrangement of ``banana'' corresponds to an ordered partition $(B,A,N)$ of the set $\{1,2, \dots ,6\}$ with $|B|=1,|A|=3,|N|=2$, where $i\in B$ if the $i$th letter in the rearrangement is a ``b'', $i\in A$ if the $i$th letter is an ``a'', and $i\in N$ if the $i$th letter is an $N$. Thus, the total number of rearrangements is again
\[
{6 \choose 3 , 2, 1} = \frac{6!}{3!2!1!}=60.
\]
\end{example}

\begin{example}
How many ways are there to choose from a set of size $10$ three disjoint subsets of sizes $5,3,$ and $2$ respectively? What about sizes $4,4,$ and $2$ respectively? \\

Let $S=\{1,2,\dots,10\}$. There are ${10\choose 5,3,2}$ ways of choosing an ordered partition of $S$ into a first set of size 5, a second set of size 3, and a third set of size 2 by Theorem \ref{t:mutlinomial}.

%Note that given an {\it unordered} partition $\{A,B,C\}$ of sizes $5,3,2$, we can order them from smallest to largest to get an {\it ordered} partition of sizes $5,3,2$, and every ordered partition arises in this way. Thus, there are as many unordered partitions as ordered partitions, of which there are ${10 \choose 5,3,2}$.

For the second part, there are ${10\choose 4,4,2}$ ways of picking an {\it ordered} partition with sets of sizes $4,4,$ and $2$. But what if we instead wanted the number $N$ of {\it unordered} partitions $\{A_{1},A_{2},A_{3}\}$ where $|A_{1}|=|A_{2}|=4$ and $|A_{3}|=2$? Given such a partition, there are $2!$ ordered partitions we can make from it, (by swapping the two sets $A_1$ and $A_2$ of the same size $4$), so that the first two sets have size 4 and the last has size 2.
Thus, counting the number of ordered partitions overcounts the number of unordered partitions by a factor of $2$, so the number of partitions is $N= {10\choose 4,4,2}/2$.

\end{example}

\section{The Binomial and Multinomial Theorems}%
\label{binomialmultinomial}

A very natural place that binomial coefficients appear (and the reason why they are called binomial coefficients) is in the following theorem, with which you are probably already familiar from high school, and use of which we have already made in Weeks 3 and 4 of this course :

\begin{theorem}[The Binomial Theorem]
Let $n\in\mathbb{N}$ and $a,b\in \mathbb{C}$. Then\footnote{Liebeck assumes $a,b\in \mathbb{R}$, but this is not necessary. }
\begin{equation}
\label{e:binomialtheorem}
(a+b)^{n} = \sum_{k=0}^{n} {n\choose k} a^{k}b^{n-k}.
\end{equation}
\end{theorem}

As an application, let's give a second (faster) proof of Corollary \ref{c:2^n=nk}: by the Binomial Theorem,
\[
2^{n} = (1+1)^{n} =  \sum_{k=0}^{n} {n\choose k} 1^{k}1^{n-k}=\sum_{k=0}^{n} {n\choose k} .
\]

\begin{proof}
We have
\[ (a+b)^n =  \underbrace{(a + b)}_{1}\cdot \underbrace{(a+b)}_{2}\cdots \underbrace{(a+b)}_{n}.
\]
When we multiply out this product, we will get a sum of terms of the form $a^k b^{n-k}$ for $0 \leq k \leq n$. How many of each of these do we get? Each bracket contributes either an $a$ or a $b$ in the final answer, and the number of $a^k b^{n-k}$ terms
in this final answer will be precisely the number of ways of picking $k$ brackets from the brackets labelled $1,2 ,\dots , n$ above. That is, it is $n \choose k$. In other words, the coefficient of $a^k b^{n-k}$ in the expansion is exactly  $n \choose k$.
\end{proof}

The binomial theorem is a special case of the following theorem which is proved in much the same way:

\begin{theorem}[The Multinomial Theorem]
Let $n\in\mathbb{N}$ and $x_{1}, \dots ,x_{k}\in \mathbb{C}$. Then
\begin{equation}
\label{e:multinomialtheorem}
 (x_{1}+\cdots + x_{k})^{n} =
 \sum {n\choose r_{1},\dots ,r_{k}} x_1^{r_{1}}\cdots x_{k}^{r_{k}}
\end{equation}
where the sum is taken over all $r_{i}\in \{0,1, \dots ,n\}$ such that $r_{1}+\cdots + r_{k}=n$.
\end{theorem}

\begin{proof}
%We will give two proofs, one by induction and one combinatorial proof. First, we prove by induction. Let $P(n)$ be the statement that \eqref{e:binomialtheorem} holds for an integer $n$ and for all $a,b\in \mathbb{C}$.
%\begin{itemize}
%\item[{\bf Base Case:}] If $n=1$, then
%\[
%(a+b)^{1}=a+b = {1\choose 0} a+ {1 \choose 1} b.
%\]
%Hence, the base case holds.
%\item[{\bf Induction Step:}] Assume $P(n)$ holds for some integer $n\geq 1$. Then
%\begin{align*}
%(a+b)^{n+1}
%& =(a+b)  (a+b)^{n}
% =(a+b) \sum_{k=0}^{n} {n\choose k} a^{k}b^{n-k} \\
%&=\sum_{k=0}^{n} {n\choose k} a^{k+1}b^{n-k}
%+\sum_{k=0}^{n} {n\choose k} a^{k}b^{n-k+1}\\
%& = \sum_{k=1}^{n+1} {n\choose k-1} a^{k}b^{n+1-k}
%+\sum_{k=1}^{n} {n\choose k} a^{k}b^{n-(k-1)}+b^{n+1}\\
%& = b^{n+1} +\sum_{k=1}^{n+1} \left({n\choose k-1} +{n\choose k} \right) a^{k}b^{n+1-k}\\
%& = b^{n+1} +\sum_{k=1}^{n+1} {n+1 \choose k} a^{k}b^{n+1-k}\\
%& =\sum_{k=0}^{n+1} {n+1 \choose k} a^{k}b^{n+1-k}
%\end{align*}
%In the fourth equality, we removed the first term of the second sum and placed it outside (this was the $b^{n+1}$), then we combined the two sums (now that they are both summing from $k=1$ to $n$), and in the next line we used \eqref{e:n+1/r} to get the last line. This proves the induction step, and thus the theorem.
%\end{itemize}

Let's number the parentheses in the product:
\[
(x_{1}+\cdots + x_{k})^{n} = \underbrace{(x_{1}+\cdots + x_{k})}_{1}\cdot \underbrace{(x_{1}+\cdots + x_{k})}_{2}\cdots \underbrace{(x_{1}+\cdots + x_{k})}_{n}.
\]
When we multiply out this product, we will get terms of the form $x_{1}^{r_{1}}\cdots x_{k}^{r_{k}}$ where $r_{1}+\cdots + r_{k}=n$. How many of these terms do we get? We can form such a product by picking a partition $A_{1}, \dots ,A_{k}$ of $\{1,2, \dots ,n\}$ with $|A_i|=r_i$ and then picking $x_j$ from $A_i$ to include in our $x_{1}^{r_{1}}\cdots x_{k}^{r_{k}}$ term if $j\in A_i$. Thus, the number of times $x_{1}^{r_{1}}\cdots x_{k}^{r_{k}}$ appears when we multiply out the above product is the same as counting the number of ordered partitions $(A_{1}, \dots ,A_{k})$ of $\{1,2, \dots ,n\}$ with $|A_i|=r_i$, which is  ${n\choose r_{1}, \dots ,r_{k}} $.

%
%For example, if $A$ is a subset of $\{1,2,...,n\}$, we could multiply each $a$ that is in the $i$th term of the product for some $i\in A$, and then pick $b$'s from the other terms. For $0\leq k\leq n$, there are ${n \choose k}$ ways of picking a subset $A$ of size $k$, and so there are ${n \choose k}$ ways to obtain a product of the form $a^{k}b^{n-k}$ when we expand the product.
\end{proof}

\begin{example}
What is the coefficient of $x$ in $(1+x+\frac{1}{x})^{5}$? \\

Let $a=1$, $b=x$ and $c=\frac{1}{x}$. When we expand $(a+b+c)^{5}$, the terms of the form $a^{r_{1}}b^{r_{2}}c^{r_{3}}$ (where $r_{i}\geq 0$ and $r_{1}+r_{2}+r_{3}=5$) that equal $x$ are of the form $a^{4}b$, $a^{2}b^2c$, and $b^3c^{2}$, so we just need to add together the coefficients of these terms, which by the multinomial theorem are
\[
{5 \choose 4,1,0} + {5 \choose 2,2,1} + {5 \choose 0,3,2}
=\frac{5!}{4!0!} + \frac{5!}{2!2!1!}+\frac{5!}{0!3!2!}
=5+\frac{120}{4} + \frac{120}{12} = 5+30+10 = 45.
\]
\end{example}

\section{Inclusion-Exclusion}%
\label{inclusionexclusion}

\indent Suppose we want to count how many numbers between 1 and 100 are divisible by either 3 or 5. A first attempt might be to say, well, there are $99/3=33$ numbers divisible by $3$ and $100/5=20$ numbers that are divisible by $5$, so the numbers that are divisible by $3$ or $5$ is the sum of these two numbers, i.e. $33+20=53$. However, we again run into an {\it overcounting} problem, since some numbers are divisible both by $3$ and $5$ and thus are being counted twice. We can't just divide our answer by 2 as before, since there are plenty of numbers that are divisible by $3$ but not $5$ and vice versa. The way to overcome this is by using the {\it inclusion-exclusion principle}:

\begin{theorem}[Inclusion-Exclusion Principle]
Let $n\geq 2$ and suppose $A_{1},A_{2}, \dots ,A_{n}$ are finite sets. Let
\[
N_{1}  = |A_{1}|+|A_{2}|+\cdots +|A_n|,
\]
\[N_{2}  = |A_{1}\cap A_{2}|+|A_{1}\cap A_{3}|+|A_{2}\cap A_{3}|+\cdots +|A_{n-1} \cap A_n|,
\]
and similarly, let
\[N_k = \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n}
|A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_k}|\]
be the sum of the sizes of the intersections of all collections of $k$ different $A_i$'s, so that \[N_{n} = |\bigcap_{k=1}^{n}A_{k}|.\] Then%\footnote{The statement in Liebeck's book is not correct, there should be a $(-1)^{n+1}$ not $(-1)^{n}$.}
\begin{equation}
\label{e:incl-excl-n}
|A_{1}\cup\cdots \cup A_{n}| = N_{1}-N_{2}+N_3 -N_4 +\cdots + (-1)^{n+1}N_n = \sum_{k=1}^{n} (-1)^{k+1}N_k .\end{equation}
In particular, for finite sets $A,B,C$,
\[
|A\cup B| = |A|+|B|-|A\cap B|,\]
and
\[
|A\cup B\cup C| = |A|+|B|+|C|-|A\cap B|-|A\cap C|-|B\cap C| + |A\cap B\cap C|.
\]
\end{theorem}

\begin{proof}
Let $A=A_{1}\cup\cdots \cup A_{n}$, and list the elements as $A=\{x_{1}, \dots ,x_{M}\}$ where $M=|A|$. Notice that when we compute $N_1=|A_{1}|+\cdots + |A_{n}|$, this is the same as computing for each $x_k$ the number of sets $A_i$ that $x_k$ appears in, and then summing over $k$. Similarly, $c_2$ is the same as adding together over all $x_k$ the number of times $x_k$ appears in some pair $A_i\cap A_j$, and so on. Thus, the sum on the right of \eqref{e:incl-excl-n} is the same as computing the sum over each $x_k$,  how many times $x_k$ appears in one set $A_i$ minus how many times it appears in the intersection of two sets, and so on, and then adding over all $x_k\in A_{1}\cup\cdots \cup A_{n}$. What we will do is show that for each $k$, this number is $1$.

Suppose $x_k$ appears in each of the sets $A_{k_{1}}, \dots ,A_{k_{m}}$ for some $1\leq k_1<k_2<\cdots < k_m\leq n$, and none of the others. This means $x_k$ appears in exactly $m$ sets, so its contribution to $N_{1}$ is exactly $m$. It also appears in ${m\choose 2}$ many pairs $A_{k_{i}}\cap A_{k_{j}}$ with $1\leq i<j\leq m$, so it contributes ${m\choose 2}$  to $N_2$. Similarly, $x$ contributes ${m\choose k}$  to $N_k$ for each $k\leq n$ (and recall that ${m\choose k}=0$ for $k>m$). Thus, by the Binomial Theorem, the total contribution of $x$ to the right side of \eqref{e:incl-excl-n} is
\[
\sum_{k=1}^{m} {m\choose k}(-1)^{k+1}
=1 + \sum_{k=0}^{n} {m\choose k}(-1)^{k+1}
=1 -\sum_{k=0}^{n} {m\choose k}(-1)^{k}
=1-(1-1)^{m}=1.
\]

\end{proof}

This proof is quite abstract, but using it is a lot easier, which we demonstrate in the following example.

\begin{example}
Let's return to the example of counting how many natural numbers at most 100 are divisible by $5$ or $3$. Let $A$ be those numbers divisible by $3$ and $B$ those numbers which are divisible by $5$. Then as we showed before, $|A|=33$ and $|B|=20$. Note that the set we want to count is $A\cup B$, which are those numbers either in $A$ or in $B$ (that is, divisible by $3$ or divisible by $5$). Also note that $A\cap B$ are those numbers divisible by both $3$ and $5$, that is, divisible by 15, of which there are only $6$. Thus,
\[
|A\cup B|=|A|+|B|-|A\cap B|=33+20-6 = 47.
\]
\end{example}

\section{Exercises}%
\label{countingexercises}

\begin{exercise}
 How many ways are there for $n$ people to stand in a circle?

 \begin{solution}
 Let's first count the number of ways to order people clockwise around a circle where the first person stands at a specific spot. This is just $n!$. However, we are counting the number of ways for $n$ people to stand in a circle, and so there is no first position, and for such an arrangement, there are $n$ ways to pick who should be the ``first person.'' Thus, we overcounted by a factor $n$, so dividing $n!$ by $n$ gives the answer $(n-1)!$.
 \end{solution}
 \end{exercise}

\begin{exercise}
How many ways are there to partition a set of size $n$ into two subsets (one of which can be empty)?
\begin{solution}
For 2 sets, if $S$ is our set, notice that to each $A\subseteq S$ there corresponds a partition of $S$, namely $\{A,S\backslash A\}$. Thus, the number of partitions is the same as the number of subsets, which is $2^{n}$.
\end{solution}
\end{exercise}

\begin{exercise}
Determine how many ways there are to distribute $9$ sweeties in $4$ boxes if
\begin{enumerate}[label=(\alph*)]
\item The sweeties and boxes are distinct.
\begin{solution}
For each sweetie, there are $4$ options for which box to place it in, thus by the multiplication principle, the number of options is $4^{9}$.
\end{solution}
\item The sweeties are identical and the boxes are distinct.
\begin{solution}
Using the method in Example~\ref{identicalballsinboxes}, this is equal to the number of ways of placing $4-1 = 3$ dividers in a line of $9$ balls, which is ${{9+3}\choose {3}} = {12 \choose 3}$.
\end{solution}
\end{enumerate}
\end{exercise}

\begin{exercise}
How many numbers at most 200 are divisible by
\begin{enumerate}[label=(\alph*)]
\item 5 or 6?
\begin{solution}
The largest multiple of 6 at most 200 is 192, and so there are 192/6=31 multiples of 6 at most 200 (call this set $A$). Similarly, there are 200/5=40 multiples of 5 (call this set $B$). The largest multiple of $5\cdot 6=30$ is 180, and so there are at most $180/30=6$ numbers divisible by 5 and 6, thus
\[
|A\cup B| = |A|+|B|-|A\cap B| = 31+40-6 = 65.
\]
\end{solution}
\item 3,4 or 5?
\begin{solution}
Homework problem.
%The largest multiple of $3$ less than 200 is 198, and so there are 198/3=66 multiples of 3 at most 200, call this set $A$. There are 200/4=50 multiples of $4$, call this set $B$. There are 200/5=40 multiples of 5, call this set $C$. The set $A\cap B$ is the set of multiples of $3$ and $4$ (that is, multiples of 12) at most 200; the largest multiple of 12 at most 200 is 192, so there are 192/12=16 multiples of 12 at most 200, so $|A\cap B|=12$. The largest multiple of $3\cdot 5=15$ at most 200 is 195, so $|A\cap C| = 195/15=13$. Finally, $|B\cap C|=200/20=10$. Finally, the largest multiple of $3\cdot 4\cdot 5=60$ at most 200 is 180, and so $|A\cap B\cap C| = 180/60=3$. Thus,
%\begin{align*}
%|A\cup B\cup C|
%& =|A|+|B|+|C| -|A\cap B|-|A\cap C|-|B\cap C|+|A\cap B\cap C|\\
%& =66+50+40-12-13-10+3=124.
%\end{align*}
\end{solution}
\item 3,5, or 6?
\begin{solution}
Let $A,B,C$ be those numbers at most 200 which are divisible by 3,5 and 6 respectively. We want to figure out what $|A\cup B\cup C|$ is. Notice that $C\subseteq A$, and so $A\cup B\cup C = A\cup B$. Thus, by the inclusion exclusion principle,
\[
|A\cup B\cup C|=| A\cup B|=|A|+|B|-|A\cap B|
=66 + 40-13 = 93.
\]
\end{solution}
\end{enumerate}
\end{exercise}

\begin{exercise} Use a combinatorial argument to prove that  for $2\leq k\leq n$,
\[
{k \choose 2} + {n-k \choose 2}+k(n-k) = {n \choose 2}.\]
\begin{solution}
The term on the right is the number of ways to choose two elements from a set $S$ of size $n$. We can alternatively count this as follows: first let $2\leq k\leq n$ and let $A\subseteq S$ have size $k$. Then we can pick a subset of $S$ of size two either by picking two elements from $A$ (of which there are ${k \choose 2}  $ options), or two elements from $S\setminus A$ (of which there  are ${n-k \choose 2}$ options) , or one from $A$ and one from $S\setminus A$ (of which there are $k(n-k)$ options). Adding these all together gives the left side of the above equation.
\end{solution}
\end{exercise}

\begin{exercise}
 Use a combinatorial argument to show
\[
\binom{nk}{2} = k \binom{n}{2} + n^2 \binom{k}{2}.
\]

\begin{solution}

Consider a grid of $nk$ squares with $k$ columns and $n$ rows, and let $A$ be the set of unordered pairs of distinct squares from the $nk$ squares in the grid. Then $|A|=\binom{nk}{2}$. Alternatively, we could compute $|A|$ as follows: let $A_{1}$ be those pairs of squares that are in different columns and $A_{2}$ be those pairs of squares in the same column: there are $ \binom{k}{2}$ ways to pick the two columns, then $n$ ways to pick a square from one column, then another $n$ ways to pick a square from the second column, thus $|A_{1}|=n^2 \binom{k}{2}$. Next, to count $A_{2}$, there are $k$ columns to pick from, and then we can pick a subset of size $2$ in $ \binom{n}{2}$ ways from that column, so $|A_{2}|=k \binom{n}{2} $. Thus,
\[
\binom{nk}{2} = |A|=|A_{1}|+|A_{2}|= n^2 \binom{k}{2}+k \binom{n}{2} .
\]

\end{solution}

\end{exercise}

 \begin{exercise}
  Give two proofs that ${2n\choose n}$ is even.
  \begin{solution}
  First solution: notice that ${2n\choose n}$ counts the number of sets of size $n$ from a set $S$ of size $2n$. Notice that each such set $A$ of size $n$ appears once in the first component of the following pairs
 \[P=\{(B,S\setminus B) \; | \; B\subseteq S,\;\; |B|=n\}.\]
Thus, ${2n\choose n}=|P|$. If we look at the set of {\it unordered} pairs $\{A,S\setminus A\}$ where $|A|=n$, then this is exactly half the size of $P$ since to each undordered pair $\{A,S\setminus A\}$ there corresponds two ordered pairs $(A,S\setminus A)$ and $(S\setminus A,A)$. Hence, $P$ must be even, and so ${2n \choose n}$ is even. \\
\end{solution}

\begin{solution}
Second solution: for a second proof, by the binomial theorem,
\begin{align*}
2^{2n} & = (1+1)^{2n} = \sum_{k=0}^{2n} {2n\choose k}
=\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{k=n+1}^{2n} {2n\choose k}\\
& =\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{k=n+1}^{2n} {2n\choose 2n-k}\\
& =\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{j=0}^{n-1} {2n\choose j}\\
& = 2 \sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}\\
%& =\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{k=0}^{n-1} {2n\choose 2n-(k+n+1)}\\
%& =\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{k=0}^{n-1} {2n\choose n-k-1}\\
%& =\sum_{k=0}^{n-1} {2n\choose k}+{2n\choose n}+\sum_{k=0}^{n-1} {2n\choose n-k-1}\\
\end{align*}
where in the second line we used the fact that ${m \choose r} = {m \choose m-r}$ and in the third line we made a change of variables $j = 2n-k$. Thus ${2n\choose n}$ is a difference of even numbers and so it must be even.
\end{solution}
\end{exercise}

\begin{exercise} If $n\geq 5$ distinct objects are arranged in a circle, how many ways are there to choose three of these $n$ objects so that no two of them are next to each other?

\begin{solution}
{\bf Claim:}
There are $\frac{1}{6}n(n-4)(n-5)$ ways to choose three of $n$ distinct objects arranged in a circle so that no two of them are next to each other.

\begin{proof}
Choose one object, which can be done in $n$ ways. Then the remaining two must be picked from those that aren't adjacent to it, so there are $n-3$ objects to pick from, (and we can think of these objects as now being arranged along a line). There are ${{n-3} \choose 2}$ ways of choosing {\em any} two of these objects and
$n-4$ ways of choosing them to be adjacent. So there are
$${{n-3} \choose 2} - (n-4) = \frac{(n-3)(n-4)}{2} - (n-4) = \frac{(n-4)(n-5)}{2}$$
ways of choosing the remaining two to be non-adjacent. By the multiplication principle, there are $\frac{n(n-4)(n-5)}{2}$ ways of making the choice, having fixed the first one. But this overcounts by a factor of $3! = 3$, so the final answer is $\frac{n(n-4)(n-5)}{6}$.
\end{proof}

\end{solution}
\end{exercise}

%
%\begin{exercise}
%Prove by induction that
%\[
%\sum_{k=0}^n {k\choose i} = {n+1\choose i+1} \mbox{ for $n\geq i\geq 0$}.
%\]
%Then prove it using a combinatorial argument instead.
%
%\begin{solution}
%First we prove by induction. For base case $n=0$, we can only have $i=0$ and so
%\[
%\sum_{k=0}^0 {k\choose 0}
%=1 = {0+1\choose 0+1}.
%\]
%For the induction step, suppose we have shown the claim is true for some $n\geq 0$. Then if $0\leq i\leq n$,
%\[
%\sum_{k=0}^{n+1} {k\choose i}
%=\sum_{k=0}^n {k\choose i} + {n+1 \choose i}
%={n+1\choose i+1} + {n+1 \choose i}
%={n+2 \choose i+1}
%\]
%where in the last equation we used \eqref{e:n+1/r}.  If $i=n+1$, if $k\leq n$ then ${k \choose n+1}=0$, and so
%\[
%\sum_{k=0}^{n+1} {k\choose n+1}
%=\sum_{k=0}^n {k\choose n+1} + {n+1 \choose n+1}
%=0+1={n+2 \choose (n+1)+1}
%\]
%This proves the induction step and hence the claim. \\
%
%Now we give a combinatorial proof. If $i\leq n$, then ${n+1 \choose i+1}$ is the number of ways to choose a subset $A$ of size $i+1$ from a set $S$ of size $n+1$.
%\end{solution}
%\end{exercise}

%
%
%\begin{exercise} Suppose that $A$ is a non-empty finite set. Prove that $A$ has as many even-sized subsets as it does odd-sized subsets.
%
%
%\begin{solution}
%We will give three different solutions.\\
%
% Let $n=|S|$, $E$ be the number of even subsets, and $O$ the number of odd subsets. If $n$ is odd, if we look at the pairs $P=\{(A,S\backslash A):A\subseteq S \mbox{ is even}\}$, then each even set appears in exactly one of these pairs, since if $|A|$ is even, then $|S\backslash A|$ is odd. Similarly, every odd number appears in exacly one of these pairs, so
%\[
%|E|=|P|=|O|.
%\]
%If $S$ is even (so $|S|\geq 2$ since $S\neq\emptyset$), let $a\in S$ and let $S'=S\backslash \{a\}$, so $|S'|$ is odd (and $|S'|\geq 1$ so it is nonempty). Then the number of even and odd subsets of $S'$ are equal, so we just need to ensure that the number of even and odd subsets of $S$ that contain $a$ are also equal, but notice that if $A\subseteq S$ is even and contains $a$, then $A$ appears in exactly one of the pairs $(A,A\backslash \{a\})$, and similarly, every $B\subseteq S'$ of odd size appears in exactly one of the pairs $(A,A\backslash \{a\})$ (we just set $A=B\cup \{a\})$.
%
%
%Thus, the number of these pairs is equal to both the number of even sets in $S$ containing $a$ and the number of odd sets in $S'$. A similar argument shows that the number of odd sets in $S$ containing $a$ is equal to the number of even sets in $S'$. Since the number of odd and even sets in $S'$ are equal, this means the number of odd and even sets in $S$ containing $a$ are equal.
%
%\end{solution}
%
%\begin{solution} An alternative proof is as follows: by the binomial theorem, if $n=2m$, then
%\begin{align*}
%0 & =(-1+1)^{n} = \sum_{k=0}^{n} {n\choose k} (-1)^{k}\\
%&  =\sum_{j=0}^{n/2} {n\choose 2j} (-1)^{2j}+\sum_{j=0}^{n/2-1} {n\choose 2j+1} (-1)^{2j+1} \\
%& =\sum_{j=0}^{n/2} {n\choose 2j}-\sum_{j=0}^{n/2-1} {n\choose 2j+1}
%\end{align*}
%and so
%\[
%\sum_{j=0}^{n/2} {n\choose 2j}=\sum_{j=0}^{n/2-1} {n\choose 2j+1}
%\]
%but the sum on the left is equal to the number of even subsets, and the quantity on the right is equal to the number of odd subsets.
%
%Similarly, if $n=2m+1$,
%\begin{align*}
%0 & =(-1+1)^{n} = \sum_{k=0}^{n} {n\choose k} (-1)^{k}\\
%& =\sum_{j=0}^{(n-1)/2} {n\choose 2j} (-1)^{2j}+\sum_{j=0}^{(n-1)/2} {n\choose 2j+1} (-1)^{2j+1} \\
%& =\sum_{j=0}^{(n-1)/2} {n\choose 2j}-\sum_{j=0}^{(n-1)/2} {n\choose 2j+1}
%\end{align*}
%and so
%\[
%\sum_{j=0}^{(n-1)/2}{n\choose 2j}=\sum_{j=0}^{(n-1)/2}{n\choose 2j+1}
%\]
%and again, the left and right sides count the number of even and odd sized subsets as well.
%
%\end{solution}
%
%\begin{solution} Finally, one last proof: Let $E$ be the set of even sets and $O$ the set of odd sets in $S$. Fix an element $x\in S$ (such an element exists because $S$ is nonempty). Define a map $f:E\rightarrow O$ by
%\[
%f(A)=\left\{\begin{array}{ll} A\backslash \{x\} & \mbox{ if }x\in A\\
%A\cup \{x\} & \mbox{ if }x\not\in A\end{array}\right.
%.\]
%
%Then if $A$ is even, then $A$ is odd, since we either add an element to $A$ (hence increasing its size by 1) or we remove an element of $A$ (hence decreasing its size by 1). Moreover, it is bijective: To see that it is surjective, note that if $B$ is an odd set, then if $x\in B$, $f(B\backslash \{x\})=B$ and if $x\not\in B$, then $f(B\cup \{x\})=B$, and in each case $B\backslash \{x\}$ and $B\cup \{x\}$ are in $E$. To see that $f$ is injective, suppose  $f(A)=f(B)$ for some even sized sets $A$ and $B$. Assume for the sake of a contradiction that $f(A)=A\backslash \{x\}$ and $f(B)=B\cup \{x\}$, then $|A|=|f(A)|+1$ and $|B|=|f(B)|-1$, so in particular, $|A|=|B|+2$, so they differ by at least two points. One of those is $x$, since $x$ is in $A$ and $x$ is not in $B$ by assumption. Thus, there is another point in $A\backslash B$ besides $x$, and that point won't change under $f$,  so $y\in f(A)\backslash f(B)$, which means $f(A)\neq f(B)$, which is a contradiction. Thus, we must have $f(A)=A\backslash \{x\}$ and $f(B)=B\backslash \{x\}$, or $f(A)=A\cup \{x\}$ and $f(B)=B\cup \{x\}$. In the first case, we then have
%\[
%A=f(A)\cup \{x\}=f(B)\cup \{x\} = B
%\]
%and similarly for the second case. This proves injectivity, and thus bijectivity.
%
%Now by proposition 19.1, since $f:E\rightarrow O$ is bijective, $|E|=|O|$.
%
%
%
%if $f(A)=f(B)$, then that means $A\backslash \{x\}=B\backslash \{x\}$,
%
%
%
%
%
%\end{solution}
%\end{exercise}

\begin{exercise}
Give both an algebraic argument and a combinatorial argument to show (for $0 \leq k \leq n$)
\[k\binom{n}{k} = n\binom{n-1}{k-1}\]
in two different ways. {\it( {\bf Hint for the combinatorial proof:}  For a set $S$ with $|S|=n$, the left side is the same as counting the number of pairs $(a,A)$ where $a\in A\subseteq S$ and $|A|=k$. Can you show the right side counts the same set in a different way?)}

\begin{solution}
First we do the algebraic argument:
\[k\binom{n}{k}
=\frac{k\cdot n!}{(n-k)!k!} = \frac{n!}{(n-k)!(k-1)!}  = \frac{n\cdot (n-1)!}{(n-k)!(k-1)!}\]
\[=\frac{n\cdot (n-1)!}{((n-1)-(k-1))!(k-1)!} = n\binom{n-1}{k-1}.
\]
Now we give a combinatorial proof. We need to find a common thing these two numbers are counting, or that they count two sets that are in one-to-one correspondence. If you see a term with a product, it might mean that it is counting some set using the Multiplication Principle. In fact, the left side counts the number of ways of pairing a set $A$ of size $k$ with an element $a\in A$, i.e. the set of pairs $(a,A)$ where $A\subseteq S$, $|A|=k$, and $a\in A$. Let's see if the right side is in one-to-one correspondence with this set. Note that $n$ is the number of ways of picking one element $a\in S$, and $\binom{n-1}{k-1}$ is the number of ways of picking a subset $B$ of size $k-1$ from the remaining $n-1$ elements of $S$, i.e. this counts the set of pairs $(a,B)$ where $a\in S$, $|B|=k-1$, and $a\not\in B$. But each such pair is in one-to-one correspondence with a pair $(a,A)$ (where $a\in A$ and $|A|=k$ as before by setting $A=B\cup \{a\}$. Thus, these sets of pairs are in one-to-one correspondence and so they have the same number. This proves the formula.

\end{solution}
\end{exercise}

\begin{exercise} Use a combinatorial argument to show
\[
\sum_{k=0}^{n} k{n\choose k} =n2^{n-1}.
\]

\begin{solution}
\begin{proof}
Let $S$ have $n$ elements. Then $n2^{n-1}$ is the number of ways to pick an element $a\in S$ and pair it with a subset $A\subseteq S\backslash \{a\}$. We can also count these pairs by first picking $k$, then picking a subset $A$ of size $n-k$ -- which can be done ${n\choose n-k}={n\choose k}$ ways -- and then picking an element $a\in S\backslash A$, and then adding over all $k$.
\end{proof}
\end{solution}
\end{exercise}

\begin{exercise} {\bf (Challenging!)} How many subsets does the set $\{1,2, \dots ,n\}$ have that contain no two consecutive integers?  {\bf Hint}: maybe compute this for the first few values of $n$ to see if you can spot a pattern.

\begin{solution}
See the solutions to the workshop problems.
\end{solution}

\end{exercise}

%
%\begin{enumerate}
%
%
%%
%%
%%
%%\item Show that
%%\[
%%\sum_{k=1}^{2n} {2n \choose k}
%%= \left(\sum_{k=0}^{n}{n\choose k}\right)^{2}.
%%\]
%%
%%\begin{solution}
%%
%%By the binomial theorem,
%%\[
%%\sum_{k=1}^{2n} {2n \choose k}
%%=(1+1)^{2n} =( (1+1)^{n})^{2} =  \left(\sum_{k=0}^{n}{n\choose k}\right)^{2}.
%%\]
%%\end{solution}
%%
%
%
%
%
%
%%
%%
%%\item Prove that for $n>i\geq 0$,
%%\[
%%\sum_{k=0}^n {k\choose i}k = {n+1\choose i+1}n-{n+1\choose i+2}.
%%\]
%%
%%\begin{solution}
%%
%%\end{solution}
%
%
%
%
%\item  Find the number of ways to write a natural number $n$ as an ordered sum of $1$'s and $2$'s. For example, when $n=4$, there are five ways: $1+1+1+1$, $2+1+1$, $1+2+1$, $1+1+2$, and $2+2$.
%

%
%
%\item Prove that for $x>0$, $\lim_{n\rightarrow\infty} x^{1/n}=1$. {\it Hint: First suppose $x\geq 1$, let $x_{n} = x^{1/n}-1$, and first show that $(x_{n}+1)^{n}\geq 1+nx_{n}$. What about the case $0<x< 1$?}
%
%\begin{solution}
%\begin{proof}
%Let's first assume $x>1$. Then by the binomial theorem
%\[
%x = (x_{n}+1)^{n} =\sum_{k=0}^{n} {n \choose k} x_{n}^{k}
%\geq \sum_{k=0}^{1} {n \choose k} x_{n}^{k}
%=1+{n\choose 1} x_{n} = 1+nx_{n}
%\]
%Thus rearranging the inequality, we get that
%\[
%\frac{x-1}{n} \geq x_{n}\geq 0
%\]
%Since $\frac{x-1}{n}\rightarrow 0$, $x_{n}\rightarrow 0$ by the Squeeze theorem, and so $x^{1/n}\rightarrow 1$.
%\end{proof}
%\end{solution}
%
%\item Prove that $\lim_{n\rightarrow\infty} n^{\frac{1}{n}}=1$.
%
%\end{enumerate}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------



\part{Week 10: Permutations}


\chapterimage{Figures/blank.png} 


\chapter{Permutations}%
\label{permutations}

\section{Permutations}%
\label{permutationssection}

This week we combine the techniques of the last two weeks to study a particular class of functions called permutations. 

\begin{definition}
Given $n\in\mathbb{N}$, denote by $\mathrm{S}_{n}$ the set of all bijections
$$
\big\{1,2,3,4,\ldots,n\big\}\longrightarrow\big\{1,2,3,4,\ldots,n\big\}.
$$
 We call elements in $\mathrm{S}_{n}$ \textcolor[rgb]{0.98,0.00,0.00}{permutations} of the set $\{1,2,3,4,\ldots,n\}$.
\end{definition}

As we saw last week, permutations are important for combinatorics and counting. They are also interesting from an algebraic perspective: as we'll see below.


 \begin{example}  
The set $\mathrm{S}_{1}$ consists of one
bijection $f:\mathrm{S}_{1}\rightarrow\mathrm{S}_{1}$ defined by $f(1)=1$.\\

The set $\mathrm{S}_{2}$ consists of two
bijections:
\begin{itemize}%
\item a function $f$ such that $f(1)=1$ and $f(2)=2$,%
\item a function $g$ such that $g(1)=2$ and $g(2)=1$.%
\end{itemize}
\end{example}

We know exactly how many permutations there are on $S_{n}$.

\begin{lemma}%[Proposition~20.1 in Liebeck]
 The set $\mathrm{S}_n$ consists of exactly $n!$ permutations.
\end{lemma}
\begin{proof}
Permutations in $S_{n}$ are just ways of rearranging the numbers $\{1,2,...,n\}$ written in order. That is, we can identify each rearrangement of numbers $\{1,2,...,n\}$ written in order with the unique function which maps $\{1,2,...,n\}$ onto that rearrangement. 
\end{proof}

Notice that each element $f\in S_{n}$ is actually a function.
So, we are dealing with the whole function itself, \(f\) and not just with the value of the function \(f(n)\) for some number \(n\).

\section{Table Notation}%
\label{tablenotation}

Here we will introduce some useful notation for denoting permutations. To represent a permutation $f\in S_n$,  we just need to remember where each integer $k\in \{1,2,...,n\}$ gets sent to under $f$ and then write out a chart using a matrix:
 $$
\left(\begin{matrix} %
1 & 2 & 3 & \cdots & n \cr%
f(1) & f(2) & f(3) & \cdots & f(n) \cr%
\end{matrix}\right)
$$
where the first row represents the domain of $f$ and the bottom row represents where each value is sent to under $f$. For example,
 $$
\left(\begin{matrix} %
1 & 2 & 3  \\
3 & 1 & 2
\end{matrix}\right)
$$
is the permutation in $S_{3}$ so that $f(1)=3$, $f(2)=1$, and $f(3)=2$. 

\begin{exercise}
$$
\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7  & 8 & 9 & 10 \cr%
7 & 5 & 1 & \textcolor[rgb]{1.00,0.00,0.00}{\bigstar} & 9 & 2 & 10 & 4 & 3 & 8\cr%
\end{matrix}\right)
$$
denotes a permutation in $\mathrm{S}_{10}$.  What is
$\textcolor[rgb]{1.00,0.00,0.00}{\bigstar}$?

\begin{solution}
Recall that permutations are bijective, which means that every number must appear \emph{exactly once} in the second row. The only number missing is $6$, so the number under $\textcolor[rgb]{1.00,0.00,0.00}{\bigstar}$ must be 6. 
\end{solution}
\end{exercise}

\section{Composition and identity}%
\label{compositionandidentity}

For any two permutations $f\in\mathrm{S}_n$ and
$g\in\mathrm{S}_n$,  the composition
$$
f\circ g\colon \{1,2,3,\ldots,n\}\to \{1,2,3,\ldots,n\}
$$
is defined by $f\circ g(k)=f(g(k))$ for every
$k\in\{1,2,3,\ldots,n\}$. 

Since we will be working with compositions of many functions below, we will often drop the $\circ$ all together and just  write $fg$ instead of $f\circ g$.

\begin{example}
Let 
$$
f= \left(\begin{matrix} %
1 & 2 & 3 & 4  \cr%
2 & 3 & 1 & 4  \cr%
\end{matrix}\right), \;\;\; \mbox{ and } \;\;\; 
g= \left(\begin{matrix} %
1 & 2 & 3 & 4  \cr%
4 & 3 & 2 & 1 \cr%
\end{matrix}\right)
$$
We can determine $fg$ by plugging in each number $k\in \{1,2,3,4\}$ and see what $g(k)$ and then $f(g(k))$ is:
\[
 \left(\begin{array}{c|cccc} %
k & 1 & 2 & 3 & 4  \\  %
g(k)  & 4 & 3 & 2 & 1   \cr%
f(g(k)) & 4 & 1 & 3 & 2  \\%
\end{array}\right)
\]
Thus, taking the top and bottom rows, we see that
\[
fg = \left(\begin{matrix} %
1 & 2 & 3 & 4  \cr%
4 & 1 & 3 & 2 \cr%
\end{matrix}\right)
\]
\end{example}

We use symbol $\iota$ (the Greek letter ``iota'') to denote the  {\it identity permutation}
$$
\iota =  \left(\begin{matrix} %
1 & 2 & 3 & 4 & \ldots & n \cr%
1 & 2 & 3 & 4 & \ldots & n\cr%
\end{matrix}\right), \;\;\; \iota(x) =x \;\;\mbox{ for all } x\in \{1,2,...,n\}.
$$

In particular, for every $f\in\mathrm{S}_{n}$,  we have $\iota\circ f=f\circ\iota=f$, that is, $\iota f=f\iota=f$.\\

%



%
% \begin{exercise}    Let $f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 \cr%
%2 & 1 & 4 & 3 \cr%
%\end{matrix}\right)\in\mathrm{S}_4$. What is $f\circ f$?
%\begin{solution}
%For $f\circ f$, we have
%$$
%f\circ f(1)= f(f(1)) = f(2) = 1,\  f\circ f(2) = f(f(2)) = f(1) = 2,
%$$\vspace{-.2in}
%$$
%f\circ f(3)= f(f(3)) = f(4) = 3,\   f\circ f(4)= f(f(4))= f(3) = 4,
%$$
% which implies that $f\circ f$ is the identity permutation in $\mathrm{S}_4$.
%\end{solution}
%\end{exercise}

Another reason we drop the $\circ$ from the above notation is so that we can think of composing functions as if we are multiplying elements together, and the function $\iota$ is playing the role of the number $1$ in that, when you multiply it by anything other element, you get the same element back.  However, unlike multiplication for integers, compositions of functions is not commutative. 



 \begin{example}
  Let us consider composition of two permutations in $\mathrm{S}_5$.
  \[
  f= \left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
2 & 3 & 4 & 5 & 1\cr%
\end{matrix}\right)\;\;\;\mbox{ and } \;\;\; g=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
2 & 1 & 3 & 4 & 5\cr%
\end{matrix}\right).
\]
Then
\[
fg= \left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
3 & 2 & 4 & 5 & 1\cr%
\end{matrix}\right), \;\;\; \mbox{ and }\;\;\; gf = \left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
1 & 3 & 4 & 5 & 2\cr%
\end{matrix}\right).
\]
That is, $fg\neq gf$.
\end{example}

This doesn't mean we don't always have $fg\neq gf$--in particular, $fg=gf$ for all $f\in S_{2}$...exercise!--it just means that we can't assume $fg=gf$ in general, that is, $\forall n\geqslant 3$,  $\exists$ $f$ and $g$ in $\mathrm{S}_n$  such that  $fg\ne gf$. 

%
%
% Let us denote permutations in $\mathrm{S}_3$ as follows:
%
%$$ \iota=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%1 & 2 & 3
%\end{array}
%\right),\  \textbf{a}=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%2 & 3 & 1
%\end{array}
%\right),\  \textbf{b}=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%3 & 1 & 2
%\end{array}
%\right),
%$$
%
%$$
%\textbf{c}=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%2 & 1 & 3
%\end{array}
%\right),\  \textbf{d}=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%3 & 2 & 1
%\end{array}
%\right),\  \textbf{e}=\left(
%\begin{array}{ccc}
%1 & 2 & 3 \\
%1 & 3 & 2
%\end{array}
%\right).
%$$\vspace{-.1in}
%
% The table below gives $f\circ g$ for any
%$f\in\mathbb{S}_{3}$ and $g\in\mathbb{S}_3$.
%
%\renewcommand\arraystretch{1.1}
%\begin{center}
%\begin{tabular}{|c||c|c|c|c|c|c|}
%\hline
%$\circ$& $\iota$ & $\textbf{a}$ & $\textbf{b}$ & $\textbf{c}$ & $\textbf{d}$ & $\textbf{e}$ \\
%\hline\hline
%$\iota$ & $\iota$ & $\textbf{a}$ & $\textbf{b}$ & $\textbf{c}$ & $\textbf{d}$ & $\textbf{e}$ \\
%\hline
%$\textbf{a}$ & $\textbf{a}$ & $\textbf{b}$ & $\iota$ & $\textbf{d}$ & $\textbf{e}$ & $\textbf{c}$ \\
%\hline
%$\textbf{b}$ & $\textbf{b}$ & $\iota$ & $\textbf{a}$ & $\textbf{e}$ & $\textbf{c}$ & $\textbf{d}$ \\
%\hline
%$\textbf{c}$ & $\textbf{c}$ & $\textcolor[rgb]{1.00,0.00,0.00}{\bigstar}$ & $\textbf{d}$ & $\iota$ & $\textbf{b}$ & $\textbf{a}$ \\
%\hline
%$\textbf{d}$ & $\textbf{d}$ & $\textbf{c}$ & $\textbf{e}$ & $\textbf{a}$ & $\iota$ & $\textbf{b}$ \\
%\hline $\textbf{e}$ & $\textbf{e}$ & $\textbf{d}$ & $\textbf{c}$ & $\textbf{b}$ & $\textbf{a}$ & $\iota$\\
%\hline
%\end{tabular}
%\end{center}
%
% What is $\textcolor[rgb]{1.00,0.00,0.00}{\bigstar}$? 
%Since $\textbf{c}\circ\textbf{a}=\textbf{e}$,  we see that
%$\textcolor[rgb]{1.00,0.00,0.00}{\bigstar}=\textbf{e}$.
%
%
%

\section{Inverse permutations}%
\label{inverseofpermutation}

Let $f$ be any permutation in $\mathrm{S}_n$.  Then
$$
1\mapsto f(1),\  2\mapsto f(2),\  3\mapsto f(3),  \ldots,\  n\mapsto f(n),%
$$
where all numbers $f(1), f(2), f(3),\ldots, f(n)$ are different (here $x\mapsto y$ means that $f(x)=y$, or that ``$x$ is mapped to $y$ under $f$''.

Given $f\in S_{n}$, the inverse permutation $f^{-1}$ always exists since $f$ is bijection and
$$
f f^{-1}=f^{-1} f=\iota.
$$

To figure out what $f^{-1}$ is given $f$, just note that the permutation $f^{-1}$ maps
$$
f(1)\mapsto 1,\  f(2)\mapsto 2,\  f(3)\mapsto 3,  \ldots,\  f(n)\mapsto n.%
$$
Thus, if we have our matrix representing $f$, 
\[
f=\left(\begin{matrix} %
1 & 2 & 3 & ... & n\cr%
f(1) & f(2) & f(3) & ... & f(n)\cr%
\end{matrix}\right).
\]
Then the inverse is 
\[
f^{-1}=\left(\begin{matrix} %
f(1) & f(2) & f(3) & ... & f(n)\cr%
1 & 2 & 3 & ... & n\cr%
\end{matrix}\right).
\]
Since $f(1),...$ runs through each integer $1,2,...,n$ exactly once, the top row is just a rearrangement of $\{1,2,...,n\}$, so if we rearrange the columns so that the top row is in order, then we will get the chart
\[
f^{-1}=\left(\begin{matrix} %
1 & 2 & 3 & ... & n\cr%
f^{-1}(1) & f^{-1}(2) & f^{-1}(3) & ... & f^{-1}(n)\cr%
\end{matrix}\right).
\]

\begin{example}  Let $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
2 & 3 & 4 & 5 & 1\cr%
\end{matrix}\right)\in\mathrm{S}_5$.   Find $f^{-1}$.

First flip the matrix (switching the bottom and top rows) and then reorder the columns so that the top is ordered from 1 to 5:
\[
f^{-1} = \left(\begin{matrix} %
2 & 3 & 4 & 5 & 1\cr%
1 & 2 & 3 & 4 & 5\cr%
\end{matrix}\right)
=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
5 & 1 & 2 & 3 & 4\cr%
\end{matrix}\right).
\]
\end{example}



%
%\section{Inverse permutations practice}
%
% \begin{example}  Put $f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%5 & 4 & 3 & 2 & 1\cr%
%\end{matrix}\right)\in\mathrm{S}_5$.  What is $f^{-1}$?
%\begin{enumerate}
%\item $\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%4 & 3 & 2 & 1 & 5\cr%
%\end{matrix}\right)$.
%
%\item $\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%5 & 4 & 3 & 2 & 1\cr%
%\end{matrix}\right)$.
%
%\item $\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%2 & 3 & 4 & 5 & 1\cr%
%\end{matrix}\right)$.
%
%\item None of the above.
%\end{enumerate}
%\end{example}
%
% The correct answer is $f^{-1}=f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%5 & 4 & 3 & 2 & 1\cr%
%\end{matrix}\right)$.   Check that
%$$
%f\circ f = \left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%5 & 4 & 3 & 2 & 1\cr%
%\end{matrix}\right)\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%5 & 4 & 3 & 2 & 1\cr%
%\end{matrix}\right) = \left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%1 & 2 & 3 & 4 & 5\cr%
%\end{matrix}\right)  =  \iota.
%$$





\section{Powers}

 Let $f$ be any permutation in $\mathrm{S}_n$,  let
$m$ be any positive integer.  Let
$$
f^{m}=\underbrace{f\circ f\circ f\circ\cdots\circ f\circ f}_{m\ \mathrm{times}}.%
$$
We also let $f^{-m}=(f^{-1})^m$,  and $f^{0}=\iota$. Then we have the familiar power rules for compositions as we do for multiplication: for $r,s\in\mathbb{Z}$,
$$
f^{r}f^{s}=f^{r+s}\  \text{and}\  \Big(f^{r}\Big)^{s}=f^{rs}.
$$

 \begin{example}
Let $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
2 & 3 & 4 & 5 & 1\cr%
\end{matrix}\right)\in\mathrm{S}_5$.  Then, one can check
$$f^{-1}=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
5 & 1 & 2 & 3 & 4\cr%
\end{matrix}\right),\  f^{0}=\iota,\  f^{1}= f,\  f^{2}=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
3 & 4 & 5 & 1 & 2\cr%
\end{matrix}\right),
$$

$$
f^{3}=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
4 & 5 & 1 & 2 & 3\cr%
\end{matrix}\right),\ 
f^{4}= f^{-1},\ 
f^{5} = \iota,  f^{6}= f,  f^{7}= f^2,  \ldots.
$$
\end{example}

\begin{exercise}  Let $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5\cr%
2 & 1 & 3 & 4 & 5\cr%
\end{matrix}\right)\in\mathrm{S}_5$. What is $f^{2019}$?
\begin{solution}
 Since $f^{2}=\iota$,  we have
$$
\underbrace{f\circ f\circ f\circ\cdots\circ f\circ f}_{2019\ \mathrm{times}}=\underbrace{f\circ f\circ f\circ\cdots\circ f\circ f}_{1009\times 2\ \mathrm{times}}\circ f$$
$$=\Big(f\circ f\Big)^{1009}\circ f=\iota^{1009}\circ f= f.%
$$
\end{solution}
\end{exercise}

Let us summarize some of the properties about permutations in $\mathrm{S}_{n}$  we have proved.

\begin{lemma}%[Proposition 20.2 in Liebeck]
The set $\mathrm{S}_n$ equipped with the composition rule $\circ$ has the following
properties:
\begin{itemize}
\item {\bf Closure}: for any $f$ and $g$ in $\mathrm{S}_n$,  one has $f\circ g\in\mathrm{S}_n$, %

\item {\bf Associativity}: for any $f$, $g$, and $h$ in $\mathrm{S}_n$, one has
$$f\circ \Big(g\circ h\Big)=\Big(f\circ g\Big)\circ h,$$

\item {\bf Identity element}: there is unique permutation $\iota\in\mathrm{S}_n$ such that
$$ f\circ\iota=\iota\circ f=f$$
for any $f\in\mathrm{S}_n$ (and we call $\iota$ the identity permutation),%

\item {\bf Inverse}: for any $f\in\mathrm{S}_n$,  there is unique $f^{-1}\in\mathrm{S}_n$~ such~that
$$
f\circ f^{-1}=f^{-1}\circ f=\iota.
$$
\end{itemize}
\end{lemma}

In {\it Fundamentals of Pure Math} (FPM), you will learn more about {\it groups}, which are sets equipped with the above structure, that is, a way of composing elements that satisfies the above properties. We have seen several examples so far: 

\begin{itemize}
\item  real numbers equipped with $+$, %

\item  non-zero real numbers equipped with $\cdot$ (i.e. multiplication), %

\item  complex numbers equipped with $+$, %

\item  non-zero complex numbers equipped with $\cdot$, %

\item  $n$-th root of unity equipped with $\cdot$, %

\item  $\mathbb{Z}_n$ equipped with $+$, %

\item  $\mathbb{Z}_n$ equipped with $\cdot$ if $n$ is prime (in which case every element has an inverse),
\end{itemize}

%We won't go into group theory much in this class, but we bring it up for a few reasons: one is that some of the topics we will cover this week for permutations will be topics you will cover in more generality for general groups (like the order of an element). Secondly, $S_{n}$ (which will be called the {\it permutation group} when you study algebra) is an important example in group theory whose properties will be useful to know starting Week 1 of FPM. 


\section{Cycles}%
\label{cycles}

\begin{definition} 
 Let $\{a_{1},a_{2},\ldots,a_{r}\}$ be a~non-empty subset of
the~set $\{1,2,\ldots,n\}$. The permutation
$f\in\mathrm{S}_{n}$ such that $$
f\big(a_{1}\big)=a_{2},\ f\big(a_{2}\big)=a_{3}, \ldots,\  f\big(a_{r-1}\big)=a_{r},\  f\big(a_{r}\big)=a_{1}%
$$
and 
  $$f(k)=k\iff k\not\in\{a_{1},a_{2},\ldots,a_{r}\}$$
 is denoted by
$$
\big(a_{1}\ a_{2}\ \cdots\ a_{r}\big)
$$
 and is called a \emph{cycle}   or \emph{cyclic
permutation}   (of length $r$).
\end{definition}

We call these ``cycles'' since, if you imagine arranging the numbers $a_{1},...,a_{r}$ around a circle, $f$ just rotates these numbers. In other words, this is the permutation that sends $a_{1}$ to $a_{2}$, $a_{2}$ to $a_{3}$,..., and $a_{r}$ to $a_{1}$, and leaves everything else alone. 

Notice that if $j\in \{1,2,...n\}$, then $(j)=\iota$, so $\iota$ is a cycle of length 1.


 \begin{example}
To find the matrix representation of the cycle $f=\big(2\ 5\ 4\ 7\big)$, observe that for $k\not\in \{2,5,4,7\}$, $f(k)=k$. Moreover, the cycle notation tells us that $f(2)=5$, $f(5)=4$, $f(4)=7$ and $f(7)=2$. Thus, $f$ has matrix representation $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 \cr%
1 & 5 & 3 & 7 & 4 & 6 & 2 \cr%
\end{matrix}\right)\in\mathrm{S}_{7}$. 

\end{example}




 \begin{example}
Let $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6\cr%
1 & 5 & 3 & 2 & 4 & 6 \cr%
\end{matrix}\right)\in\mathrm{S}_6$.  If we are given that $f$ is a cycle, then we can figure out its cycle notation by first finding the values $a_{1},...,a_{r}$ that $f$ cycles through, that is, the values $k$ for which $f(k)\neq k$. These are $2,5,$ and $4$, and $f$ leaves all other elements of $\{1,2,3,4,5,6\}$
fixed (that is $f(k)=k$ for $k\not\in \{2,5,4\}$). Then we just look at where $f$ sends these numbers in succession:
$$
f\big(2\big)=5,\  f\big(5\big)=4,\  f\big(4\big)=2,
$$
Thus, $f=(2\ 5\ 4)$.
\end{example}


%
%
%\section{TopHat question (cycles)}
%
% \begin{exercise}  Which one of the following
%permutations in $\mathrm{S}_5$ is a cycle?
%
%\begin{enumerate}[label=\alph*]
%\item
%$\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%2 & 3 & 1 & 5 & 4\cr%
%\end{matrix}\right)$.
%
%\item
%$\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%3 & 2 & 4 & 5 & 1\cr%
%\end{matrix}\right)$.
%
%\item
%$\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%3 & 5 & 4 & 1 & 2\cr%
%\end{matrix}\right)$.
%
%\item None of the above.%
%\end{enumerate}
%\end{exercise}
%
%\begin{itemize}
%\item
%$\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%2 & 3 & 1 & 5 & 4\cr%
%\end{matrix}\right)=(1\ 2\ 3)\circ (4\ 5)$,
%
%\item $\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%3 & 2 & 4 & 5 & 1\cr%
%\end{matrix}\right)=(1\ 3\ 4\ 5)$,
%
%\item
%$\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5\cr%
%3 & 5 & 4 & 1 & 2\cr%
%\end{matrix}\right)=(1\ 3\ 4)\circ (2\ 5)$.
%\end{itemize}
%

Notice that the notation of a cycle is not unique:


 Indeed, for a cycle $(a_{1}\ a_{2}\ \cdots\
a_{r})\in\mathrm{S}_n$, we have:
\begin{equation}
\label{e:rotations}
\big(a_{1}\ a_{2}\ \cdots\ a_{r}\big)=\big(a_{2}\ \cdots\ a_{r}\ a_{1}\big)=\big(a_{3}\cdots\ a_{r}\ a_{1}\ a_{2}\big)=\cdots%
\end{equation}


\begin{example}
Let $f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \cr%
1 & 5 & 3 & 7 & 4 & 6 & 8 & 2 \cr%
\end{matrix}\right)\in\mathrm{S}_{9}$.  Then
$$
f=\big(2\ 5\ 4\ 7\ 8\big)=\big(5\ 4\ 7\ 8\ 2\big)=\big(4\ 7\ 8\ 2\ 5\big)=\big(7\ 8\ 2\ 5\ 4\big)=\big(8\ 2\ 5\ 4\ 7\big).%
$$
\end{example}



Note that if $f$ is a cycle of length $r$ in $\mathrm{S}_n$, 
 then the powers follow the following pattern:
$$
\ldots f^{-1}=f^{r-1},\  \underbrace{\iota=f^{0},\ f^{1},\ f^{2},\ f^{3},\ldots,\ f^{r-1}},\  f^{r}=\iota,\  f^{r+1}=f,  \ldots%
$$
where the permutations $f^{0},f^{1},...,f^{r-1}$ are all distinct. This is because, if $f=(a_{1}\; \cdots \; a_{r})$ and $0\leq  k < r$, then $f^{k}(a_{1})=a_{1+k}$, which ranges through $a_{1},a_{2},...,a_{r}$ as $k=0,1,...,r-1$ and these are all distinct numbers. Moreover, $f^{r}(a_{1})=f\circ f^{r-1}(a_{1})=f(a_{r})=a_{1}$, and recalling \eqref{e:rotations}, we can also show that $f^{r}(a_{i})=a_{i}$ for $i=1,...,r$. Since $f(k)=k$ for all $k\not\in \{a_{1},...a_{r}\}$ anyway, this means $f^{r}(k)$ for those $k$, and thus $f^{r}=\iota$. We have thus shown the following.

 \begin{corollary}  If $a_{1},...,a_{r}$ are distinct numbers in $\{1,2,...n\}$ and $f=(a_{1}\; a_{2} \; \cdots \; a_{r})$, then $r$ is the smallest positive integer such
that $f^r=\iota$.
\end{corollary}

In particular, this implies that $\iota=f^{2r}=f^{3r}=f^{4r}=\ldots$, and that  $f^{kr+m}=f^{m}$ for all $k,m\in\mathbb{Z}$.

Recall that $S_{n}$ is not commutative for $n\geq 3$, that is, we can find $f,g\in S_{n}$ so that $fg\neq gf$. However, if $f$ and $g$ are cycles that cycle through disjoint sets of numbers, then we do have $fg=gf$. We make this more precise below:

\begin{definition}  If $f=(a_{1}\; a_{2}\; \cdots \; a_{r})$ and $g=(b_{1}\; b_{2}\; \cdots \; b_{s})$ where $\{a_{1},...,a_{r}\}$ and $\{b_{1},...,b_{s}\}$ are disjoint sets, we say that $f$ and $g$ are {\it disjoint} in $\mathrm{S}_{n}$.
\end{definition}

\begin{proposition}
If $f$ and $g$ are disjoint cycles in $S_{n}$, then $fg=gf$. 
\end{proposition}

\begin{proof}
Let $f=(a_{1}\; a_{2}\; \cdots \; a_{r}\}$ and $g=(b_{1}\; b_{2}\; \cdots \; b_{s}\}$ where $\{a_{1},...,a_{r}\}$ and $\{b_{1},...,b_{s}\}$ are disjoint sets in $\{1,2,...,n\}$. Let us look at the charts for $f$ and $f\circ g$, but ordering the first row so that they begin with $a_{1},...,a_{r},b_{1},...,b_{s}$:

\[
 \left(\begin{array}{c|ccccccccccc} %
k & a_1 & a_2 & \cdots  &  a_{r-1} &  a_r & b_{1} & b_{2} & ...  & b_{s-1} &  b_{s} & ... \\  %
g(k) & a_1 & a_2 & \cdots  &  a_{r-1} &  a_r & b_{2} & b_{3} & ...  & b_{s} &  b_{1} & ... \\  %
f(g(k)) & a_2 & a_3 & \cdots  &  a_{r} &  a_1 & b_{2} & b_{3} & ...  & b_{s} &  b_{1} & ... \\  %
\end{array}\right)
\]
Above, the final numbers in ``$...$'' are the remaining integers $\{1,2,...,n\}$ not in either $\{a_{1},...,a_{r}\}$ or $\{b_{1},...,b_{s}\}$, and for $k$ in this range, we have $f(k)=g(k)=k$. Notice that since the $a$'s are not in  $\{b_{1},...,b_{s}\}$, $g$ fixes the numbers $a_{1},...,a_{r}$ (that is, $g(a_{i})=a_{i}$ for $i=1,...,r$) and then cycles through the $b$'s. Similarly,  since the $b$'s are not in  $\{a_{1},...,a_{r}\}$, $f$ fixes the numbers $b_{1},...,b_{s}$ (that is, $f(b_{i})=b_{i}$ for $i=1,...,s$) and then cycles through the $a$'s. Thus, we get that
\[
fg=  \left(\begin{array}{c|cccccccccc} %
 a_1 & a_2 & \cdots  &  a_{r-1} &  a_r & b_{1} & b_{2} & ...  & b_{r-1} &  b_{r} & ... \\  %
 a_2 & a_3 & \cdots  &  a_{r} &  a_1 & b_{2} & b_{3} & ...  & b_{r} &  b_{1} & ... \\  %
\end{array}\right).
\]
Notice that if we did the exact same computation but with $gf$ instead of $fg$, we would get the same chart, thus $fg=gf$. 

\end{proof}


\section{Cycle Notation}%
\label{cyclenotation}
Of course, not every permutation is a cycle. However, we do have the following theorem.

\begin{theorem}%[Proposition 20.3 in the book]  
Any permutation $f\in \mathrm{S}_{n}$ is a~composition of disjoint
cycles, that is, $f=f_{1}\cdots f_{k}$ where $f_{1},...,f_{k}$ are disjoint cycles.
\end{theorem}

When $f$ is written in such a way, we say it is written in {\it cycle notation} or {\it cycle decomposition}. Note that we consider the identity $\iota$ to be a product of $n$ $1$-cycles $\iota = (1)(2) \cdots (n)$. 

\begin{proof}
We prove by strong induction that for $k=0,1,...,n$, if $f\in S_n$ and $n-k$ is the number of integers $j\in \{1,2,...,n\}$ for which $f(j)=j$, then $f$ can be written in cycle notation. 

If $k=0$, then we see that if $f$ is a permutation so that $f(j)=j$ for $n-0=n$ distinct integers in $\{1,2,...,n\}$, then $f=\iota$, which is a product of disjoint $1$-cycles. This proves the base case. 

Now suppose we have show that for some $K<n$ and $k=0,1,...,K$, any cycle for which $f(j)=j$ for $n-k$ distinct integers $j\in \{1,2,...,n\}$ can be written as a product of disjoint cycles. Note that the sequence $1,f(1),f(2),...$ must eventually repeat by the pigeonhole principle, that is, since $f(j)$ only takes on values $\{1,2,...n\}$, we must have that $f^i(1)=f^j(1)$ for some $0\leq i<j\leq n$. Thus,
\[
1= f^{-i}f^{i}(1) = f^{-i}f^{j}(1) = f^{j-i}(1).
\]
and so if we let $\ell=j-i-1$ and $a_{r}=f^{r}(1)$ for $r=0,...,\ell$, we then get that 
\[
f=(a_{1}\; a_{2}\;\cdots \; a_{\ell}) g
\]
where $g(k)=k$ for $k\in \{a_{1},...,a_{\ell}\}$ and $g(k)=f(k)$ for every other $k\in \{1,2,...,n\}$. By the strong induction hypothesis, we can decompose $g$ as a product of disjoint cycles, and since $g(k)=k$ for $k\in \{a_{1},...,a_{\ell}\}$, they will also be disjoint from the cycle $(a_{1}\; a_{2}\;\cdots \; a_{\ell})$. Composing all these cycles gives us $f$. This proves the induction step.

Thus the theorem holds by the principle of mathematical induction.
\end{proof}


How do we find the cycle decomposition in practice? The proof above gives us a roadmap: given a permutation $f$, we know there is an integer $r$ so that if $a_{i}= f^{i}(1)$, then $a_{r}=1$, and we know that one of the cycles in the decomposition for $f$ is $(1 \; a_{1}\; \cdots \; a_{r-1})$. To find another cycle, pick $k\not\in \{1,a_{1},...,a_{r-1}\}$ and repeat the process: there is $s$ so that if $b_{i}=f^{i}(k)$, then $b_{s}=k$, and another cycle in the decomposition is $(k,b_{1},...,b_{s-1})$. Repeat this process until each integer is in one of the cycles you've found. 



 \begin{example}  Let
$f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6\cr%
4 & 6 & 1 & 5 & 3 & 2 \cr%
\end{matrix}\right)\in\mathrm{S}_{6}$.  Then
\[
f(1)=4,\  f(4)=5,\  f(5)=3,\  f(3)=1,
\]
so one of the cycles in the cycle decomposition of $f$ is $(1\; 4 \; 5 \; 3)$. Now we look at the cylce that contains $2$ (where we picked $2$ because it is not one of the integers we have placed in a cycle so far), we see
\[
f(2)=6,\  f(6)=2,
\]
 which implies that $f=(1\ 4\ 5\ 3)(2\ 6)$. Recall that disjoint cycles commute, so we could also write $f$ as $=(2\
6)(1\ 4\ 5\ 3)$.
\end{example}


If the cycle decomposition consists of some $1$-cycles like $(j)$, note that these are just the identity mapping, so we can omit them when writing a permutation in cycle notation. For example, the permutation on $S_{10}$ that just switches the numbers $1$ and $2$ can be written succinctly as $(1\; 2)$ instead of $(1 \; 2 ) (3)(4)(5)(6)(7)(8)(9)(10)$. 
%
%\begin{example}
%What happens if instead we have $f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5 & 6 & 7 \cr%
%4 & 6 & 1 & 5 & 3 & 2 & 7 \cr%
%\end{matrix}\right)\in\mathrm{S}_{7}$? If we repeat the above process, we get the cycle decomposition $f=(1\ 4\ 5\ 3)(2\ 6)(7)$. Notice that any one-cycle is just the identity, so we can also just write $f=(1\ 4\ 5\ 3)(2\ 6)$. That is, $f$ has the same cycle decomposition as in the previous example, but these are still different functions: in the previous example, $f$ was a permutation on $\{1,2,...,6\}$, and in this example, it is a permutation on $\{1,2,...,7\}$. 
%\end{example}

Recall that disjoint cycles commute, but compositions of cycles that aren't disjoint in general won't commute. 

\begin{example}
Let $f=(1 \; 2 \; 3)(4 \; 5)$ and $g=(1 \; 2 \; 3 \; 4)$ be permutaitons in $S_{5}$. To find the cycle notation for $fg$, we do as before: start with $1$ and look at the values we get by repeatedly plugging into $fg$:
\begin{align*}
fg(1) = f(g(1)) = f(2) = 3, \\
fg(3) = f(g(3))=f(4)=5,\\
  fg(5) = f(g(5)) = f(5) = 4,\\
  fg(4) = f(g(4)) = f(1) = 2, \\
  fg(2) = f(g(2)) = f(3) = 1.
\end{align*}
So the first cycle is $(1 \; 3 \; 5 \; 4 \; 2)$. Note that all numbers in $\{1,2,...,5\}$ have been used, and so $fg = (1 \; 3 \; 5 \; 4 \; 2)$. Now we do the same for $gf$, starting with $1$ again:
\begin{align*}
gf(1) = g(f(1)) = g(2) = 3, \\  gf(3) = g(f(3)) = g(1) = 2,  \\ gf(2) = g(f(2)) = g(3) = 4,  \\ gf(4) = g(f(4)) = g(5) = 5,  \\ gf(5) =  g(f(5))  = g(4) = 1
\end{align*}
and since this uses up all numbers from 1 to 5, we get $gf = (1\; 3 \; 2 \; 4 \; 5)\neq fg$. 
\end{example}



For $f\in S_{n}$, the {\it cycle shape} of $f$ is the sequence of numbers we get by listing the orders of the cycle decomposition in decreasing order. If a number appears several times in the sequence, we replace it with that number to the power of how many times it appears.

\begin{example}
\begin{itemize}
\item 
The permutation $f=(1\ 4\ 5\ 3)(2\ 6)(7)\in S_{7}$ has cycle shape $(4,2,1)$. 
\item The permutation $g=(1 \; 2) (3 \; 4) \in S_{4}$ has cycle shape $(2,2)$, or more succinctly, $(2^2)$. 
\item The permutation $h = (1 \; 4)(2\; 3) (\;5 \; 7)(8 \; 9 \;11)$ in $S_{11}$ has cycle shape $(3,2^3,1^2)$. Why the $1^2$? Recall that $6$ and $10$ don't appear in any of the cycles in $h$, so $h(6)=6$ and $h(10)=10$, so really the full cycle decomposition of $h$ is $(1 \; 4)(2\; 3) (\;5 \; 7)(8 \; 9 \;11)(6)(10)$. 
\end{itemize}
\end{example}






%
% \begin{example}  For $f=(1\ 2\ 4\ 5)\in\mathrm{S}_5$,
% we have 
%$$
%f^0=\iota,\  f^1=(1\ 2\ 4\ 5),\  f^2=(1\ 4)(2\ 5),\  f^3=(1\ 5\ 4\ 2),%
%$$
% and \textcolor[rgb]{0.00,0.40,0.29}{all other powers} are
%among these four permutations.
%\end{example}



%
%
%\section{TopHat question (interlacing $12$ cards)}
%
% Let $m$ be a positive integer. Put $n=2m$.
%
% Consider a permutation $f\in\mathrm{S}_{n}$ such that
%$$
%f=\left(\begin{matrix} %
%1 & 2   & 3 & 4   & 5 & 6   & 7 &  \cdots & 2m-1 & 2m \cr%
%1 & m+1 & 2 & m+2 & 3 & m+3 & 4 & \cdots &  m   & 2m \cr%
%\end{matrix}\right).
%$$
%
%
%\begin{exercise}
% Suppose that $m=6$.  What is the smallest
%$r\in\mathbb{N}$ such that $f^r=\iota$?
%\end{exercise}
%
% If $m=6$, then $f$ is given by
%$$
%\left(
%\begin{array}{cccccccccccc}
%1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \cr%
%1 & 7 & 2 & 8 & 3 & 9 & 4 & 10 & 5&  11 & 6 & 12 \cr%
%\end{array}
%\right)=(2\ 7\ 4\ 8\ 10\ 11\ 6\ 9\ 5\ 3),
%$$
% which implies that the smallest $r$ such that $f^r=\iota$
%is $10$.
%





% \begin{remark}  Decomposition into a product of
%disjoint cycles is \emph{almost unique}.
%\end{remark}



\section{Order of a permutation}

Recall that $S_{n}$ has $n!$ elements. In particular, this means that if we look at the sequence $f,f^{2},f^{3},...$, eventually this must repeat on itself (otherwise we would have infinitely many distinct permutations on $n$ integers, whereas we know there are only $n!$ many). In other words, if we have a deck of $n$ cards, and we shuffle the cards according to some permutation $f$, then no matter how random the shuffle is, so long as we use the {\it same} shuffle $f$, eventually the cards will return to their original order. 

In this section we'll show how to compute how soon powers of a permutation repeat. 

 \begin{definition}  Let $f$ be a permutation in $\mathrm{S}_n$.  The smallest positive integer
$m$ such that 
$$
f^m=\iota
$$
 is called the \emph{order} of the permutation $f$.
\end{definition}

The order of a cycle of length $r$ is just $r$, which we saw earlier. In a few steps we will show how to compute the order of a permutation in general. 

\begin{lemma}  Let $f\in\mathrm{S}_n$ have order $m$.   If $f^k=\iota$ for some integer $k\in\mathbb{N}$, then $m$ divides $k$.\end{lemma}

\begin{proof}
If $m$ is the order of $f\in\mathrm{S}_n$ and $f^k=\iota$, then by definition of $m$, $m\leq k$. By the remainder theorem, there are $q\geq 0$ and $0\leq r<m$ so that $k=qm+r$, and so
\[
f^{k} = f^{qm+r} = (f^{m})^{q} f^{r} = (\iota)^{q}f^{r} = \iota f^{r}= f^{r}.
\]
Recall that $m$ is the smallest positive integer for which $f^{m}=\iota$, but now we have $r\in \{0,1,...,m-1\}$ (and so $r<m$) for which $f^{r}=\iota$. This is only possible if $r$ is not positive, i.e. if $r=0$. Thus, $k=qm+0=qm$ so $m|k$.
\end{proof}



\begin{lemma}%[Proposition 20.4 in Liebeck]  
Let $f\in\mathrm{S}_n$ and let $m$ be its order. Write
$$
f=\sigma_{1}\sigma_{2}\cdots\sigma_{s}
$$
 where $\sigma_{1},\ldots,\sigma_{s}$ are disjoint cycles of
lengths $r_{1},\ldots,r_{s}$ respectively. Then 
$$
m=\mathrm{lcm}\big(r_{1},r_{2},\ldots,r_{s}\big),
$$
 where $\mathrm{lcm}$ stands for
\textcolor[rgb]{0.00,0.50,0.50}{the~least common multiple}, that is, the smallest positive integer that is divisible by $\sigma_{i}$ for each $i=1,...,s$.
\end{lemma}

\begin{proof}
Recall that the order of a cycle is just its length, so we know that $\sigma_i^{r_i}=\iota$. 
Notice that if $d= \mathrm{lcm}\big(r_{1},r_{2},\ldots,r_{s}\big)$, then $r_i|d$ for all $i$. Also recall that disjoint cycles commute, so we can split the following product:
\[
f^{d} = (\sigma_{1}\sigma_{2}\cdots\sigma_{s})^{d}
=\sigma_{1}^{d}\sigma_{2}^{d}\cdots \sigma_{s}^{d} 
=(\sigma_{1}^{r_{1}})^{d/r_{1}}(\sigma_{2}^{r_{2}})^{d/r_{2}}\cdots(\sigma_{s}^{r_{s}})^{d/r_{s}} =
(\iota)^{d/r_{1}}(\iota)^{d/r_{2}}\cdots (\iota)^{d/r_{s}}=\iota. 
\]
By the previous lemma, this means $m|d$. However, note that since $m$ is the order of $f$,
\[
\iota = f^{m} = (\sigma_{1}\sigma_{2}\cdots\sigma_{s})^{m}
=\sigma_{1}^{m}\sigma_{2}^{m}\cdots \sigma_{s}^{m} 
\]
and since these cycles are disjoint, we must have that $\sigma_{i}^{m}=\iota$ for each $i$. By the previous lemma again, this means $r_i|m$ (since $r_i$ is the order of $\sigma_i$). By the definition of the least common multiple, this means $d|m$. Finally, $m|d$ and $d|m$ (and both being positive integers) imply $d=m$.
\end{proof}

Using this Lemma, you can now always compute $m$ by finding the cycle decomposition.

 \begin{example} The~order of the~permutation
$$
\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\cr%
3 & 4 & 6 & 1 & 5 & 2 & 8 & 7\cr%
\end{matrix}\right) = \big(1\ 3\ 6\ 2\ 4\big)\big(7\ 8\big)\in\mathrm{S}_{8},%
$$
 is $\mathrm{lcm}(2, 5, 1)=\mathrm{lcm}(2, 5)=10$, while the order of 
 $$
\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\cr%
5 & 6 & 7 & 8 & 1 & 2 & 3 & 4\cr%
\end{matrix}\right) =  (1 \; 5)(2 \; 6)(3\; 7)(4\; 8) \in\mathrm{S}_{8},%
$$
is $\mathrm{lcm}(2,2,2,2)=2$.
 \end{example}


%
%
%\section{Tophat question (interlacing $16$ cards)}
%
% Let $m$ be a positive integer.  Put $n=2m$ and put
%
%$$
%f=\left(\begin{matrix} %
%1 & 2   & 3 & 4   & 5 & 6  & \cdots& 2m-1 & 2m \cr%
%1 & m+1 & 2 & m+2 & 3 & m+3& \cdots&  m & 2m \cr%
%\end{matrix}\right)\in\mathrm{S}_{n}.
%$$
%
%
%\begin{exercise}
%Suppose that $m=8$.   What is the order of $f$?
%\end{exercise}
%
% If $m=8$,  then $f=(2\ 9\ 5\ 3)(4\ 10\ 13\ 7)(6\
%11)(8\ 12\ 14\ 15)$ is 
%$$\left(
%\begin{array}{cccccccccccccccc} %
%1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12& 13& 14& 15 & 16\cr%
%1 & 9 & 2 & 10 & 3 & 11 & 4 & 12 & 5&  13 & 6 & 14 & 7& 15& 8 & 16\cr%
%\end{array}\right),
%$$
% which implies that the order of the permutation $f$ is $4$.
%


\section{Even and Odd permutations}%
\label{evenandoddpermutations}

The content of this section will be a little easier to talk about with a motivating example. Recall the {\it 9-puzzle}: this is a puzzle that involves a table with 9 numbers and a missing space in a 3 by 3 grid, like so.

$$
\begin{matrix} %
 \Box  & 2  & 3  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
$$
where the box represents an empty space. The objective is to put all the numbers in order by sliding numbers into the empty space, creating a new empty space, and so on.  So for example, the above puzzle can be solved using the following moves:
\[
\begin{matrix} %
 \Box  & 2  & 3  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\longrightarrow
\begin{matrix} %
1 & 2  & 3  \cr%
 \Box  & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\longrightarrow
\begin{matrix} %
1 & 2  & 3  \cr%
4 &  \Box  & 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\longrightarrow 
\begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & \Box  \cr%
7 & 8 & 6 \cr%
\end{matrix}
\longrightarrow 
\begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & 6  \cr%
7 & 8 & \Box \cr%
\end{matrix}
\]

This puzzle is a 3 by 3 variant of the {\it 15-puzzle} (on a 4 by 4 grid) that was famous in the late 1800s. 

Not every arrangement of numbers can be solved. For example, there is no way of solving the puzzle below
\[
\begin{matrix} %
 \Box  & 3  & 2  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}.
\]
How do we know that a given arrangement of numbers and a space can be solved? The math in this section will give us a way of figuring this out.


We start with the following lemma.

 \begin{lemma}
 \label{l:prodof2cycles}
Every permutation in $\mathrm{S}_n$ is
a product of cycles of length $2$.
\end{lemma}

\begin{proof}  Let $f$ be a permutation in
$\mathrm{S}_n$. We can also assume that $f$ is a cycle of length $r$ since otherwise, any permutation in $\mathrm{S}_n$ is a product of
disjoint cycles, and we can just multiply together the respective products of 2-cycles we get.

So suppose $f=(a_1\ a_2\ a_3\ \ldots a_{r-1}\ a_r)$. 
Then
$$
f=\big(a_1\ a_2\ a_3\ \ldots a_{r-1}\ a_r\big)=\big(a_1\ a_{r}\big)\big(a_1\ a_{r-1}\big)\big(a_1\ a_{r-2}\big)\cdots \big(a_1\ a_{3}\big)\big(a_1\ a_{2}\big).%
$$
One can check this by plugging in various values $a_i$. If we plug in $a_3$, for example, remember that when plugging a number into a composition $f_{1}...f_{n}$, we plug the number into $f_n$ first, then the resulting value into $f_{n-1}$, and so on. So in the above product for $f$, we first plug $a_3$ into $(a_{1}\; a_{2})$, which just returns $a_{3}$. Then we plug that into $(a_{1}\; a_{3})$, which gives $a_{1}$. Then we plug that into $(a_{1}\; a_{4})$ ad we get $a_{4}$. But now $a_{4}$ does not appear in any other $2$-cycle, and so they all return $a_{4}$. Hence, $f(a_{3})=a_{4}$, and we can show similarly that $f(a_{i})=a_{i+1}$ for all $i$ except $r$ where $f(a_{r})=a_{1}$.
\end{proof}

Note that the product of cycles in this Lemma is far from being
unique, and the cycles of order $2$ in this Lemma are not necessary
disjoint.



%
%\section{Tophat question (simple interchanges)}
%
% Every permutation in $\mathrm{S}_n$ is a product of cycles
%of length $2$.
%
% Consider the following permutation
% $$f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5 \cr%
%2 & 3 & 1 & 5 & 4 \cr%
%\end{matrix}\right)\in\mathrm{S}_{5}.
%$$
%
%\begin{exercise}  Which one of the following
%assertions is true?
%
%\begin{enumerate}
%\item $f$ is a product of two cycles of order $2$.
%
%\item $f$ is a product of four cycles of order $2$.
%
%\item $f$ is a product of five cycles of order $2$.
%
%\item None of the above.
%\end{enumerate}
%\end{exercise}
%
%\vspace{-0.2in}$$f=\left(\begin{matrix} %
%1 & 2 & 3 & 4 & 5 \cr%
%2 & 3 & 1 & 5 & 4 \cr%
%\end{matrix}\right)=\big(1\ 2\ 3\big)\big(4\ 5\big)=\big(1\ 3\big)\big(1\ 2\big)\big(4\
%5\big),$$
%which implies that $f$ is a product of
%\textcolor[rgb]{0.98,0.00,0.00}{five} cycles of order $2$.
%
%


We would like to define a permutation to be {\it even} if it is
a product of even number of cycles of order $2$, and {\it odd} if it is a
product of odd number of cycles of order $2$. However, it is not clear from this definition whether or not a permutation can be both even and odd at the same time. Hence, we will actually define the properties of being even and odd slightly differently, and then later deduce that even and odd permutations can be written as an even or odd number of $2$-cycles.

Our ad-hoc definition of even and odd permutations in $S_{n}$ will rely on the following function of $n$ variables $x_{1},...,x_{n}$:
\[\Delta (x_{1},...,x_{n}) = \prod_{1\leq i<j\leq n} (x_{i}-x_{j}),\]
that is, the product of all pairs $(x_{i}-x_{j})$ where $1\leq i<j\leq n$. So for example, in $S_{3}$, we have 
\[\Delta (x_{1},...,x_{n}) = (x_{1}-x_{2})(x_{2}-x_{3})(x_{1}-x_{3}).\]
Given a permutation $f\in S_{n}$, we will define $f(\Delta)$ to be the function 
\[f(\Delta)(x_{1},...,x_{n})  = \prod_{1\leq i<j\leq n} (x_{f(i)}-x_{f(j)}).\]
Notice that $f(\Delta)$ is actually just $\pm \Delta$, since all the permutation does is change some of the $(x_{i}-x_{j})$ to either $(x_{i}-x_{j})$ or $(x_{j}-x_{i})$. We define the {\it signature} of $f$ to be
\[
\mbox{sgn}(f) = \frac{f(\Delta)}{\Delta},
\]
that is, $\mbox{sgn}(f) =1$ if $f(\Delta)=\Delta$ and $\mbox{sgn}(f) =-1$ if $f(\Delta)=-\Delta$. 

\begin{definition}
We say a permutation $f\in S_{n}$ is {\it even} if $\mbox{sgn}(f) =1$ and {\it odd} if  $\mbox{sgn}(f) =-1$.
\end{definition}

The above definition of even and odd and the signature may seem really technical to work with, but the lemma below will provide us with some short-cuts so that we never actually have to refer to $n$-variable polynomials:


\begin{lemma}%[Propositions 20.5, 20.6, and 20.7 in the book]
Let $n\in\mathbb{N}$.
\begin{enumerate}[label=(\alph*)]
\item The signature of $\iota$ is $1$ and any $2$-cycle is $-1$. 
\item For any $f$ and $g$ in $\mathrm{S}_n$,  we have
$\mathrm{sgn}(fg)=\mathrm{sgn}(f)\mathrm{sgn}(g)$.
\item The signature of a cycle of length $r$ is $(-1)^{r-1}$.%
\item For every permutation $f\in\mathrm{S}_n$,  let $f=\sigma_{1}\sigma_{2}\cdots\sigma_{s}$
 where $\sigma_{1},\ldots,\sigma_{s}$ are disjoint cycles of
lengths $r_{1},\ldots,r_{s}$.  Then
$$
\mathrm{sgn}(f)=(-1)^{r_1-1}(-1)^{r_2-1}(-1)^{r_3-1}\cdots (-1)^{r_s-1}.%
$$

\item If $f\in S_{n}$, then $\mathrm{sgn}(f)=\mathrm{sgn}(f^{-1})$. 

\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item First, $\mbox{sgn}(\iota)=\frac{\Delta}{\Delta}=1$. Next, let $(a\; b)$ be a $2$-cycle with $a<b$. Then the only terms $(x_{i}-x_{j})$ in the polynomial $\Delta$ that change sign (i.e. are reversed) when we compute $(a\; b)(\Delta)$ are the terms
\[
\Big((x_{a}-x_{a+1})(x_{a}-x_{a+2})\cdots (x_{a}-x_{b})\Big)
\Big((x_{a+1}-x_{b})(x_{a+2}-x_{b})\cdots (x_{b-1}-x_{b})\Big)
\]
There are $b-a$ terms in the first set of big parentheses and $b-a-1$ in the second set, for a total of $2(b-a)-1$, an odd number, hence 
\[
(a\; b)(\Delta) = (-1)^{2(b-a)-1} \Delta = -\Delta,
\]
so $\mbox{sgn}((a\; b))=-1$. 
\item Let $f$ and $g$ be two permutations. Notice that $f(-\Delta)=-f(\Delta)$, and so
\[
fg(\Delta)
=f(g(\Delta)) = f(\mbox{sgn}(g)\Delta) =  \mbox{sgn}(g) f(\Delta)
=\mbox{sgn}(g)\mbox{sgn}(f)\Delta
\]
and this implies $\mathrm{sgn}(fg)=\mathrm{sgn}(f)\mathrm{sgn}(g)$

\item Now things get a lot easier: If $f$ is an $r$-cycle, then by the proof of Lemma \ref{l:prodof2cycles}, it is a product of $r-1$ many $2$-cycles $\sigma_{1},...,\sigma_{r}$, so by (a) and (b),
\[
\mbox{sgn}(f) = \mbox{sgn}(\sigma_{1})\cdots  \mbox{sgn}(\sigma_{r})  =(-1)^{r-1}.\]
\item The same proof as in (c) proves this one. 
\item Finally, 
\[
1= \mbox{sgn}(\iota)  = \mbox{sgn}(ff^{-1})=\mbox{sgn}(f)\mbox{sgn}(f^{-1})
\]
and since the signature is always $\pm 1$, we can divide both sides by $\mbox{sgn}(f^{-1})$ to get 
\[
\mbox{sgn}(f)=\mbox{sgn}(f^{-1})^{-1} = \mbox{sgn}(f^{-1}).
\]
\end{enumerate}

\end{proof}

As a corollary of the above lemma, we get the following.

\begin{corollary}
Let $f$ be a permutation in $\mathrm{S}_n$.
\begin{enumerate}
\item $f$ is \textcolor[rgb]{0.98,0.00,0.00}{even} if and only if it is
a product of even number of cycles of order $2$.

\item $f$ is \textcolor[rgb]{0.98,0.00,0.00}{odd} if and only if it is a
product of odd number of cycles of order $2$.
\end{enumerate}
\end{corollary}



\begin{example}  The permutation
$$
\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
2 & 3 & 1 & 5 & 4 \cr%
\end{matrix}\right)=\big(1\ 2\ 3\big)\big(4\ 5\big)=\big(1\ 3\big)\big(1\ 2\big)\big(4\ 5\big)
$$
 is odd,  because it is a product of $3$ cycles of
length $2$.


$\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 \cr%
2 & 3 & 4 & 5 & 1 \cr%
\end{matrix}\right)=\big(1\ 2\ 3\ 4\ 5\big)=\big(1\ 5\big)\big(1\ 4\big)\big(1\ 3\big)\big(1\ 2\big)
$  is even.
\end{example}



\begin{example}
Let's revisit the $9$-puzzle and start with the one we solved earlier, that is, where we wanted to rearrange 
\[\begin{matrix} %
 \Box  & 2  & 3  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix} \;\;\;\; \mbox{ into } \;\;\;\; \begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & 6  \cr%
7 & 8 & \Box \cr%
\end{matrix}.\]
If we consider the $\Box$ as being $9$, the above arrangement on the left corresponds to applying the permutation 
\[
f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \cr%
4 & 2 & 3 & 5 & 6 & 9 & 7 & 8 & 1\cr%
\end{matrix}\right)
=(1 \; 4 \; 5 \; 6 \; 9 )(2)(3)(7)(8)=(1 \; 4 \; 5 \; 6 \; 9 )
\]
to the grid on the right. Performing a move by moving a number into the $\Box$ corresponds to applying a $2$-cycle. Let's revisit the moves we did earlier to solve this puzzle and indicate what $2$-cycles we are using:
\[
\begin{matrix} %
 \Box  & 2  & 3  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\;\;
\stackrel{(1 \; 4)}{\longrightarrow}
\;\;
\begin{matrix} %
1 & 2  & 3  \cr%
 \Box  & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\;\;
\stackrel{(4 \; 5)}{\longrightarrow}
\;\;
\begin{matrix} %
1 & 2  & 3  \cr%
4 &  \Box  & 5 \cr%
7 & 8 & 6 \cr%
\end{matrix}
\;\;
\stackrel{(5 \; 6)}{\longrightarrow}
\;\;
\begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & \Box  \cr%
7 & 8 & 6 \cr%
\end{matrix}
\;\;
\stackrel{(6 \; 9)}{\longrightarrow}
\;\;
\begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & 6  \cr%
7 & 8 & \Box \cr%
\end{matrix}
\]

Thus, applying the permutation $(6 \; 9)(5 \; 6)(4 \; 5)(1 \; 4) $ returns the board to the original order, that is, it inverts the permutation $(1 \; 4 \; 5 \; 6 \; 9 )$. 

Now let's look at the following puzzle:

\[
\begin{matrix} %
 \Box  & 3  & 2  \cr%
1 & 4& 5 \cr%
7 & 8 & 6 \cr%
\end{matrix} \;\;\;\; \mbox{ into } \;\;\;\; \begin{matrix} %
1 & 2  & 3  \cr%
4 & 5 & 6  \cr%
7 & 8 & \Box \cr%
\end{matrix}.\]
The permutation corresponding to this arrangement is
\[
g=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \cr%
4 & 3 & 2 & 5 & 6 & 9 & 7 & 8 & 1\cr%
\end{matrix}\right)
=(1 \; 4 \; 5 \; 6 \; 9 )(2\; 3)(7)(8)=(1 \; 4 \; 5 \; 6 \; 9 )(2 \; 3)
\]
Notice that if the puzzle can be solved, that requires moving the square up or down an even number of times, and left and right an even number of times. Thus, the puzzle must take an even number of moves, and since each move is a $2$-cycle, that implies that the inverse of the above permutation must be even, and hence the permutation itself must also be even (this uses Lemma 7.10 (a),(d) and (e)). However, the signature of the above permutation (using Lemma 7.10 (b) and (c)) is 
\[
\mbox{sgn}\Big((1 \; 4 \; 5 \; 6 \; 9)(2 \; 3)\Big)
=\mbox{sgn}\Big((1 \; 4 \; 5 \; 6 \; 9)\Big)\mbox{sgn}\Big((2 \; 3)\Big)
=(-1)^{5-1}(-1)^{2-1} = -1.
\]
and so this is an odd permutation, and this gives a contradiction. Thus, the original puzzle is not solvable.

\end{example}


\section{Exercises}%
\label{permutationexercises}
The relevant exercises to this section are in Chapter 20 of Liebeck.


\begin{exercise}
Consider the permutation
\[f=\left(\begin{matrix} %
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \cr%
3  &  5 &   4   & 8  &  7  &  6  &  2  &  1
\end{matrix}\right)
\]
Write $f, f^{2}$, $f^{-1}$, and $f^{146}$ in cycle notation. 

\begin{solution}
\[
f=(1 \; 3 \; 4 \; 8 )(2 \; 5 \;7 ) ,\;\; f^2 = (1 \; 4)(2 \; 7 \; 5)(3 \; 8 ),\;\; f^{-1} = (8 \; 4 \; 3 \; 1)(7 \; 5 \; 2)
\]
To compute $f^{146}$, note thatthe order of $f$ is $lcm(4,3)=12$, and $146  = 12\cdot 12 + 2$, thus
\[
f^{146} = (f^{12})^{12}\cdot f^2 = f^2 = (1 \; 4)(2 \; 7 \; 5)(3 \; 8 ).
\]
\end{solution}


\end{exercise}

\begin{exercise}
Given $2\leq k\leq n$, how many $k$-cycles are there in $S_{n}$?

\begin{solution}
There are ${n \choose k}$ ways of picking numbers $\{a_{1},...,a_{k}\}$ in $\{1,2,...,n\}$. Now we count how many cycles there are on these numbers via the multiplication principle. For a cycle $f=(f_{1} \; \cdots  f_{k})$ with $f_{i}\in  \{a_{1},...,a_{k}\}$, remember that we also have 
\[
f=(f_{2} \; f_{3}\cdots f_{k} \; f_{1}) 
=(f_{3} \; f_{4} \cdots f_{k} \; f_{1} \; f_{2})\cdots
\]
and so we can assume that $f_{1} = a_{1}$, otherwise we can shift the numbers in the cycle as above until the first term in parentheses is $a_{1}$. Then there are $k-1$ choices for what $f_{2}$ can be, and after picking that there are $k-2$ choices for what $f_{3}$ can be, and so on. Thus, by the multiplication principle, there are $k(k-1)(k-2)\cdots 1=(k-1)!$ many cycles using the numbers  $\{a_{1},...,a_{k}\}$, and thus there are
\[
{n \choose k} (k-1)!
\]
many $k$ cylces in $S_{n}$. 
\end{solution}

\end{exercise}


\begin{exercise}
How many order $3$ permutations are there in $S_{4}$?
\begin{solution}
The only way a permutation in $S_{4}$ can have order $3$ is if it is a $3$-cycle, so by the previous exercise, the number of such cycles is ${4 \choose 3}(3-1)!$.
\end{solution}
\end{exercise}

\begin{exercise}
How many order $3$ permutations are there in $S_{6}$?


\begin{solution}
By looking at the cycle notation for a permutation $f\in S_{6}$, the only way it can have order $3$ is if it is either a $3$-cycle or a product of two disjoint $3$-cycles. As in one of the previous exercises, there are ${6\choose 3}\cdot 2$ $3$-cycles. To count disjoint products of $3$-cycles, notice that for each $3$-cycle $p$ there are $(3-1)!=2$ $3$-cycles $q$ we can have on the remaining $3$ numbers in $\{1,...,6\}$ not appearing in $p$. If we count them in this way though, each product of two disjoint $3$-cycles is counted twice (once as $pq$ and then as $qp$), so we have to divide our answer by $2$, so the number of products of disjoint $3$ cycles is 
\[
\left( {6\choose 3} \cdot 2\right) \cdot 2\cdot\frac{1}{2} = {6\choose 3} \cdot 2.
\]
\end{solution}
Thus, the total number of order $3$ permutations is ${6\choose 3} \cdot 2+ {6\choose 3} \cdot 2= {6\choose 3} \cdot 4$.
\end{exercise}

\begin{exercise} We say a set $S\subseteq S_n$ {\it generates} $f\in S_{n}$ there are $f_{1},...,f_{k}\in S$ so that $f=f_{1}\cdots f_{k}$ (where we could have $k=1$, that is, this could just be a composition of just one function), and $S$ {\it generates} $S_{n}$ if it generates $f$ for every $f\in S_{n}$.

\begin{enumerate}
\item Show that if $f\in S$, then $S$ generates $\iota$ and $f^{-1}$.
\begin{solution}
Recall that each permutation has an order $r$, and so $\iota = f^{r}$ and $f^{-1} = f^{r-1}$ are both products of elements of $S$ (in particular, just products of $f$ with itself over and over). 
\end{solution}
\item Show that $S=\{ (1\; 2), (2 \; 3),...,(n-1 \; n)\}$ generates $S_{n}$. 
\begin{solution}
Let $f\in S_{n}$. Since $f$ is a composition of cycles by Proposition 20.3, we just need to show that each cycle is a composition of functions in $S$. So let $(a_{1}....a_{r})$ be one of the cycles. The claim will follow from the following subclaim:

{\bf Subclaim:} If $(a_{1}...a_{r})$ is an $r$-cycle, then it is a composition of functions from $S$.
 
Note that by the proof of Lemma \ref{l:prodof2cycles} (Proposition 20.6 in the book),
\[
(a_{1}\; \cdots \; a_{r}) = (a_{1} \; a_{r})\cdots (a_{1}\; a_{2}),
\]
Thus, it suffices to show that the cycles $(1 \; j)$ where $1<j\leq n$ are compositions of functions from $S$. We will prove this by induction on $j=2,...,n$.

If $j=2$, then this is immediate since $(1\;2)\in S$ already. This proves the base case. Now we prove the induction step. Assume we have shown that $(1\; j)$ is a composition of functions from $S$ for some $1<j<n$. Then
\[
(1 \; j+1) = (1 \; j)(j\; j+1)(1 \; j).
\]
We already have $(j\; j+1)\in S$, and $(1\; j)$ is a composition of functions from $S$ by the induction hypothesis, hence so is $(1 \; j+1)$, which proves the induction step. 
\end{solution}
\item Show that $T=\{(1\; 2),(1\; 2 \; 3),..., (1 \; 2 \; ...\; n)\}$ generates $S_{n}$.




\begin{solution}
Note that since $(1 \; 2 \; \cdots \; k-1 )\in T$, $T$ generates $(1 \; 2 \; \cdots \; k-1 )^{-1}$, and so it also generates
\[
(1 \; 2 \; \cdots \; k-1 )^{-1} (1 \; 2 \; \cdots \; k-1 \; k)  
=(k-1 \; k)
\]
for every $k$. We have already shown that every $f$ is a product of elements in $S_{n}$ is a product of permutations of the form $(k-1\; k)$, and now we have shown that each such permutation is a product of elements in $T$, thus $T$ generates $S_n$.
\end{solution}


\end{enumerate}
\end{exercise}




\begin{exercise}
 Given $x,y\in S_{n}$, we call $z=xyx^{-1}$ the {\it conjugate of $y$ by $x$}. If $z=xyx^{-1}$ for some $x\in S_{n}$, we say $z$ is a {\it conjugate} of $y$. 

(a) Show that the relation $y\sim z$ if $y$ is conjugate to $z$ is an equivalence relation on $S_{n}$. 

(b) Fix $x\in S_{n}$. Show that the function sending $y\in S_{n}$ to $xyx^{-1}$ is a bijection on $S_{n}$. 

(c) If $c=(a_{1} \; a_{2}...a_{k})$ is a cycle in $S_{n}$ and $x\in S_{n}$, prove that 
\[
xcx^{-1}=(x(a_{1})...x(a_{k})). 
\]
(d) If $y,z$ are conjugates, what can you say about their cycle shapes? Is the converse also true?



\begin{solution}
(a) Clearly $y\sim y$ since $y=\iota y\iota^{-1}$, so $\sim$ is reflexive. If $y\sim z$, then $y=xzx^{-1}$ for some $x\in S_{n}$. Composing both sides on the left with $x^{-1}$, we get $x^{-1}y=zx^{-1}$, and then composing again on the right gives $x^{-1}yx=z$, and so $z$ is a conjugate of $y$, hence $z\sim y$ and we have reflexivity. Finally, if $y\sim z$ and $z\sim w$, then $y=xzx^{-1}$ and $z=sws^{-1}$ for some $x,s\in S_{n}$. Then $y=xzx^{-1} = xswx^{-1}s^{-1} = xsw(xs)^{-1}$, so $y\sim w$. This proves transitivity.

(b) Let $F(y)=xyx^{-1}$. Then $F$ is surjective since, for every $y\in S_{n}$, $y=F(x^{-1}yx)$. It is injective since, if $xyx^{-1}=xzx^{-1}$, then we can compose both sides on the left with $x^{-1}$ to get $yx^{-1}=zx^{-1}$, and then compose with $x$ on the right to get $y=z$. 

(c) If we plug in $x(a_{i})$ (this is the function $x$ evaluated at $a_{i}$), we get $xcx^{-1}(x(a_{i}))=xc(a_{i})=x(a_{i+1})$ if $i<k$ and when $i=k$, $xcx^{-1}(x(a_{k}))=xc(a_{k})=x(a_{1})$. Thus, $xcx^{-1}=(x(a_{1})...x(a_{k}))$. 

In particular, $xcx^{-1}$ is a cycle of the same length.

(d) If $y,z$ are conjugates, then they have the same cycle-shape. If $z=xyx^{-1}$, and $y\in S_{n}$ decomposes into disjoint cycles $y=c_{1}\cdots c_{k}$, then 
\[
xyx^{-1}=xc_{1}x^{-1} xc_{2}x^{-1}\cdots xc_{k}x^{-1}, \]
so $xyx^{-1}$ is also a composition of cycles of the same lengths, and so it has the same cycle-shape as $y$. 

If $y$ and $z$ have the same cycle shape, then they are also conjugate. To see this, note that if $y=c_1...c_{r}$ is a product of disjoint cycles, we can write $z=d_{1}...d_{r}$ as a product of cycles so that $d_{i}$ has the same length as $c_{i}$. If $c_{i}=(a_{i,1}...a_{i,k_i})$ and $d_{i}=(b_{i,j},...,b_{i,k_{i}})$, define $x\in S_{n}$ so that $x(a_{i,j})=b_{i,j}$. Then $xc_ix^{-1}=(x(a_{i,1})...x(a_{i,k_i}))=(b_{i,1},...,b_{i,k_{i}})=d_i$, and so $xyx^{-1}=z$. 
\end{solution}







\end{exercise}





 
\begin{exercise}
{\bf Landau's function} $L(n)$ is defined for every $n\in\mathbb{N}$ to be the largest order of an element of $S_{n}$. This has no convenient formula, but we will prove some simple bounds on it in the next two exercises. \\

(a) Show that  for all $n\in\mathbb{N}$, $L(n)<2^{n}$. {\it Hint: it may help to show $n<2^{n}$ for all $n\in\mathbb{N}$.}

\begin{solution}
{\bf Claim:}
For all $n\in \mathbb{N}$, the order of any cycle $f\in S_{n}$ is at most $2^{n}$. 

{\bf Proof:}
We first need to show that for all natural numbers $n$,
\begin{equation}
\label{e:n<2^n}
n<2^{n}.
\end{equation}
(I have shown this in class, so students may also just cite the lectures for this fact.) This is true for the base case $n=1$, so suppose $n\geq 1$ is an integer so that $2^{n}>n$. Then
\[
n+1<2^{n}+1<2^{n}+2^{n}=2^{n+1}. 
\]
This proves the induction step and hence (1).

Let $n\in\mathbb{N}$ and let $f\in S_{n}$. Then $f$ can be written as a product of cycles of lengths $r_{1},....,r_{k}$ where $r_{i}\in \mathbb{N}$ and $r_{1}+\cdots  + r_{k}=n$. Then the order is the least common multiple of these numbers, which is at most 
\[
r_{1}\cdots r_{k}<2^{r_{1}}\cdots 2^{r_{k}}=2^{r_{1}+\cdots + r_{k}}=2^{n}.
\]
\end{solution}

(b)  Show that $L(n)\geq\frac{n}{2}\cdot \left(\frac{n}{2}-1\right)$ for every $n\in \mathbb{N}$.  {\it Hint: Remember that $L(n)$ is the maximum order of an element in $S_{n}$, so to prove the above claim, you just need to find one element of $S_{n}$ with order at least $\frac{n}{2}\cdot \left(\frac{n}{2}-1\right)$. Try playing around with different products of cycles to see how big you can make their order.}



\begin{solution}
{\bf Claim:}
$L(n)\geq\frac{n}{2}\cdot \left(\frac{n}{2}-1\right)$ for $n\in \mathbb{N}$.

\begin{proof}
We split into two cases depending on if $n$ is even or odd. If $n$ is even, then $n/2$ and $n/2-1$ are coprime. Let $f$ have cycle decomposition 
\[
f=(1\; 2\cdots n/2)(n/2+1 \;\; n/2+2...n-1).
\]
Then these two cycles have lengths $n/2$ and $n/2-1$ respectively, and the order of $f$ is the least common multiple of these two numbers by Proposition 20.4, and since they are comprime, this must be $\frac{n}{2}\left(\frac{n}{2}-1\right)$. 

If $n$ is odd, then $\frac{n-1}{2}$ and $\frac{n+1}{2}$ are coprime. Let $g$ have cycle decomposition 
\[
g=(1\; 2\cdots (n-1)/2)((n+1)/2  ... n)
\]
then the lengths of the cycles in this decomposition are $\frac{n-1}{2}$ and $\frac{n+1}{2}$, and so the order of the cycle is the least common multiple of these numbers by Proposition 20.4 which (because they are coprime) is just their product: $\frac{n-1}{2}\frac{n+1}{2}= \frac{n^2-1}{4}$. 

Taking the minimum of this and $\frac{n}{2}\left(\frac{n}{2}-1\right)$, we see that $L(n)\geq \frac{n}{2}\left(\frac{n}{2}-1\right) $ for all $n\in\mathbb{N}$.
\end{proof}
\end{solution}
\end{exercise}



\end{document}
